---
layout: book
title: 1.3 强化学习
book_id: RLOverview
book_title: 强化学习综述
book_description: 这篇综述介绍了强化学习的基本概念、算法和应用，适合对强化学习感兴趣的读者。
date: 2025-03-13
---

## 1.3 强化学习

在本节中，我们简要概述当环境模型未知时如何计算最优策略，这是强化学习要解决的核心问题。我们主要关注马尔可夫决策过程（MDP）的情况，但会在1.3.4节讨论部分可观测马尔可夫决策过程（POMDP）的情况。

我们可以从两个主要维度对强化学习方法进行分类：（1）根据智能体表示和学习的内容，如价值函数、策略和/或模型；（2）根据动作的选择方式，即**同策略**（动作必须由智能体当前的策略选择）和**异策略**（动作可以由任何类型的策略选择，包括人类演示）。表1.1列出了一些有代表性的例子。后续章节将给出更多细节。

### 1.3.1 基于价值的强化学习（近似动态规划）

在本节中，我们简要介绍基于价值的强化学习，也称为近似动态规划（ADP），更多细节见第2章。

我们在公式(1.1)中引入了价值函数$$V_{\pi}(s)$$，为方便起见，在此重复：

$$
V_{\pi}(s) \triangleq \mathbb{E}_{\pi}[G_0\vert s_0 = s] = \mathbb{E}_{\pi}\left[\sum_{t = 0}^{\infty} \gamma^{t} r_t\vert s_0 = s\right] \tag{1.27}
$$

已知最优策略$$\pi^*$$的价值函数满足以下递归条件，即**贝尔曼方程**：

$$
V_*(s) = \max_{a} R(s, a) + \gamma\mathbb{E}_{p_S(s'\vert s,a)}[V_*(s')] \tag{1.28}
$$

这源于动态规划原理，即通过组合各种子问题的最优解（这里是下一状态$$s'$$的价值）来计算问题的最优解（这里是状态$$s$$的价值）。由此可以推导出以下学习规则：

$$
V(s) \leftarrow V(s) + \eta[r + \gamma V(s') - V(s)] \tag{1.29}
$$

其中，$$s' \sim p_S(\cdot\vert s, a)$$是从环境中采样得到的下一状态，$$r = R(s, a)$$是观测到的奖励。这被称为**时序差分**（Temporal Difference，TD）学习（详见2.3.2节）。不幸的是，如果我们只知道价值函数，并不清楚如何推导出策略。现在我们来描述这个问题的一个解决方案。

我们首先将价值函数的概念进行扩展，通过定义**Q函数**来为状态 - 动作对赋予一个值，定义如下：

$$
Q_{\pi}(s, a) \triangleq \mathbb{E}_{\pi}[G_0\vert s_0 = s, a_0 = a] = \mathbb{E}_{\pi}\left[\sum_{t = 0}^{\infty} \gamma^{t} r_t\vert s_0 = s, a_0 = a\right] \tag{1.30}
$$

这个量表示如果我们在状态$$s$$下执行动作$$a$$，然后按照策略$$\pi$$选择后续动作，所获得的期望回报。最优策略的$$Q$$函数满足修正后的贝尔曼方程：

$$
Q_*(s, a) = R(s, a) + \gamma\mathbb{E}_{p_S(s'\vert s,a)}\left[\max_{a'} Q_*(s', a')\right] \tag{1.31}
$$

由此产生了以下时序差分（TD）更新规则：

$$
Q(s, a) \leftarrow r + \gamma\max_{a'} Q(s', a') - Q(s, a) \tag{1.32}
$$

其中，$$s' \sim p_S(\cdot\vert s, a)$$是从环境中采样得到的。每一步的动作从隐式策略中选择：

$$
a = \underset{a'}{\arg\max} Q(s, a') \tag{1.33}
$$

这被称为**Q学习**（详见2.5节）。

### 1.3.2 基于策略的强化学习

在本节中，我们简要介绍基于策略的强化学习，详细内容见第3章。

在基于策略的方法中，我们尝试关于参数$$\boldsymbol{\theta}$$直接最大化$$J(\pi_{\boldsymbol{\theta}}) = \mathbb{E}_{p(s_0)}[V_{\pi}(s_0)]$$，这被称为**策略搜索**。如果$$J(\pi_{\boldsymbol{\theta}})$$关于$$\boldsymbol{\theta}$$可微，我们可以使用随机梯度上升来优化$$\boldsymbol{\theta}$$，这被称为**策略梯度**（见3.1节）。

策略梯度方法的优点是，对于许多常见的策略类别，它们可以证明收敛到局部最优解，而$$Q$$学习在使用近似时可能会发散（2.5.2.4节）。此外，策略梯度方法可以轻松应用于连续动作空间，因为它们不需要计算$$\underset{a}{\arg\max} Q(s, a)$$。不幸的是，$$\nabla_{\boldsymbol{\theta}}J(\pi_{\boldsymbol{\theta}})$$的分数函数估计量的方差可能非常高，因此所得方法的收敛速度可能很慢。

一种减少方差的方法是学习一个近似价值函数$$V_{\boldsymbol{w}}(s)$$，并将其作为分数函数估计量中的基线。我们可以使用时序差分学习来学习$$V_{\boldsymbol{w}}(s)$$。或者，我们可以学习一个优势函数$$A_{\boldsymbol{w}}(s, a)$$，并将其作为基线。这些策略梯度的变体被称为**演员 - 评论家方法**，其中“演员”指的是策略$$\pi_{\boldsymbol{\theta}}$$，“评论家”指的是$$V_{\boldsymbol{w}}$$或$$A_{\boldsymbol{w}}$$。详细内容见3.3节。

### 1.3.3 基于模型的强化学习

在本节中，我们简要介绍基于模型的强化学习，更多细节见第4章。

基于价值的方法，如Q学习，以及策略搜索方法，如策略梯度，样本效率可能非常低，这意味着它们在找到一个好的策略之前，可能需要与环境进行多次交互。当在现实世界中交互成本很高时，这会成为一个问题。在基于模型的强化学习中，我们首先学习马尔可夫决策过程（MDP），包括$$p_S(s'\vert s, a)$$和$$R(s, a)$$函数，然后计算策略，要么对学习到的模型使用近似动态规划，要么进行前瞻搜索。在实际应用中，我们经常将模型学习和规划阶段交替进行，这样我们可以使用部分学习到的策略来决定收集哪些数据，以帮助学习更好的模型。

### 1.3.4 处理部分可观测性

在马尔可夫决策过程（MDP）中，我们假设环境状态$$s_t$$与智能体获得的观测值$$o_t$$相同。但在许多问题中，观测值只能提供关于世界潜在状态的部分信息（例如，在迷宫中导航的啮齿动物或机器人）。这被称为部分可观测性。在这种情况下，使用形式为$$a_t = \pi(o_t)$$的策略是次优的，因为$$o_t$$不能给出完整的状态信息。相反，我们需要使用形式为$$a_t = \pi(\boldsymbol{h}_t)$$的策略，其中$$\boldsymbol{h}_t = (a_1, o_1, \ldots, a_{t - 1}, o_t)$$是过去所有观测和动作的完整历史，再加上当前观测值。由于对于一个长期运行的智能体来说，依赖整个过去的信息是不可行的，因此已经开发出了各种近似求解方法，下面我们将进行总结。

#### 1.3.4.1 最优解

如果我们知道世界的真实潜在结构（即，使用1.1.2节中的符号表示的$$p(o\vert z)$$和$$p(z'\vert z, a)$$ ），那么我们可以使用为部分可观测马尔可夫决策过程（POMDP）设计的求解方法，这在1.2.1节中讨论过。这需要使用贝叶斯推断来计算信念状态$$\boldsymbol{b}_t = p(z_t\vert \boldsymbol{h}_t)$$（见1.2.5节），然后使用这个信念状态来指导我们的决策。然而，学习一个POMDP的参数（即生成潜在世界模型的参数）是非常困难的，递归地计算和更新信念状态同样困难，根据信念状态计算策略也很困难。事实上，已知对任何方法来说，最优地求解POMDP在计算上都极具挑战性[PT87; KLC98]。因此在实际应用中会使用更简单的近似方法。我们将在下面讨论其中一些方法。（更多细节见[Mur00]。）

请注意，可以对POMDP的潜在状态$$z_t$$进行边缘化，从而得出对下一个可观测状态的预测$$p(o_{t + 1}\vert \boldsymbol{h}_t, a_t)$$。这随后可以成为一个模型的学习目标，该模型经过训练后可直接预测未来的观测值，而无需明确引入潜在状态的概念。这被称为**预测状态表示（PSR）** [LS01]。这与可观测算子模型的概念[Jae00] 相关，也与我们将在4.4.2节中讨论的后继表示的概念相关。

#### 1.3.4.2 有限观测历史

处理部分可观测性问题的最简单方法是将状态定义为过去$$k$$个观测值的有限历史，$$s_t = \boldsymbol{h}_{t - k:t}$$；当观测值$$o_t$$是图像时，这通常被称为**帧堆叠**。然后我们可以使用标准的马尔可夫决策过程方法。不幸的是，这种方法无法捕捉数据中的长程依赖关系。

#### 1.3.4.3 有状态（循环）策略

一种更强大的方法是使用有状态策略，它可以记住整个过去的信息，而不仅仅是对当前输入或过去$$k$$帧做出反应。例如，我们可以像R2D2论文[Kap+18]中提出的那样，用循环神经网络（RNN）来表示策略，并且该方法也在许多其他论文中被使用。现在，RNN的隐藏状态$$z_t$$将隐含地总结过去的观测值$$\boldsymbol{h}_t$$，并且可以在任何标准的强化学习算法中替代状态$$s_t$$。

RNN策略被广泛使用，并且这种方法在解决部分可观测问题时通常很有效。然而，它们通常不会规划执行收集信息的动作，因为没有关于信念状态或不确定性的明确概念。不过，这种行为可以通过元学习来实现[Mik+20]。

### 1.3.5 软件

实现强化学习算法比实现监督学习方法，或者像语言建模和扩散这样的生成方法要棘手得多，因为后两者都有稳定（易于优化）的损失函数。因此，在现有软件基础上进行开发通常是明智之举，而不是从头开始。我们在1.3.5节列出了一些有用的库。

此外，强化学习实验的方差可能非常高，这使得很难得出有效的结论。有关一些推荐的实验实践，请参见[Aga+21b; Pat+24; Jor+24]。例如，在报告不同环境（具有不同的内在难度，比如不同类型的雅达利游戏）下的性能时，[Aga+21b]建议报告性能指标的**四分位均值（IQM）**，即位于0.25和0.75分位数之间样本的均值（这是截尾均值的一种特殊情况）。设这个估计值为$$\hat{\mu}(\mathcal{D}_i)$$，其中$$\mathcal{D}$$是第$$i$$次运行的经验数据（例如，奖励与时间的关系）。我们可以使用非参数方法（如自助重采样）或参数近似方法（如高斯近似）来估计这个估计值的不确定性。（这需要计算均值的标准误差$$\frac{\hat{\sigma}}{\sqrt{n}}$$，其中$$n$$是试验次数，$$\hat{\sigma}$$是（截尾后）数据的估计标准差。）

---
layout: mypost
title: k053 Gaze-LLE, Gaze Target Estimation via Large-Scale Learned Encoders
categories: [Transformer, Vision Transformer]
---

# 论文链接

[Arxiv Link](https://arxiv.org/abs/2412.09586)

[Github Link](https://github.com/fkryan/gazelle)

发表时间（Arxiv）：

[v1] Thu, 12 Dec 2024 18:55:30 UTC (10,240 KB)


# 论文重点与难点

## 1 **研究背景与问题**

- **背景**：视线目标估计（Gaze Target Estimation）是预测场景中人物注视的目标位置，对于理解人类行为、社交互动和人机交互具有重要意义。以往方法通过复杂的多分支架构融合场景特征、头部特征和多模态信号（如深度、姿态）来实现这一目标，但这些方法依赖于小规模数据集，训练复杂且效率低下。

- **问题**：能否利用大规模自监督预训练模型（如DINOv2）来简化视线目标估计的架构，并提升性能？

## 2 **Gaze-LLE框架**

- **核心思想**：提出了一种基于大规模预训练编码器（DINOv2）的新型架构Gaze-LLE，通过冻结预训练编码器并引入轻量级解码器来实现视线目标估计。该方法仅使用单一流程，避免了传统多分支架构的复杂性。

- **架构组成**：

- **冻结的场景编码器**：使用DINOv2提取场景特征，不进行微调。

- **头部位置提示（Head Position Prompting）**：通过在场景特征中注入与头部位置相关的提示，使模型能够针对特定人物进行视线解码。

- **轻量级解码器**：包含几个Transformer层和预测头，用于将场景特征解码为视线热图，并判断视线是否在画面内。

- **难点**：如何有效地将大规模预训练模型的特征应用于视线目标估计任务，同时避免复杂的数据融合和多任务训练。

## 3 **关键技术挑战与解决方案**

- **挑战1**：直接将DINOv2替换为传统方法中的场景编码器会导致性能下降。

- **解决方案**：设计了专门的解码器架构，通过头部位置提示和全局特征更新来适应视线估计任务。

- **挑战2**：如何在不微调预训练编码器的情况下，有效利用头部位置信息。

- **解决方案**：在场景特征提取后注入头部位置嵌入（embedding），而不是作为输入通道添加到编码器中。

- **挑战3**：传统卷积解码器在处理大规模特征时效果不佳。

- **解决方案**：采用Transformer解码器，利用其全局注意力机制更好地捕捉场景中的长距离视线目标。

## 4 **实验与结果**

- **数据集**：在GazeFollow、VideoAttentionTarget、ChildPlay和GOO-Real等数据集上进行了实验。

- **性能指标**：使用AUC（热图分类性能）和L2距离（预测目标与真实目标的欧几里得距离）进行评估。

- **关键结果**：

- Gaze-LLE在所有主要基准测试中均达到了**SOTA（State-of-the-Art）**性能，同时仅使用约5%的可训练参数。

- 在GazeFollow数据集上，使用ViT-L作为骨干网络的Gaze-LLE达到了$0.958$的AUC和$0.041$的最小L2距离。

- 在跨数据集测试中，Gaze-LLE展现出强大的泛化能力，无需针对特定数据集进行微调。

- **难点**：如何在不同数据集之间保持性能的稳定性，尤其是在领域差异较大的数据集（如GOO-Real）上。

## 5 **贡献与创新点**

- 提出了首个基于冻结预训练编码器的视线目标估计方法，证明了大规模预训练模型在该任务中的有效性。

- 设计了一种轻量级解码器架构，显著简化了模型结构，减少了可训练参数数量。

- 在多个基准测试中达到了SOTA性能，同时训练效率更高（在单个RTX 4090 GPU上训练时间少于1.5小时）。

- 提供了广泛的实验分析，验证了设计选择的合理性，并展示了模型的泛化能力。

## 6 **局限性与未来工作**

- **局限性**：模型性能依赖于预训练编码器的质量，且在处理头部背对相机或严重遮挡的场景时性能下降。

- **未来工作**：探索更强大的预训练模型，以及利用时间信息来解决视频数据中的模糊和遮挡问题。

# 论文详细讲解

## 1 **研究背景与动机**

视线目标估计（Gaze Target Estimation）旨在预测场景中人物注视的目标位置，对于理解人类行为和社交互动具有重要意义。传统方法依赖于复杂的多分支架构，分别提取头部特征、场景特征和多模态信号（如深度、姿态），然后通过复杂的融合机制进行预测。然而，这些方法存在以下问题：

- 训练复杂且效率低下，依赖于小规模数据集。

- 需要多个辅助模型，增加了计算开销。

- 多分支架构难以优化，收敛速度慢。

本文提出了一种基于大规模预训练编码器（如DINOv2）的新型架构 **Gaze-LLE**，通过冻结预训练编码器并引入轻量级解码器，简化了模型架构，同时提升了性能。

## 2 **Gaze-LLE框架**

Gaze-LLE 的核心思想是利用预训练的视觉特征提取器（如 DINOv2）作为冻结的场景编码器，并通过轻量级解码器进行视线目标估计。整体架构包括以下部分：

- **冻结的场景编码器**：

- 使用预训练的 DINOv2 模型作为场景编码器，提取输入图像的特征表示。

- 特征图尺寸为 $d_F \times H \times W$，通过线性层投影到目标维度 $d_{\text{model}}$，得到特征图 $x_F \in \mathbb{R}^{d_{\text{model}} \times H \times W}$。

- **头部位置提示（Head Position Prompting）**：

- 将头部位置信息以提示的形式注入到场景特征中，而不是作为输入通道添加到编码器中。

- 构建一个二值化掩码 $M$，表示头部在特征图中的位置，并将一个可学习的位置嵌入 $p_{\text{head}}$ 添加到包含头部的场景特征中：

$$
S = x_F + (M \ast p_{\text{head}})
$$

- 这种设计避免了对预训练编码器的微调，同时保留了头部位置信息。

- **轻量级解码器**：

- 包含几个 Transformer 层，利用自注意力机制更新场景特征。

- 输入为场景特征 $S$ 和可选的辅助任务标记（如判断视线是否在画面内）。

- 输出为视线热图和辅助任务的分类结果。

- **预测头（Prediction Heads）**：

- 将更新后的场景特征解码为视线热图，使用卷积层上采样到目标尺寸 $H_{\text{out}} \times W_{\text{out}}$。

- 对于辅助任务（如判断视线是否在画面内），使用 MLP 进行分类。

## 3 **关键技术与设计选择**

- **预训练编码器的选择**：

- 实验表明，直接将 DINOv2 替换为传统方法中的场景编码器会导致性能下降。Gaze-LLE 的关键在于设计轻量级解码器，使其能够适应预训练特征。

- 使用 DINOv2 的冻结特征，避免了复杂的多任务训练和特征融合。

- **头部位置提示的设计**：

- 将头部位置信息注入到场景特征中，而不是作为输入通道添加到编码器中。这种设计在冻结预训练编码器的情况下表现更好。

- 实验表明，头部位置提示对于多人物场景至关重要，但在单人物场景中，模型仍能通过场景特征预测视线目标。

- **解码器的设计**：

- 传统方法使用卷积层解码特征，但 Gaze-LLE 使用 Transformer 层，利用全局注意力机制捕捉场景中的长距离视线目标。

- Transformer 解码器在性能上优于卷积解码器，尤其是在处理复杂场景时。

## 4 **实验与结果**

- **数据集**：

- **GazeFollow**：包含约 10 个注释的图像，用于视线目标估计。

- **VideoAttentionTarget**：视频数据集，包含人物注视目标的注释。

- **ChildPlay**：专注于儿童视线行为的数据集。

- **GOO-Real**：零售环境中的视线目标数据集。

- **性能指标**：

- 使用 AUC（热图分类性能）和 L2 距离（预测目标与真实目标的欧几里得距离）进行评估。

- **关键结果**：

- 在 GazeFollow 数据集上，Gaze-LLE 使用 ViT-L 骨干网络达到了 **0.958** 的 AUC 和 **0.041** 的最小 L2 距离，超越了所有现有方法。

- 在 VideoAttentionTarget 数据集上，Gaze-LLE 达到了 **0.937** 的 AUC 和 **0.103** 的 L2 距离，表现出色。

- 在跨数据集测试中，Gaze-LLE 展现出强大的泛化能力，无需针对特定数据集进行微调。

- **训练效率**：

- Gaze-LLE 的训练时间显著缩短，仅需 **1.5 小时**（单个 RTX 4090 GPU），而传统方法需要数小时甚至数天。

## 5 **贡献与创新点**

- 提出了首个基于冻结预训练编码器的视线目标估计方法，证明了大规模预训练模型在该任务中的有效性。

- 设计了一种轻量级解码器架构，显著简化了模型结构，减少了可训练参数数量（仅约 5% 的传统方法参数）。

- 在多个基准测试中达到了 **SOTA（State-of-the-Art）** 性能，同时训练效率更高。

- 提供了广泛的实验分析，验证了设计选择的合理性，并展示了模型的泛化能力。

## 6 **局限性与未来工作**

- **局限性**：

- 模型性能依赖于预训练编码器的质量，且在处理头部背对相机或严重遮挡的场景时性能下降。

- 对于视频数据，当前模型无法利用时间信息来解决模糊和遮挡问题。

- **未来工作**：

- 探索更强大的预训练模型，进一步提升性能。

- 利用时间信息（如视频帧）来解决视频数据中的模糊和遮挡问题。

- 将 Gaze-LLE 应用于更多实际场景，如人机交互和自动驾驶。

## 7 **总结**

Gaze-LLE 通过利用大规模预训练编码器和轻量级解码器，简化了视线目标估计的架构，提升了性能，并显著提高了训练效率。这一方法为视线估计领域带来了新的思路，证明了预训练模型在密集预测任务中的潜力。

### 论文方法部分详细讲解

## 1 **问题定义**

给定一张RGB图像 $x_{\text{img}} \in \mathbb{R}^{3 \times H_{\text{in}} \times W_{\text{in}}}$ 和特定人物头部的边界框 $x_{\text{bbox}} \in \mathbb{R}^4$，目标是预测一个热图 $H \in [0, 1]^{H_{\text{out}} \times W_{\text{out}}}$，其中每个像素值表示该像素是视线目标的概率。部分数据集还要求预测一个值 $y \in [0, 1]$，表示人物的视线目标是否在画面内。

## 2 **模型架构**

Gaze-LLE 的整体架构由两部分组成：冻结的大规模通用场景编码器和轻量级的视线解码器模块。具体如下：

### 2.1 场景编码器（Scene Encoder）

- 使用预训练的视觉特征提取器 $F$（如 DINOv2）作为场景编码器，输入图像 $x_{\text{img}}$，输出低分辨率的特征图 $x_F \in \mathbb{R}^{d_F \times H \times W}$。

- 通过线性层将特征图投影到目标维度 $d_{\text{model}}$，得到特征图 $x_F \in \mathbb{R}^{d_{\text{model}} \times H \times W}$。

### 2.2 头部位置嵌入（Head Position Embedding）

- 构建一个二值化掩码 $M$，表示头部在特征图中的位置，并将一个可学习的位置嵌入 $p_{\text{head}} \in \mathbb{R}^{d_{\text{model}}}$ 添加到包含头部的场景特征中：

$$
S = x_F + (M \ast p_{\text{head}})
$$

- 这种设计允许模型在不微调预训练编码器的情况下，利用头部位置信息进行视线预测。

### 2.3 Transformer 层

- 使用一个小型可训练的 Transformer 模块 $T$，利用自注意力机制更新场景特征。

- 将特征图 $S$ 展平为场景标记列表 $[s_1, s_2, \dots, s_{H \times W}]$，并添加绝对二维正弦位置嵌入 $P$：

$$
T(S + P)
$$

- 默认情况下，Transformer 模块包含 3 层标准 Transformer 编码器。

### 2.4 预测头（Prediction Heads）

- 将更新后的特征图 $S'$ 重构为 $d_{\text{model}} \times H \times W$，并通过热图解码器 $D_{\text{hm}}$ 上采样到目标尺寸 $H_{\text{out}} \times W_{\text{out}}$，输出每个像素是视线目标的概率。

- 对于辅助任务（如判断视线是否在画面内），使用一个 2 层 MLP $D_{\text{in/out}}$ 输出分类结果。

## 3 **训练目标**

- 使用像素级二值交叉熵损失函数 $L_{\text{hm}}$ 训练热图预测。

- 对于辅助任务，使用二值交叉熵损失函数 $L_{\text{in/out}}$，总损失为：

$$
L = L_{\text{hm}} + \lambda L_{\text{in/out}}
$$

- 预训练的编码器 $F$ 在训练过程中保持冻结，模型的可训练参数主要集中在解码器部分。

## 4 **关键技术决策**

### 4.1 头部位置的注入时机

- 传统方法将头部位置作为额外通道输入到场景编码器中，但这种方法在冻结预训练编码器时效果不佳。

- Gaze-LLE 在特征提取后注入头部位置嵌入，显著提升了性能。

### 4.2 特征解码方式

- 传统方法使用卷积层解码特征，但这种方法在处理大规模预训练特征时效果有限。

- Gaze-LLE 使用 Transformer 层解码特征，利用全局注意力机制捕捉长距离视线目标，性能优于卷积解码器。

### 4.3 是否需要头部分支

- 传统方法使用单独的头部编码器提取头部特征，但 Gaze-LLE 证明，大规模预训练编码器已经能够捕捉足够的头部信息。

- 通过 Transformer 解码器，Gaze-LLE 在不使用头部分支的情况下达到了与使用头部分支相当的性能，进一步简化了架构。

## 5 **总结**

Gaze-LLE 通过冻结预训练的 DINOv2 编码器和轻量级的 Transformer 解码器，实现了高效的视线目标估计。该方法在多个基准测试中达到了 SOTA 性能，同时显著减少了可训练参数和训练时间。
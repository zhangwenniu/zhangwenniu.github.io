---
layout: mypost
title: 019 NeuMesh, Learning Disentangled Neural Mesh-based Implicit Field for Geometry and Texture Editing
categories: [论文阅读, Mesh重建, NeRF]
---

- [标题](#标题)
  - [链接](#链接)
  - [作者](#作者)
- [要点](#要点)
  - [目的](#目的)
  - [思想](#思想)
  - [方法](#方法)
- [想法](#想法)
  - [优点](#优点)
  - [缺点](#缺点)
- [后续要读的文章](#后续要读的文章)

# 标题

NeuMesh: Learning Disentangled Neural Mesh-based Implicit Field for Geometry and Texture Editing

2022, ECCV, 0 citations.

## 链接

论文链接：

- [Paper Page Link](https://arxiv.org/abs/2207.11911); [PDF Link](https://arxiv.org/pdf/2207.11911.pdf)
- [Project Link](https://zju3dv.github.io/neumesh/)

代码实现：

- [NeuMesh GitHub](https://github.com/zju3dv/neumesh)。

相关知识链接：

- 论文的一个贡献点是同时考虑了空间中的Mesh网格和纹理信息，并将几何结构和纹理表示解耦合，信息对应到空间中各个顶点上中。论文中提到UVMap，相关参考资料：[UVMap-Blog](https://blog.csdn.net/cgsmalcloud/article/details/114542644)


##  作者

> Bangbang Yang1∗, Chong Bao1∗, Junyi Zeng1, Hujun Bao1, Yinda Zhang2†, Zhaopeng Cui1†, and Guofeng Zhang1† 
>
> 
>
> 1 State Key Lab of CAD&CG, Zhejiang University 
>
> 2 Google

浙江大学计算机辅助设计和计算机图形学国家重点实验室的几个人。这篇文章的突出风格是，为了解决纹理编辑的目标任务，不断深入处理，引入多篇文章中的解决思路和处理办法，最终实现三维模型的隐式场纹理编辑。

计算机视觉希望能够得到一个通用的解决方法，计算机图形学可能更倾向于将一个问题做的比较好。

这篇文章算是神经隐式场表面重建工作的下游任务，并不致力于解决更好的表面重建效果。而是在默认表面已经有好的重建效果之后，使用特定的模型来解决实际的应用目标任务。


# 要点

## 目的


## 思想


## 方法


# 想法

## 优点



## 缺点



# 后续要读的文章

- [    ]  ARAP. As rigid as possible. 引用这篇文章的时候，是在说明移动某些顶点，以进行网格编辑。需要交互式的移动某些顶点，使用" out-of-box mesh deforming methods"。out of box是论文中一直没有明确说明的问题。
  - As-rigid-as-possible Surface Modeling. In: Symposium on Geometry Processing.
  - 47. Sorkine, O., Alexa, M.: As-rigid-as-possible Surface Modeling. In: Symposium on Geometry Processing. vol. 4, pp. 109–116 (2007) 7, 9, 22
- [    ] SGPN. 基于点的实例分割的文章。文章中用这篇工作的方法，选择两个苹果对应的三维模型的区域，进行纹理交换。2018年的文章。
  - SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation. 
  - 58. Wang, W., Yu, R., Huang, Q., Neumann, U.: SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2569–2578 (2018) 9, 24


---
layout: mypost
title: k032 U2NeRF, Unifying Unsupervised Underwater Image Restoration and Neural  Radiance Fields
categories: [3DGS, 水下]
---

如果有研究水下3D重建的研究者，可以通过邮箱zhangwenniu@163.com联系我，我目前创建了一个水下3D重建的研究者交流群，欢迎感兴趣的研究者加入。

# 论文链接

- [Arxiv Link](https://www.arxiv.org/abs/2411.16172)

- [GitHub Link](https://github.com/Vinayak-VG/U2NeRF)

- [Project Link](https://vinayak-vg.github.io/U2NeRF/)

发表时间：[v1] Mon, 25 Nov 2024 08:02:28 UTC (36,494 KB)

# 重点难点讲解

## 论文重点

1. **提出了一种新的无监督水下图像恢复和视图合成方法**：U2NeRF（Unsupervised Underwater Neural Radiance Field），基于Transformer架构，能够同时进行水下图像的视图渲染和恢复。

2. **创新性地将神经辐射场（NeRF）与水下图像恢复结合**：通过将预测的颜色分解为场景辐射、直接传输图、后向散射传输图和全局背景光四个分量，并结合这些分量重建水下图像，实现了自监督训练。

3. **发布了新的水下视图合成（UVS）数据集**：包含12个水下场景，既有合成数据也有真实数据，为水下图像恢复和视图合成提供了新的基准。

4. **实验结果表明U2NeRF性能优越**：在单场景优化时，U2NeRF在感知质量（LPIPS ↓11%）、色彩恢复（UIQM ↑5%、UCIQE ↑4%）等指标上优于多个基线方法，并且在跨场景泛化能力上也表现出色。

## 论文难点

1. **无监督学习的挑战**：

   - 水下图像恢复通常需要大量标注数据来训练模型，但真实场景中很难获取高质量的标注数据。U2NeRF采用自监督学习，需要设计合适的损失函数来指导模型训练，例如重建损失$L_{\text{rec}}$、对比度增强损失$L_{\text{con}}$等，以确保模型能够有效地学习恢复图像。

   - 在没有干净的真值图像的情况下，如何让模型自动学习到物理上有意义的图像分量（如场景辐射、传输图等），并正确地组合这些分量来重建图像，是一个关键的难点。

2. **多视图几何与图像恢复的结合**：

   - U2NeRF需要同时处理多视图几何信息和图像恢复任务。如何在渲染新视图的同时，有效地利用多视图信息来恢复图像质量，是一个复杂的问题。例如，模型需要在预测图像块时提供足够的空间上下文，以便进行自动恢复。

   - 不同视图之间的光照条件和水下环境差异较大，如何在这些复杂条件下保持一致的恢复效果，是该方法需要解决的难点之一。

3. **模型复杂度与性能的平衡**：

   - U2NeRF基于Transformer架构，模型复杂度较高。在实际应用中，如何在保证恢复和渲染质量的同时，降低模型的计算复杂度和训练成本，是一个重要的问题。例如，论文中提到的图像块大小$p$的选择，需要在性能和复杂度之间找到平衡。

   - 在跨场景泛化时，模型需要在有限的训练数据上学习到通用的特征表示，同时在新场景上能够快速适应并优化，这对模型的泛化能力和适应性提出了较高的要求。

4. **物理模型的隐式学习**：

   - U2NeRF需要隐式地学习物理上有意义的图像分量（如直接传输图和后向散射传输图），这些分量与水下成像的物理过程密切相关。如何让模型在没有显式监督的情况下，自动学习到这些物理分量，并且能够正确地重建出高质量的图像，是一个具有挑战性的问题。

# 论文详细讲解

U2NeRF - Unifying Unsupervised Underwater Image Restoration and Neural Radiance Fields

## 1. 研究背景与动机

水下图像由于光的散射、吸收和折射，常常存在色彩偏移、对比度低和模糊等问题。这些问题严重影响了水下图像在可视化和下游任务（如目标检测、跟踪等）中的应用。因此，开发能够增强水下图像的方法具有重要意义。近年来，深度学习在图像恢复和视图合成领域取得了显著进展，但大多数方法依赖于合成数据，难以处理真实世界中的复杂退化问题。此外，现有的神经辐射场（NeRF）方法虽然在视图合成中表现出色，但主要针对干净、高分辨率的图像场景，无法直接应用于水下图像恢复。

为了解决这些问题，本文提出了一种新的方法——**U2NeRF（Unsupervised Underwater Neural Radiance Field）**，它结合了Transformer架构和物理模型，能够在无监督的情况下同时进行水下图像的视图渲染和恢复。

## 2. 研究方法

### 2.1 U2NeRF框架概述

U2NeRF基于**Generalizable NeRF Transformer (GNT)** 架构，通过多视图几何信息和Transformer模块，实现从多个视角合成新视图的同时恢复图像质量。具体来说，U2NeRF将预测的颜色分解为四个分量：
 
1. **场景辐射（Scene Radiance, $J$）**：表示场景的真实颜色。

2. **直接传输图（Direct Transmission Map, $T_D$）**：表示光从场景直接到达相机的比例。

3. **后向散射传输图（Backscatter Transmission Map, $T_B$）**：表示由水体散射引起的光的比例。

4. **全局背景光（Global Background Light, $A$）**：表示场景中的背景光照。

这四个分量结合水下成像的物理模型，可以重建原始水下图像：

$$
I(i) = J(i) \cdot T_D(i) + (1 - T_B(i)) \cdot A
$$

通过这种方式，U2NeRF能够在没有干净真值图像的情况下，以自监督的方式训练模型。

### 2.2 核心模块与技术

U2NeRF的主要模块包括：

1. **View Transformer**：用于聚合多视图图像特征，将不同视角的特征对齐并融合。

2. **Ray Transformer**：沿光线方向对特征进行采样和组合，生成目标像素的颜色。

3. **图像分量预测**：通过卷积和上采样层，将特征图预测为图像块，并分解为场景辐射、传输图和背景光等分量。

4. **自监督损失函数**：为了实现无监督训练，U2NeRF设计了多种损失函数，包括重建损失$L_{\text{rec}}$、对比度增强损失$L_{\text{con}}$、颜色一致性损失$L_{\text{col}}$等。

### 2.3 物理模型与自监督训练

U2NeRF的核心思想是利用水下成像的物理模型，将图像分解为多个物理上有意义的分量。这些分量不仅有助于恢复图像质量，还为模型提供了足够的空间上下文信息，使得模型能够在没有干净真值的情况下进行训练。例如，通过最小化重建误差和正则化各项损失，模型能够学习到如何恢复场景辐射、减少散射和背景光的影响。

## 3. 实验与结果

### 3.1 数据集

为了验证U2NeRF的性能，作者构建了一个新的**Underwater View Synthesis (UVS) 数据集**，包含12个水下场景，分为三个难度级别：

- **Easy Split**：4个合成水下场景，基于LLFF数据集并添加水下退化效果。

- **Medium Split**：4个高质量的真实水下场景。

- **Hard Split**：4个低质量的真实水下场景，包含更多噪声和复杂场景。

### 3.2 基线方法

为了对比U2NeRF的性能，作者选择了多种基线方法：

1. **NeRF**：标准的神经辐射场方法，直接在原始水下图像上训练。

2. **NeRF + Clean**：先用NeRF渲染，再用现有的水下图像恢复方法进行后处理。

3. **Clean + NeRF**：先对输入图像进行恢复，再用NeRF进行渲染。

4. **UIESS** 和 **UPIFM**：现有的水下图像恢复方法，不涉及视图合成。


### 3.3 评估指标

作者使用了多种指标来评估渲染和恢复质量：

- **PSNR（峰值信噪比）** 和 **SSIM（结构相似性）**：用于评估渲染质量。

- **LPIPS（感知图像相似性）**：用于评估感知质量。

- **UIQM（水下图像质量度量）** 和 **UCIQE（水下色彩图像质量评估）**：用于评估恢复质量。

### 3.4 实验结果

在单场景优化实验中，U2NeRF在所有指标上均优于基线方法。例如：

- 在**Easy Split**中，U2NeRF的LPIPS分数比NeRF低20%，表明其在感知质量上显著优于其他方法。

- 在**Medium Split**和**Hard Split**中，U2NeRF在LPIPS、UIQM和UCIQE等指标上分别提升了11%、5%和4%，证明了其在复杂场景下的优越性能。


此外，U2NeRF还展示了良好的跨场景泛化能力。在未见过的场景上，预训练的U2NeRF模型能够生成高质量的视图，并且在微调后性能进一步提升。

## 4. 关键结论

U2NeRF通过结合神经辐射场和物理模型，成功地实现了水下图像的无监督视图渲染和恢复。其主要贡献包括：

1. 提出了一种新的方法，能够在无监督的情况下同时进行视图合成和图像恢复。

2. 设计了基于Transformer的架构，能够有效利用多视图几何信息。

3. 通过分解图像为物理分量，实现了自监督训练，并在多个指标上优于现有方法。

4. 提供了一个新的水下视图合成数据集，为未来的研究提供了基准。

## 5. 未来工作

尽管U2NeRF在水下图像恢复和视图合成方面取得了显著进展，但作者也指出了其局限性。例如，在处理包含显著运动的场景时，模型可能会产生模糊。未来的工作可以集中在解决这些问题，例如引入动态场景建模或进一步优化物理模型的隐式学习。

## 论文方法部分详细讲解

### 1. 研究目标

本文提出了一种无监督的水下图像恢复和视图合成方法——**U2NeRF（Unsupervised Underwater Neural Radiance Field）**，旨在同时解决水下图像的视图渲染和恢复问题。具体来说，U2NeRF通过将神经辐射场（NeRF）与水下成像的物理模型相结合，能够在没有干净真值图像的情况下，自监督地学习恢复图像。

### 2. 方法概述

U2NeRF基于**Generalizable NeRF Transformer (GNT)**架构，通过多视图几何信息和Transformer模块，实现从多个视角合成新视图的同时恢复图像质量。其核心思想是将预测的颜色分解为四个物理上有意义的分量：

- 场景辐射（Scene Radiance, $J$）：表示场景的真实颜色。

- 直接传输图（Direct Transmission Map, $T_D$）：表示光从场景直接到达相机的比例。

- 后向散射传输图（Backscatter Transmission Map, $T_B$）：表示由水体散射引起的光的比例。

- 全局背景光（Global Background Light, $A$）：表示场景中的背景光照。

这四个分量结合水下成像的物理模型，可以重建原始水下图像：

$$
I(i) = J(i) \cdot T_D(i) + (1 - T_B(i)) \cdot A
$$

通过这种方式，U2NeRF能够在没有干净真值图像的情况下，以自监督的方式训练模型。

### 3. 核心模块与技术

#### 3.1 神经辐射场（NeRF）基础

NeRF将3D场景建模为一个连续的辐射场$F: (x, \theta) \mapsto (c, \sigma)$，其中$x \in \mathbb{R}^3$表示空间坐标，$\theta \in [-\pi, \pi]^2$表示视图方向，$c \in \mathbb{R}^3$表示颜色，$\sigma \in \mathbb{R}^+$表示密度。通过体积渲染技术，NeRF可以合成从新视角观察的图像：

$$
C(r\vert \Theta) = \int_{t_n}^{t_f} T(t) \sigma(r(t)) c(r(t), d) \, dt
$$

其中，$T(t) = \exp\left(-\int_{t_n}^t \sigma(s) \, ds\right)$，$r(t) = o + td$，$t_n$和$t_f$分别表示近平面和远平面。

#### 3.2 Generalizable NeRF Transformer (GNT)

GNT将视图合成问题分为两个阶段：

1. **多视图图像特征融合**：通过View Transformer聚合多视图信息。

2. **采样基础的渲染集成**：通过Ray Transformer沿光线方向组合特征。

具体来说，GNT的操作可以总结为：

$$
F(x, \theta) = \text{View-Transformer}(F_1(\Pi_1(x), \theta), \dots, F_N(\Pi_N(x), \theta))
$$

其中，$\Pi_i(x)$将位置$x$投影到第$i$个图像平面上，$F_i(z, \theta)$通过双线性插值计算特征向量。

#### 3.3 U2NeRF的改进

U2NeRF在GNT的基础上进行了以下改进：

1. **图像块预测**：与传统NeRF预测单个像素不同，U2NeRF预测图像块（patch），提供足够的空间上下文信息用于恢复。

2. **分量分解**：将图像块分解为场景辐射、直接传输图、后向散射传输图和全局背景光四个分量。

3. **自监督训练**：通过设计多种损失函数，使模型能够在没有干净真值图像的情况下进行训练。

### 4. 损失函数设计

为了实现无监督训练，U2NeRF设计了以下损失函数：

#### 4.1 重建损失（Reconstruction Loss）

重建损失用于监督图像重建过程，通过最小化预测图像$x$和真实图像$I$之间的均方误差（MSE）来实现：

$$
L_{\text{rec}} = \|I - x\|_2^2
$$

#### 4.2 对比度增强损失（Contrast Enhancement Loss）

对比度增强损失用于监督场景辐射图$J$，通过最小化亮度$V(J(x))$和饱和度$S(J(x))$之间的差异来增强图像对比度：

$$
L_{\text{con}} = \|V(J(x)) - S(J(x))\|_2^2
$$

#### 4.3 颜色一致性损失（Color Constancy Loss）

颜色一致性损失用于纠正恢复图像中的颜色不一致性，基于Gray-World颜色一致性理论：

$$
L_{\text{col}} = \sum_{c \in \{R, G, B\}} \|\mu(J_c) - 0.5\|_2^2
$$

其中，$\mu(J_c)$表示颜色通道$c$的平均强度值。

#### 4.4 全局一致性损失（Global Consistency Loss）

全局一致性损失用于平滑全局背景光$A$，通过最小化相邻像素之间的差异来实现：

$$
L_{\text{glob}} = \sum_{i} \|A_i - A_{i+1}\|_2^2
$$

#### 4.5 传输一致性损失（Transmission Consistency Loss）

传输一致性损失用于监督后向散射传输图$T_B$，确保其在不同颜色通道之间保持一致：

$$
L_{\text{trans}} = \sum_{(c_1, c_2) \in \epsilon} \|\log T_{c_1} - \log T_{c_2} - \mu(\log T_{c_1} - \log T_{c_2})\|_2^2
$$

其中，$\epsilon = \{(R, G), (R, B), (G, B)\}$。

#### 4.6 总损失函数

U2NeRF的总损失函数是上述损失函数的加权和：

$$
L = \lambda_1 L_{\text{rec}} + \lambda_2 L_{\text{con}} + \lambda_3 L_{\text{col}} + \lambda_4 L_{\text{glob}} + \lambda_5 L_{\text{trans}}
$$

其中，$\lambda_i$是权重系数，用于平衡不同损失的贡献。

### 5. 网络架构

U2NeRF的网络架构基于GNT，具体包括：

1. **特征提取网络**：使用类似U-Net的架构，以ResNet34作为编码器，两层上采样作为解码器。

2. **View Transformer**：包含单头交叉注意力机制，用于聚合多视图特征。

3. **Ray Transformer**：包含多头自注意力机制，用于沿光线方向组合特征。

4. **输出头**：通过卷积和上采样层将特征图预测为图像块，并分解为四个分量。

### 6. 训练与优化

U2NeRF使用Adam优化器进行端到端训练，训练过程中：

1. **源视图和目标视图采样**：随机选择目标视图，并从附近视图中采样源视图，模拟不同视图密度。

2. **损失函数优化**：通过最小化总损失函数$L$，训练网络参数。

3. **超参数设置**：权重系数$\lambda_i$和学习率根据实验结果进行调整。

### 7. 方法总结

U2NeRF通过将神经辐射场与水下成像的物理模型相结合，提出了一种无监督的水下图像恢复和视图合成方法。其核心贡献包括：

1. 提出了一种新的架构，能够在无监督的情况下同时进行视图渲染和图像恢复。

2. 设计了多种损失函数，用于自监督训练，确保模型能够学习到物理上有意义的图像分量。

3. 通过预测图像块而非单个像素，提供足够的空间上下文信息，增强模型的恢复能力。

这种方法为水下图像处理提供了一种新的思路，特别是在缺乏干净真值图像的情况下，能够有效地恢复和渲染高质量的水下图像。


# 关键问题讲解

## UIQM（Underwater Image Quality Measure）详解

### 1. **背景与动机**

水下图像由于受到光的散射、吸收以及复杂水体环境的影响，常常存在色彩失真、对比度低、模糊等问题，严重影响了图像的视觉质量和后续应用（如目标检测、分类等）。为了评估水下图像的质量，需要一种专门的图像质量评价指标，这就是**UIQM（Underwater Image Quality Measure）**。

### 2. **UIQM的定义**

UIQM是一种综合性的水下图像质量评价指标，由**Karen Panetta等人**在2015年提出。它结合了多个子指标，从不同角度评估水下图像的质量，主要包括以下几个方面：

- **色彩饱和度（Color Saturation）**

- **对比度（Contrast）**

- **锐度（Sharpness）**

通过这些子指标的加权组合，UIQM能够全面反映水下图像的视觉质量。

### 3. **UIQM的计算公式**

UIQM的计算公式为：

$$
\text{UIQM} = \frac{1}{3} \left( \text{UICM} + \text{UISM} + \text{UIConM} \right)
$$

其中：

- **UICM（Underwater Image Colorfulness Measure）**：用于评估图像的色彩饱和度。

- **UISM（Underwater Image Sharpness Measure）**：用于评估图像的锐度。

- **UIConM（Underwater Image Contrast Measure）**：用于评估图像的对比度。

以下分别介绍这三个子指标的计算方法。

#### 3.1 UICM（色彩饱和度）

UICM用于评估图像的色彩饱和度，计算公式为：

$$
\text{UICM} = \alpha_1 \cdot \text{Saturation} + \alpha_2 \cdot \text{Contrast}
$$

其中：

- **Saturation**：通过计算图像在RGB颜色空间中的色彩饱和度来衡量。具体公式为：

  $$
  \text{Saturation} = \frac{1}{N} \sum_{i=1}^{N} \left( \frac{(R_i - G_i)^2 + (R_i - B_i)^2 + (G_i - B_i)^2}{3} \right)^{1/2}
  $$

  其中，$R_i, G_i, B_i$分别表示像素$i$的红、绿、蓝通道值，$N$为图像的总像素数。

- **Contrast**：通过计算图像的亮度对比度来衡量。具体公式为：

  $$
  \text{Contrast} = \frac{1}{N} \sum_{i=1}^{N} \left( \frac{(R_i + G_i + B_i)}{3} - \mu \right)^2
  $$

  其中，$\mu$为图像的平均亮度。

#### 3.2 UISM（锐度）

UISM用于评估图像的锐度，计算公式为：

$$
\text{UISM} = \frac{1}{N} \sum_{i=1}^{N} \left( \text{Sobel}(I_i) \right)^2
$$

其中：

- **Sobel($I_i$)**：通过Sobel算子计算图像在像素$i$处的梯度强度，反映图像的边缘信息。

- $N$为图像的总像素数。

#### 3.3 UIConM（对比度）

UIConM用于评估图像的对比度，计算公式为：

$$
\text{UIConM} = \frac{1}{N} \sum_{i=1}^{N} \left( \frac{(R_i - \mu_R)^2 + (G_i - \mu_G)^2 + (B_i - \mu_B)^2}{3} \right)
$$

其中：

- $\mu_R, \mu_G, \mu_B$分别表示图像在红、绿、蓝通道的平均值。

- $N$为图像的总像素数。

### 4. **UIQM的特点**

- **综合性**：UIQM结合了色彩饱和度、锐度和对比度等多个子指标，能够全面反映水下图像的质量。

- **无参考性**：UIQM是一种无参考质量评价指标，不需要与任何参考图像进行比较，适用于没有真值图像的场景。

- **适应性**：UIQM专门针对水下图像的特点设计，能够有效评估水下图像的视觉质量，适用于水下图像恢复、增强等任务。

### 5. **UIQM的应用**

UIQM在水下图像处理领域得到了广泛应用，主要用于以下方面：

- **水下图像恢复**：用于评估恢复算法的性能，比较不同方法的恢复质量。

- **水下图像增强**：用于优化增强算法的参数，确保增强后的图像具有更好的视觉质量。

- **水下目标检测与识别**：通过评估图像质量，选择高质量的图像用于目标检测和识别任务。

### 6. **总结**

UIQM是一种专门用于评估水下图像质量的指标，通过综合考虑色彩饱和度、锐度和对比度，能够全面反映图像的视觉质量。它在水下图像恢复、增强和目标检测等领域得到了广泛应用，为水下图像处理提供了重要的评价工具。


# 原文翻译

U2NeRF: 统一无监督水下图像恢复与神经辐射场

Vinayak Gupta\(^1\)*, Manoj S\(^1\)*, Mukund Varma T\(^1\)†*, Kaushik Mitra\(^1\)†  
\(^1\)印度理工学院马德拉斯分校  

{vinayakguptapokal, mukundvarmat, manoj.s.2908}@gmail.com, kmitra@ee.iitm.ac.in

## 摘要

水下图像由于光的吸收、折射、散射而导致色彩偏移、对比度低和模糊，这些问题引起了广泛关注。在这项工作中，我们提出了一种无监督水下神经辐射场（U2NeRF），这是一种基于Transformer的架构，能够在多视图几何的条件下同时学习渲染和恢复新视图。由于缺乏监督，我们尝试将恢复能力隐式地嵌入到NeRF管道中，并将预测的颜色分解为多个分量——场景辐射、直接传输图、后向散射传输图和全局背景光，然后将这些分量组合起来以自监督的方式重建水下图像。此外，我们发布了一个包含12个水下场景的水下视图合成（UVS）数据集，其中包含合成生成的数据和真实世界的数据。我们的实验表明，当在单个场景上优化时，U2NeRF在多个基线方法上表现优越，LPIPS降低了11%，UIQM提高了5%，UCIQE提高了4%（平均值），并展示了改进的渲染和恢复能力。代码将在接受后发布。

## 1. 引言

水下图像由于水中复杂的光照条件而遭受退化，具体表现为光的散射、吸收和折射[11]。因此，开发能够增强水下图像的方法具有重要意义，以便使其更适合于可视化以及下游任务（如检测、跟踪等）。近年来，深度学习在许多计算机视觉任务中取得了显著进展[10, 13]，包括水下图像增强[7, 12]。然而，由于缺乏大规模真实世界的水下图像数据集及其对应的恢复图像真值，大多数方法[9, 39]依赖于合成训练数据。然而，合成数据可能无法捕捉到真实世界中的复杂退化，从而导致域偏移[7]。最近，“零样本”方法[4, 15]在测试时训练了一个小的图像特定网络，并且除了输入图像本身外不使用任何监督。然而，由于测试时需要大量的优化迭代，这些方法并不适合实际应用。

神经辐射场（NeRF）及其后续工作[1, 6, 21]在新视图合成方面取得了显著成功，能够生成逼真、高分辨率且视图一致的场景。然而，这些方法都是在包含干净、高分辨率图像的场景上进行训练的。由于NeRF能够整合多视图信息，我们假设这些方法在多帧图像恢复任务中具有很大的潜力——在本文中，即水下图像增强。

在本文中，我们尝试使用NeRF同时进行新视图的渲染和恢复。然而，大多数方法在像素级别上操作，限制了其自动恢复预测颜色的能力。我们证明了通过预测图像块（而非像素），可以为恢复提供足够的空间上下文。受[4]的启发，我们的方法将预测的图像块分解为4个分量，分别是场景辐射、直接传输图、后向散射传输图和全局背景光。这些分量随后被组合以重建原始图像，并且通过适当的正则化，使我们的网络能够在没有干净真值图像的情况下以自监督的方式进行训练。为此，我们采用了最近提出的通用NeRF Transformer（GNT）[30]，它由一个视图变换器组成，用于聚合多视图信息，并通过光线变换器沿光线合成颜色来渲染新视图。我们的方法被称为U2NeRF（无监督水下神经辐射场），在完全无监督的设置中训练，能够同时渲染和恢复新视图。我们的主要贡献可以总结如下：

1. 我们将辐射场的概念扩展到同时渲染和恢复新视图的新任务中，特别是针对水下场景。

2. 我们提出的U2NeRF方法通过增强现有的辐射场的空间感知能力，并结合物理信息的图像形成模型，成功恢复了水下图像。

3. 我们贡献了一个新的UVS数据集，包含12个水下场景，涵盖合成生成的数据和真实世界的数据，用于新视图合成。我们的方法在感知（LPIPS ↓11%）和色彩恢复指标（UIQM ↑5%，UCIQE ↑4%）上取得了最佳性能。

4. 我们的实验结果表明，U2NeRF能够隐式地生成具有物理意义的图像分量，使我们更接近于将Transformer作为一种通用的图形建模工具。

## 2. 相关工作

### 神经辐射场（NeRF）

NeRF由[20]提出，通过将每个场景拟合为一个连续的5D辐射场（由多层感知机MLP参数化），从而合成一致且逼真的新视图。自NeRF提出以来，许多工作对其进行了改进。例如，Mip-NeRF[1, 2]高效地解决了无界场景中物体尺度的问题；Nex[35]模拟了大视图依赖效应；其他工作[22, 32, 38]改进了表面表示，扩展到动态场景[24, 25, 27]，引入了光照和反射建模[5, 31]，或者利用深度信息从少量视图进行回归[8, 36]。最近的一项工作[26]展示了NeRF在突发降噪中的能力。与这些方法不同，我们的工作旨在同时渲染和恢复新视图，特别是在水下场景的背景下。

### 水下图像增强

为了将我们的贡献与现有的水下图像增强工作进行比较，需要考虑两个关键因素：是否涉及监督，以及模型是否参考了某种物理模型。

## 3. 无监督水下神经辐射场

我们的方法U2NeRF扩展了GNT，用于水下场景的新视图渲染和恢复任务。在本节中，我们首先介绍辐射场和GNT的基础知识，然后详细描述我们提出的方法。

### 3.1 基础知识

#### 神经辐射场（NeRF）

NeRF将多视图图像转换为一个辐射场，并通过从新角度重新渲染该辐射场来插值新视图。具体来说，NeRF将底层3D场景建模为一个连续的辐射场$F: (x, \theta) \mapsto (c, \sigma)$，该场由多层感知机（MLP）$\Theta$参数化，将空间坐标$x \in \mathbb{R}^3$与视图方向$\theta \in [-\pi, \pi]^2$映射到颜色$c \in \mathbb{R}^3$和密度$\sigma \in \mathbb{R}^+$。为了形成图像，NeRF执行基于光线的渲染，从光学中心$o \in \mathbb{R}^3$向每个像素（沿方向$d \in \mathbb{R}^3$）发射光线$r = (o, d)$，然后利用体积渲染[14]在近平面和远平面之间沿光线合成颜色和密度：

$$
C(r\vert \Theta) = \int_{t_n}^{t_f} T(t) \sigma(r(t)) c(r(t), d) \, dt \quad (1)
$$

其中，$T(t) = \exp \left( -\int_{t_n}^t \sigma(s) \, ds \right)$，$r(t) = o + td$，$t_n$和$t_f$分别是近平面和远平面。在实际中，公式(1)通过数值积分方法[17]进行估计。

给定从已知相机参数的周围视图捕获的图像，NeRF通过最大化模拟结果的似然来拟合辐射场。假设我们收集了所有光线和像素颜色的配对作为训练集$D = \{(r_i, \hat{C}_i)\}_{i=1}^N$，其中$N$是采样的光线总数，$\hat{C}_i$表示第$i$条光线的真实颜色，则我们通过以下损失函数训练隐式表示$\Theta$：

$$
L(\Theta \vert  R) = \mathbb{E}_{(r, \hat{C}) \in D} \|C(r\vert \Theta) - \hat{C}(r)\|_2^2 \quad (2)
$$

#### 通用NeRF Transformer（GNT）

GNT[30]将新视图合成问题视为一个两阶段信息处理过程：多视图图像特征融合，随后是基于采样的渲染集成。它由以下两部分组成：(i) 视图变换器（view transformer），用于从对应的极线（epipolar lines）聚合像素对齐的图像特征，以预测坐标级特征；(ii) 光线变换器（ray transformer），通过注意力机制沿追踪的光线组合坐标级点特征。更正式地，整个操作可以总结如下：

$$
F(x, \theta) = \text{View-Transformer}(F_1(\Pi_1(x), \theta), \dots, F_N(\Pi_N(x), \theta)) \quad (3)
$$

其中，$\text{View-Transformer}(\cdot)$是一个Transformer编码器，$\Pi_i(x)$通过应用外参矩阵将位置$x \in \mathbb{R}^3$投影到第$i$个图像平面上，$F_i(z, \theta) \in \mathbb{R}^d$通过在特征网格上进行双线性插值计算位置$z \in \mathbb{R}^2$处的特征向量。多视图聚合的点特征被送入光线变换器，光线变换器的输出被送入视图变换器，此过程交替重复，视图变换器和光线变换器交替堆叠。然后从最后一个光线变换器中提取单个光线特征以预测目标像素颜色：

$$
C(r) = \text{MLP} \circ \text{Mean} \circ \text{Ray-Transformer}(F(o + t_1 d, \theta), \dots, F(o + t_M d, \theta)) \quad (4)
$$

其中，$t_1, \dots, t_M$在近平面和远平面之间均匀采样，$\text{Ray-Transformer}$是一个标准的Transformer编码器。

### 3.2 将恢复能力嵌入U2NeRF

NeRF将3D场景表示为一个辐射场$F: (x, \theta) \mapsto (c, \sigma)$，其中每个空间坐标$x \in \mathbb{R}^3$与视图方向$\theta \in [-\pi, \pi]^2$被映射到颜色$c \in \mathbb{R}^3$和密度$\sigma \in \mathbb{R}^+$。然而，单个像素无法提供足够的上下文用于自动恢复。在我们的工作中，我们首先将GNT适应于渲染大小为$p$的图像块。从光线变换器块获得的最终光线特征被传递到一系列卷积和上采样层。

受[4]的启发，我们将水下图像分解为几个分量——场景辐射（$J$）、全局背景光（$A$）以及退化分量——直接传输图（$T_D$）和后向散射传输图（$T_B$），分别用于描述衰减和光反射。这些分量可以组合起来重建像素$i$处的原始图像$I$：

$$
I(i) = J(i)T_D(i) + (1 - T_B(i))A \quad (5)
$$

这使得我们的网络能够在没有真值图像的情况下以完全自监督的方式进行训练。为了预测$J$、$T_D$和$T_B$，我们初始化了单独的输出头，将最终光线特征投影到所需的块大小。由于$A$与输入图像内容无关，我们将目标视图方向上最近的源图像传递给变分自编码器（VAE），以估计全局背景光。

除了公式(2)中的光度损失（$L_{\text{rec}}$），我们还：

1. 最小化编码特征$z$与VAE中从高斯分布采样的潜在代码$\hat{z}$之间的差异（$L_{\text{kl}}$）；

2. 最小化预测场景辐射的饱和度与亮度之间的差异以减少雾气（$L_{\text{con}}$）；

3. 最小化场景辐射中的潜在颜色偏差（$L_{\text{col}}$）；

4. 确保跨通道的后向散射系数保持一致（$L_{\text{trans}}$）；

5. 通过最小化每个局部邻域内的方差，强制全局背景光保持恒定（$L_{\text{glob}}$），如原始论文[4]中所提议的。

综合起来，网络被训练以优化以下目标函数：

$$
L = \lambda_1 L_{\text{rec}} + \lambda_2 L_{\text{con}} + \lambda_3 L_{\text{col}} + \lambda_4 L_{\text{kl}} + \lambda_5 L_{\text{trans}} + \lambda_6 L_{\text{glob}} \quad (6)
$$

其中，$\lambda$表示每个损失项的权重。

## 4. 实验

我们进行了广泛的实验，以比较U2NeRF与几种基线方法在水下场景的新视图合成和恢复方面的性能。我们首先在单场景训练设置中提供定性和定量结果，随后将我们的方法扩展到对未见场景的泛化。

### 4.1 实现细节

**源视图和目标视图采样**  

如[33]所述，我们通过首先选择一个目标视图，然后识别一个包含$k \times N$个附近视图的池，从中随机采样$N$个视图作为源视图，从而构建源视图和目标视图的训练对。这种采样策略在训练过程中模拟了不同的视图密度，从而帮助网络更好地泛化。在训练期间，$k$和$N$的值分别从区间$[1-3]$和$[8-12]$中均匀随机采样。

**网络架构**  

为了从源视图中提取特征，我们使用了类似U-Net的架构，其中包含ResNet34编码器，后接两个上采样层作为解码器[33]。每个视图变换器块包含一个单头交叉注意力层，而光线变换器块包含一个具有四个头的多头自注意力层。这些注意力层的输出被传递到相应的前馈块中，采用ReLU激活函数，隐藏维度为256。在每一层中，预归一化输入（LayerNorm）和输出之间应用了残差连接。在我们所有的单场景实验中，我们交替堆叠了4个视图变换器块和光线变换器块，而在更大规模的泛化实验中，我们分别使用了8个块。所有变换器块（视图和光线）的维度均为64。为了在性能和网络复杂度之间取得平衡，我们将图像块大小$p$设置为4。VAE网络包含4个卷积层，编码器的维度分别为$[16, 32, 64, 128]$，每层后接ReLU激活函数。编码后的输入被投影到一个100维的特征向量上，然后进行高斯重采样。采样后的高斯潜在变量随后被传递到一个3层的解码器网络中，其维度为$[128, 64, 32]$，用于预测全局背景光$A$。

**训练/推理细节**  

我们使用Adam优化器对特征提取网络和U2NeRF进行端到端训练，以最小化公式(6)中的损失函数。我们经验性地将权重$\lambda$设置为：$\lambda_1 = 1, \lambda_2 = 0.1, \lambda_3 = 1, \lambda_4 = 1, \lambda_5 = 0.1, \lambda_6 = 0.1$。特征提取网络和U2NeRF的基础学习率分别为$10^{-3}$和$5 \times 10^{-4}$，并且随着训练步骤的增加而指数衰减。在微调期间，我们使用较小的学习率$5 \times 10^{-4}$和$2 \times 10^{-4}$对特征提取网络和U2NeRF进行优化。对于单场景和跨场景泛化实验，我们训练U2NeRF共250,000步，每一步采样512条光线；而在每个场景的微调过程中，预训练网络仅微调50,000步，每一步采样256条光线。与大多数NeRF方法不同，我们没有使用单独的粗网络和精网络，因此为了使GNT的实验设置具有可比性，我们在所有实验中每条光线采样192个粗点（除非另有说明）。

**评估指标**  

为了评估我们方法的渲染和恢复质量，我们采用了广泛使用的指标：峰值信噪比（PSNR）、结构相似性指数（SSIM）[34]、学习感知图像块相似性（LPIPS）[40]、水下图像质量测量（UIQM）[23]以及水下色彩图像质量评估指标（UCIQE）[37]。我们在每个场景的不同视图以及每个数据集的多个场景中报告每个指标的平均值。按照以往的研究[20, 30]，我们在合成场景中计算渲染图像与真实恢复视图之间的PSNR、SSIM和LPIPS分数，而在没有参考恢复图像的真实水下场景中，我们计算UIQM和UCIQE分数。对于真实世界的数据，我们还额外报告了使用[7]（以消除颜色差异）计算的渲染目标视图和恢复目标视图的灰度图像之间的LPIPS分数，并定量评估不同方法的渲染能力。

### 4.2 水下视图合成数据集

由于缺乏适合评估新视图渲染的多视图水下场景数据集，我们建立了一个新的基准——**水下视图合成（UVS）数据集**，包含12个场景，均匀分为三个难度级别：简单（合成水下场景）、中等（真实世界高质量场景）和困难（真实世界低质量场景）。简单级别包含来自LLFF数据集[18]的4个场景，分别是“fern”、“fortress”、“flower”和“trex”，这些场景被合成地损坏以模拟水下场景[9]。对于真实世界的数据，我们从YouTube的高质量视频中手工挑选了4个场景作为中等难度的分割，而困难级别的分割则由潜水过程中获得的低质量、嘈杂的真实世界捕获组成。对于中等和困难级别的每个场景，我们分别选取大约100-150张图像，并使用开源软件包COLMAP[28, 29]中的**结构化运动（SfM）算法**对它们进行校准。在COLMAP中，我们使用“简单径向”相机模型，该模型具有一个径向畸变系数，并为所有图像共享内参。我们在SfM的穷尽匹配步骤中使用“SIFT特征引导匹配”选项，并在捆绑调整过程中优化内参的主点。图5展示了简单、中等和困难级别中的场景示例。

### 4.3 基线

由于同时进行恢复和渲染的任务是新颖的，因此我们为U2NeRF在UVS基准测试中建立了几个基线进行比较。我们选择NeRF作为我们的神经渲染器，并确定了不同的策略来自动“恢复”渲染视图。作为初始基线（标记为NeRF），我们在原始水下场景上训练了一个标准的NeRF模型，将最先进的恢复方法作为后处理策略附加到NeRF模型上（标记为NeRF + Clean），甚至在恢复后的图像上训练了一个NeRF模型（标记为Clean + NeRF）。此外，我们还考虑了非渲染基线，假设可以直接访问目标视图并尝试对其进行恢复。我们利用了最先进的水下图像恢复流程——UIESS[7]和水下物理信息图像形成模型（我们标记为UPIFM）[4]。

### 4.4 单场景结果

**数据集**  

为了评估U2NeRF在单场景视图生成方面的能力，我们在UVS数据集的简单、中等和困难分割上进行了实验。我们在每个分割的所有场景中报告平均分数——简单分割：[Fern, Fortress, Flower, Trex]，中等分割：[Starfish, Coral, Debris, Shipwreck]，困难分割：[scene1, scene2, scene3, scene4]，分别在表1a、1b、1c中展示。

**讨论**  

我们将U2NeRF与第4.3节中讨论的基线方法进行了比较。在简单分割中，我们提出的方法获得了适中的PSNR分数，但在与其他渲染基线相比时，LPIPS分数表现最佳，提升了20%。这可能是因为PSNR无法衡量结构失真、模糊，并且对亮度具有高敏感性，因此无法有效衡量视觉质量。关于PSNR分数的差异及其与渲染图像质量的相关性，[16]中也有类似的推断。在中等和困难数据分割中存在的更复杂场景中，我们可以清楚地看到U2NeRF在渲染（LPIPS ↓11%）和色彩恢复质量（UIQM ↑5%，UCIQE ↑4%）方面的优越性。更有趣的是，我们发现U2NeRF甚至优于非渲染基线，即那些假设直接访问目标视图并仅执行恢复的算法。尽管我们的方法是基于UPIFM扩展的，但我们仍然能够以足够的优势超越“仅恢复”的基线。这表明多视图几何对于自动恢复目标视图的重要性。我们在图3中展示了定性结果，可以清楚地看到，与其它方法相比，U2NeRF渲染和恢复的图像具有更高的视觉质量。在“debris”场景中，U2NeRF成功地恢复了鱼类并增强了其可见性以提高恢复质量；而在“scene2”中，U2NeRF能够渲染复杂的动态结构（如绳索），同时仍然保持岩石表面的更高细节。

### 4.5 跨场景结果

**数据集**  
U2NeRF利用符合极线几何的多视图特征，能够泛化到未见场景。我们从潜水过程中拍摄的视频数据中随机选择场景，共使用45个场景进行训练。表2讨论了训练后的网络在UVS数据集中中等和困难分割的全部8个场景上的结果。请注意，UVS数据集中的场景在训练过程中被保留，以衡量模型的泛化性能。

**讨论**  
我们将U2NeRF的泛化性能与仅在单个场景上训练的对应网络进行了比较。尽管该网络仅在我们潜水过程中拍摄的数据上进行了训练，但它仍然能够泛化到中等分割中存在的未见物体。经过微调（仅需50k训练步骤），U2NeRF的表现与在每个场景上训练的标准NeRF相当，甚至优于后者。图4可视化了在UVS数据集上的定性结果。我们可以清楚地看到，预训练模型能够成功地泛化到多个场景，并且在微调后进一步提升了性能。


### 4.6 消融研究

**图像块大小的影响**  
为了验证图像块大小对渲染图像质量的影响，我们在“starfish”场景上使用不同的图像块大小（2、4和8）训练U2NeRF。从表3可以看出，图像块大小为4时，在所有三个指标上均取得了最佳结果。较大的图像块大小需要更多的信息（不仅仅是极线上的点）来进行准确重建，而较小的图像块大小则无法作为恢复的有效先验。因此，图像块大小为4在性能和网络复杂度之间达到了理想的平衡。

**稀疏源视图的影响**  
为了测试U2NeRF在稀疏视图条件下的性能，我们在“starfish”场景上评估了一个经过训练的模型，但输入的源视图数量较少。从表4可以看出，随着源视图数量的增加，模型的性能有所提升。然而，即使仅输入3个源视图，模型的性能几乎没有显著下降。这验证了U2NeRF在稀疏输入视图条件下的适用性。

**高斯噪声的影响**  
与标准的NeRF方法不同，U2NeRF预测的是图像块而不是单个像素的颜色。因此，我们假设在没有显式训练的情况下，U2NeRF能够去除场景中存在的小扰动。为了验证这一假设，我们在被高斯噪声（未见过的）破坏的场景上评估了一个经过训练的U2NeRF模型，该噪声的均值为0，标准差为0.05。我们在图6中展示了定性结果。

### 4.7 U2NeRF的物理解释

U2NeRF试图隐式地预测各个图像分量$J$、$T_D$、$T_B$和$A$，这些分量组合起来可以恢复水下图像。图7展示了带有相应预测图像分量的水下场景示例。我们可以清楚地看到，可视化的$T_D$和$T_B$图与深度模拟非常接近，这与基于物理的图像形成模型[4]一致。$A$表示全局背景光，对应于场景中最亮的像素，可以从图7中得到确认。图7中的恢复图像对应于场景辐射。因此，在没有显式监督的情况下，U2NeRF学会了将其可学习操作物理地锚定。

## 5. 结论

我们提出了无监督水下NeRF（U2NeRF），该方法扩展了辐射场，以同时渲染和恢复新视图，特别是在水下图像的背景下。我们证明了通过增强现有的辐射场的空间感知能力，并结合基于物理的水下图像形成模型，可以成功恢复水下图像。此外，我们贡献了一个新的水下视图合成数据集（UVS数据集），包含12个水下场景，既有合成生成的数据，也有真实世界的数据。广泛的实验表明，U2NeRF优于现有的基线方法，并取得了最佳的感知指标分数（LPIPS ↓11%，UIQM ↑5%，UCIQE ↑4%）。这些结果表明，Transformer可以成功地用于建模3D视觉中的底层物理过程。

## 附录

## A. 损失函数

为了以无监督的方式实现渲染和恢复，使用适当的损失函数对模型进行正则化至关重要。我们提出了6种不同的损失函数，它们分别作用于不同的输出图，类似于[4]。

### A.1 重建损失（Reconstruction Loss）

我们使用重建损失来自监督层分解过程。通过计算原始水下图像与预测图像之间的均方误差（MSE），对重建损失进行监督，目标是最小化以下损失：

$$
L_{\text{Rec}} = \|I - x\|_2^2 \quad (7)
$$

其中，$I$表示真值图像，$x$表示预测图像。

### A.2 对比度增强损失（Contrast Enhancement Loss）

在干净的图像中，亮度与饱和度之间的差异几乎为零，如[41]中观察到的。我们创建了对比度增强损失$L_{\text{Con}}$，用于监督场景辐射图$J$：

$$
L_{\text{Con}} = \|V(J(x)) - S(J(x))\|_2^2 \quad (8)
$$

其中，$V$表示亮度，$S$表示场景辐射$J(x)$的饱和度。

### A.3 颜色一致性损失（Color Constancy Loss）

为了纠正恢复图像中可能出现的颜色不一致性，我们根据Gray-World颜色一致性理论[3]构建了颜色一致性损失$L_{\text{Col}}$，损失函数如下：

$$
L_{\text{Col}} = \sum_{c \in \Omega} \|\mu(J_c) - 0.5\|_2^2, \quad \Omega = \{R, G, B\} \quad (9)
$$

其中，$\mu(J_c)$表示估计的场景辐射中颜色通道$c$的平均强度值。

### A.4 光学全局属性损失（Light Global Property Loss）

为了减少自网络（ANet）中潜在编码$z$与其重建$\hat{z}$之间的差异，我们为变分推断创建了光学全局属性损失$L_{\text{kl}}$：

$$
L_{\text{kl}} = \text{KL}(N(\mu_z, \sigma_z^2) \| N(0, I)) \quad (10)
$$

其中，$\text{KL}(\cdot)$表示两个分布之间的Kullback-Leibler散度，$N(\mu_z, \sigma_z^2)$表示学习到的潜在高斯分布，$N(0, I)$表示标准正态分布。


### A.5 传输一致性损失（Transmission Consistency Loss）

由于后向散射系数仅取决于水的光学属性，因此它们应在后向散射传输图中为常数。我们提出了传输一致性损失$L_{\text{T}}$，用于监督后向散射传输图$T_B$：

$$
L_{\text{T}} = \sum_{c_1, c_2 \in \epsilon} \left\| \log T_{c_1} - \log T_{c_2} - \mu(\log T_{c_1} - \log T_{c_2}) \right\|_2^2 \quad (11)
$$

其中，$T_c$表示颜色通道$c$的估计后向散射传输图，$\mu$是平均因子，$\epsilon = \{(R, G), (R, B), (G, B)\}$是颜色对的集合。

### A.6 全局一致性损失（Global Consistency Loss）

全局一致性损失$L_{\text{Glob}}$的目标是完全模糊/平滑全局背景光$A$。该损失通过要求每个像素与其相邻像素具有相同的颜色，间接强制执行平滑准则。

### 总损失（Total Loss）

我们方法的总损失如下：

$$
L = \lambda_1 L_{\text{Rec}} + \lambda_2 L_{\text{Con}} + \lambda_3 L_{\text{Col}} + \lambda_4 L_{\text{kl}} + \lambda_5 L_{\text{T}} + \lambda_6 L_{\text{Glob}}
$$

其中，$\lambda$表示每个损失项的权重。我们设置$\lambda_1 = 1, \lambda_2 = 0.1, \lambda_3 = 1, \lambda_4 = 1, \lambda_5 = 0.1, \lambda_6 = 1$以获得最佳结果。

## B. 实验结果

我们在所有三个分割（共12个场景）上提供了完整的方法的定性（图8和图??）和定量（表5、表6）结果。对于中等和困难分割，我们展示了七种不同的定性结果，包括NeRF、Clean+NeRF和NeRF+Clean等基线，其中恢复是使用[7]完成的。我们还展示了两种不同恢复技术（[7]和[4]）的定性结果。对于简单分割，我们展示了NeRF和NeRF+Clean等基线以及两种恢复技术的结果。我们使用[9]来降质LLFF[19]数据以创建简单分割。

## C. 限制/未来工作

在困难分割中，我们特别观察到在植物有显著运动的区域，渲染图像中存在模糊现象。这为未来的研究提供了有趣的可能性，特别是在解决目标运动问题方面。















---
layout: mypost
title: 听论文讲座感想体会
categories: [论文阅读]
---


# 写在前面

没有输入就没有输出啊，人要警惕自己的懒惰，如果对自己不熟悉的事情，不想办法去熟悉它，就会很难有所突破。只有输入才有可能输出，只有不断输出，才有可能持续的输入。

最近读的论文越来越少了，这三五个月来读过的论文用手掰着就能数出来。这样下去不是办法。为什么没有进展呢？为什么没有想法和IDEA呢？没有阅读就没有新的思想和体会，只靠自己脑子去想是没有出路的。

如同读毛选、邓选、江选、马、列选集一样，就在自己的工作背景中不断放一些领域相关的音频，迟早会熟悉起来的。对于可以放的有声书，就用腾讯读书来听；对于论文，没有那种PDF的阅读器，就上Bilibili去找一些分享的讲座和视频，听他们如何分析论文，以及文章中有哪些存在启发性的点。

# 01 - Instant-3D: Instant Neural Radiance Field Training Towards On-Device AR/VR 3D Reconstruction

针对设备上的AR/VR 3D重建的即时辐射场训练，作者是ESCA佐治亚大学的博士生，这篇文章发表在ISCA 2023上面，第一次听到这个会议。查了一下，在中国计算机学会推荐国际学术会议(● 计算机体系结构/并行与分布计算/存储系统)是A类的会议。全称是International Symposium on Computer Architecture，体系结构领域的顶级会议。由ACM SIGARCH(计算机系统结构特殊兴趣组)和IEEE TCCA（计算机架构技术委员会）联合举办。

Bilibili上面的链接：
【[ISCA 2023] Instant-3D：针对设备上 AR/VR 3D 重建的即时神经辐射现场训练】 [link](https://www.bilibili.com/video/BV1PC4y1d7KA/?share_source=copy_web&vd_source=bf0600aee0241d49739440116f9be16a)

Bilibili的发布时间：2023-10-10 03:01:05

主要解决的问题：在Instant-NGP的基础上，继续提高三维重建的速度。

听到的主要思想：密度场和颜色场，二者的训练速度、优化速度是不一致的，可以对颜色场和密度场分别构建两种尺度。

在训练的过程中，深度图的精度比颜色图的精度更高一些。相对于密度特征，颜色特征对模型压缩不太敏感。

这说明，颜色模型在一定程度上，按照现有的网络架构，是过拟合的；重要的是物体的空间几何结构特征。

观察到：在反向传播的过程中，使用缓冲器计算写入请求。这项工作，我认为还是有价值的。

论文链接：[link](https://dl.acm.org/doi/pdf/10.1145/3579371.3589115)

# 02 - Gaussian Splatting

Gaussian Splatting是2023年的Siggraph Best Paper. 论文出自法国的学校Inria, Université Côte d’Azur, France，这个学校之前没有见过，查了一下叫做法国蔚蓝海岸大学，于2015年由13所杰出大学和科研、学术机构合并而成，是法国蔚蓝海岸地区新成立的高等教育大学机构群。论文的Project Page主页是：[Project Page](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)，包含FPS、PSNR等实验比较数据的可视化表示，以及跟各种方法的对应比较。

这篇文章在arxiv的发表时间是2023年的8月8号，我大概是在10月中期知道的这篇文章。[Submitted on 8 Aug 2023](https://arxiv.org/abs/2308.04079)。

bilibili的用户[Little_Z呀](https://space.bilibili.com/427360458)在视频[【三维重建技术：3D Gaussian Splatting，这是3D的未来吗？】 ](https://www.bilibili.com/video/BV1Nu4y1W7F9/?share_source=copy_web&vd_source=bf0600aee0241d49739440116f9be16a)下面评论说，“隔壁楼的组搞得”， IP在法国，可信度比较高。

如何安装 Gaussian Splatting? 参见Youtube的视频，该视频是YouTube的某位博主制作的，与论文的作者无关，但是论文的作者看到这个视频，感觉做的不错，于是在GitHub上面添加了该视频的链接。[Youtube](https://www.youtube.com/watch?v=UXtuigy_wYc)

## 2.1 Bilibili，外行人讲NeRF和Gaussian Splatting.

Gaussian-Splatting的GitHub链接：[Link](https://github.com/graphdeco-inria/gaussian-splatting)，截止时间2023年11月8日13:13:48，在GitHub上的Star数目为6.9K，还将持续上升。时常感觉自己不是搞科研的，最新的科研进展完全没有跟进上。

Bilibili链接：外行人讲的NeRF和Gaussian Splatting，应该是搬运自油管，配上了中文字幕：
【三维重建技术：3D Gaussian Splatting，这是3D的未来吗？】 [link](https://www.bilibili.com/video/BV1Nu4y1W7F9/?share_source=copy_web&vd_source=bf0600aee0241d49739440116f9be16a)

该视频是一个科普向的视频，简要介绍Gaussian Spaltting的原理、性能、应用场景，并介绍该工作与NeRF的工作流程之间的差异性。

基于NeRF进行渲染的过程中，比较缓慢的点在于需要逐点查找MLP对应的色彩，一些解决方案，分别是使用哈希编码空间位置，迅速查找空间位置对应的编码（Instant-NGP）。另一组解决方案，是完全放弃使用空间的MLP，采用空间立体网格进行编码、查找对应的空间信息、颜色信息（Plenoxels）。这些方法存在着时间空间与质量上的平衡。

而Gaussian Splatting的方法，是主要基于空间的三维稀疏点云，对空间中的点赋予他们所在的位置、朝向等信息，包括协方差矩阵（用于控制空间的朝向）、不透明度alpha、位置信息（XYZ），组合成为三维空间中的高斯。将球谐系数存储在三维高斯中，进一步对这些信息上色。

对于性能而言，GS(Gaussian Splatting)方法的FPS能够达到惊人的137.3帧每秒（30K iterations迭代量），在7K iterations（迭代次数）的时候，能够达到167.9帧每秒，相当惊人的渲染速度。Instant-NGP方法只能达到9.0FPS，（我对此数据存疑，因为我使用Instant-NGP跑过实验，感觉FPS不止9，应该能够达到30以上）。在视频中还表示，MipNeRF360的训练时间是48小时，而GS的训练时间是38.8分钟（30K 迭代次数），GS的7K训练时间是6.1分钟。

GS的PSNR比MipNeRF360要略低一些，MipNeRF是27.11，而GS则为26.91。

GS在可视化效果上面，关于自行车的车辐条以及草地细节上，表现的比Mip-NeRF更优一些。并且，可以重建大场景（工厂），以及水体反射效果。（我比较好奇，为什么可以处理水体反射效果呢？有可能是因为他在球谐函数上面做了编码，能够处理高光反射的效果，主要来源自Ref-NeRF的场景建模）。

> Jon Barron表示：
> 3D Gaussian Splatting is not just a NeRF where the MLP las been replaced by a set of Gaussians, as I've seen some say. This sort of idea was explored in the (underrated) [underrate somebody/something to not recognize how good, important, etc. somebody/something really is过低评价；低估] Pulsar papre: [https://arxiv.org/abs/2004.07484](https://arxiv.org/abs/2004.07484). The important delta for 3DGS is rasterizarion instead of raytracing.

这位MipNeRF360的作者表示，3DGS的重要意义在于，他采用栅格化而不是射线追踪的方法进行场景表示。这一点的重要性我还没有完全领会。

## 2.2 Bilibili, 内行人做的论文讲解

Bilibili链接：【【论文讲解】用点云结合3D高斯构建辐射场，成为快速训练、实时渲染的新SOTA！】 [Link](https://www.bilibili.com/video/BV1uV4y1Y7cA/?share_source=copy_web&vd_source=bf0600aee0241d49739440116f9be16a)

视频的作者主要汇总的一个重点，讲解出Gaussian Splatting方法名字的缘由：Splatting是一个抛雪球的方法。如果将一个雪球向墙上扔，会得到一滩雪的痕迹，如果玩过雪球，会知道这样抛出去的雪球会带来一个椭球型的痕迹，而且根据雪球的大小、雪球本身的硬度、力度的不同，会形成比较集中的小圆球形或者是椭球型的分布。这就是抛雪球的方法。而图形学中也借鉴这种方法，将点云看成雪球投射到成像平面上，从前到后进行平面上的叠加，会生成一组不同朝向的雪球图案，最后生成图像。

下一个，讲解到Spherical harmonics的内容，如果使用到Spherical harmonics，就比较类似于Ref-NeRF中间，对向量方向进行编码，有利于处理反射和高光区域。

关于旋转矩阵，论文中采用了四元数而不是旋转矩阵，就从旋转矩阵的9个参数，转换到四元数的4个参数。

## 2.3 Youtube，论文作者的视频演示

Youtube链接：3D Gaussian Splatting for Real-Time Radiance Field Rendering. [link](https://youtu.be/T_kXY43VZnk?si=I70CcTgiU_Kom_oc)

在视频演示中，作者显示出，他们的工作在包含Structure From Motion重建出的点云前提下，重构出的场景精度更加高效，不过即便没有初始的点云，也可以构造场景，但是场景预测视角的精度就不精确了。在谈及这个问题的时候，一些作者解读说，通过随机初始化的点云，作者在Splitting and Deleting的时候，可以删除掉占用率（透明度）较低的区域，进而让数据更加稳定。

视频还有另外两个点，第一是Instant-NGP包含模糊的云雾结构，第二是各向同性的高斯在预测效果上占劣势。

首先谈一谈Instant-NGP的模糊云雾结构，这是我之前自己训练Instant-NGP方法的时候，遇到的实际测试结果。确实会出现比较模糊的点云效果。

接下来是谈各向同性和各向异性，如果三维高斯场景中包含协方差矩阵的朝向，那么在三个方向xyz里面的朝向、比例是不一致的，需要考虑如果使用协方差矩阵，各个方向的朝向就会有差异，能够更好的拟合场景中的几何结构。如果在各个方向都保持相同的比例结构，输出的结构就有可能会模糊。

## 2.4 Github, Introduction to 3D Gaussian-Splatting

链接：[Link](https://github.com/huggingface/blog/blob/main/gaussian-splatting.md)

这是一个在Hugging Face上面发布的博客，主要内容是讲3DGS中的一些关键点。里面的一个关键点是：

> If the gradient is large for a given gaussian (i.e. it's too wrong), split/clone it
>
> If the gaussian is small, clone it
>
> If the gaussian is large, split it
>
> If the alpha of a gaussian gets too low, remove it

谈到的内容主要是：

> 如果在一个指定的高斯上面，高斯的梯度很大，说明在这个地方错误的程度比较大，需要分割或者是克隆它。
>
> 如果高斯比较小，就克隆它；如果高斯比较大，就分割它。
>
> 如果高斯的密度比较小，就移除它。

## 2.5 YouTube: How to install 3D Gaussian-Splatting

链接：[Link](https://youtu.be/UXtuigy_wYc?si=-XBszuCBE7KDgPjy)

该视频是YouTube的一名视频创作者自制的，但是因为质量比较高，被3D Gaussian Splatting的论文作者，将链接放到GitHub的主页上面了。主要内容是讲解如何安装、配置环境，训练并可视化三维的高斯Splatting。

重点：该视频创作时间是2023年8月28日，目前（2023年11月8日21:02:19）视频已经有78772次观看。GitHub上面可对于FFMPEG和Microsoft Visual Studio的下载链接，与YouTube视频中有一些区别。

# 03 - 张凯的关于NeRF的一系列讲座

## 3.1 公开课第一节 - 基于NeRF的三维内容生成

### 3.1.1 拓展-关于拓展到360度向外朝向的问题

张凯认为，NeRF能够成功的一个主要原因是，NeRF采用的是soft的编码结构，在新视角合成的过程中是占优的。

张凯谈到在NeRF渲染过程中，可以将视角的朝向，分为几类：

- 前向拍摄，例如站在广场上拍前面的城楼。
- 向内拍摄，例如拍摄一个放在正中心的物品，比如说小熊。
- 向外拍摄，旋转360°，拍摄周围的环境。
- 任意视角拍摄，在移动中拍摄，类似于slam。

该问题在张凯的工作中，通过NeRF++的论文，将内球和外球两个球做了球归一化的弯折映射。后面进一步在其他的论文中讨论不同朝向问题，比如MipNeRF-360论文，以及F2-NeRF。F2-NeRF的论文讲解可以参见：Bilibili【【论文讲解】F2-NeRF：第一个大规模、无边界场景下自由相机轨迹的快速NeRF重建】 [link](https://www.bilibili.com/video/BV1Lz4y187jL/?share_source=copy_web&vd_source=bf0600aee0241d49739440116f9be16a).

关于NeRF++, 张凯给出如下几个关键的信息：

- NeRF faces resolution issue for 360 captures of unbounded scenes.
- NeRF++ separates foreground/background modelling, as NeRF is compositional.
- NeRF++ uses inverted sphere parameterization to establish symmetry between 360 inward-facing and 360 outward-facing captures.

翻译一下，就是：

- NeRF面临着对于360度无边界场景拍摄时候的建模问题。
- NeRF++对前景和背景分别建模，因为NeRF的渲染过程是可以组合的。
- NeRF++使用一种逆球形的参数化表示，用于构建360度向内拍摄与360度向外拍摄之间的对称性，这样不用更改算法，就可以用于360度无边界的问题。

在2022年7月12日的时候，张凯谈到，当时还没有办法对NeRF进行实时渲染。不过后面2022年8月就出来了Instant-NGP，时隔一年之后的2023年8月迎来了Gaussian Splatting。

有人提问说，NeRF++和MipNeRF-360在参数上的表示有什么本质上的差异以及优劣？

张凯回答说：二者在本质上是很相似的，都是Invert 360 sphere的方法。

NeRF++在边界上是连续的。


### 3.1.2 拓展-关于拓展到去锯齿的问题

张凯在这里谈到的论文是MipNeRF. 参见Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields在Arxiv上的论文链接[MipNeRF](https://arxiv.org/abs/2103.13415), 以及他们的代码仓库[GitHub Link](https://github.com/google/mipnerf)。 Google在跟进NeRF相关工作的时候，跟进很快，而且做了不少有价值的工作。并试图将这些NeRF的快速、高质量的渲染，整合到谷歌的VR/AR系统、自动驾驶系统、谷歌地图系统中。

我是有这样的看法：如果一个公司以盈利、落地为目的，对某项技术进行跟进和开发，会在很大程度上推进这项工作出现低功耗、高优化、高精度的完善。

关于Mip-NeRF的Motivation和Target出发点，如果在高分辨率图像上进行训练，随后利用MLP渲染比较低分辨率的图像的时候，会出现Jittering（锯齿状）的问题。

- 张凯在这里提出一个问题/思考方式：深度学习行业的从业者，会考虑，如果训练数据是高分辨率的图像，而目标是输出低分辨率的图像，为什么不直接在训练的时候将高分辨率的图像进行降采样得到低分辨率的图像进行训练，并直接获取到低分辨率的输出，而是一定要采用高分辨率的图像训练，并输出低分辨率呢？也就是说，为什么不先做输入端的数据增强以适应输出目的呢？

- 张凯的回答是这样的：我们并不希望损失高分辨率的精度和数据。也就是说，在训练的时候，就应该保持高分辨率的空间优势。



- 问题：对于NeRF而言，预测出来图像的PSNR是多少，算比较高的结果呢？
  回答：如果是合成数据集，能够达到30就算是比较高的结果了，两张图片的区分度就比较低，难以区分。但是对真实场景而言，27、28左右就很高了。

# 04 - ClearPose

ClearPose: Large-scale Transparent Object Dataset and Benchmark

我之前写过一份这篇文章的博客。参见：[MyBlog-ClearPose](https://zhangwenniu.github.io/posts/2023/03/13/036-ClearPose.html)

该作者在Youtube上面有自己的账号[Xiaotong Chen](https://www.youtube.com/@xiaotongchen6968)，发了两个ClearPose的视频：[ClearPose-人声讲解](https://www.youtube.com/watch?v=DMg1cDI8i-Q)，[ClearPose-PPT-无声视频](https://www.youtube.com/watch?v=i8LjxicAaps)。

根据我的观察，这篇文章首先买了一些透明物体，接下来使用Blender自己手工对每个透明物体建立三维模型。第二步，采用ORB-SLAM3对场景进行标定，获取相机位姿。第三步，通过自己之前的标定工作ProgressLabeller标定每个物体的位姿。将不同视角下的场景图片导入Blender中，拖拽自己对物体的建模进入场景中，拖拽透明物体直至边缘与多视角下该物体的轮廓与拍摄图片重合。

ClearPose-PPT-无声视频的讲解提纲：

> ClearPose
>
> 1. Label Object Poses
> 2. Depth Completion Baseline
> 3. Pose Estimation Baseline
>
> Label Object Poses with ProgressLabeller
>
> 1. Set the configurations and import the data
> 2. Generate camera pose with ORB-SLAM3
> 3. Set the origin and align the plane
> 4. Set up Multi-view interface
> 5. Import object models
> 6. Label objects
>
> Model Gallery
>
> 这里展示了Input: RGB, Raw Depth; Output: Segmentatioon, Normal, Fixed Depth. 我不太清楚ClearPose的工作流程，他究竟是如何利用这个数据集训练的？可能有一个工作的任务是补全深度图，在补全深度图的时候需要用到真实深度进行训练。这篇文章通过手动标注数据，获取带有玻璃物体的深度图，便于该深度补全网络以学习深度？
>
> Depth Completion 
>
> 展示了RGB, Raw Depth, GT Depth, TransCG, ImpplicitDepth; 在多个场景：Heavy Occlusion, New Background, Opaque Distractor, Translucent Cover, Non Planar, Filled Liquid的实验结果。
>
> Pose Estimation Baseline
>
> 展示了GT, Xu et al., FFB6D g/g, FFB6D g/c在Heavy Occlusion, New Background, Opaque Distractor, Translucent Cover, Non Planar, Filled Liquid下的实验结果。

目前还没有找到在该数据集上做好预训练，可以直接拿过来使用的深度预测网络。本篇文章本质上是一个数据集的工作。

# 05 SIGGRAPH 2020: Computing Light Transport Gradients using the Adjoint Method

视频链接：[https://developer.nvidia.com/siggraph/2020/video/sigg06](https://developer.nvidia.com/siggraph/2020/video/sigg06)

文章链接：[https://arxiv.org/abs/2006.15059](https://arxiv.org/abs/2006.15059)

Nvidia At Siggraph 2020: [https://developer.nvidia.com/events/recordings/siggraph-2020](https://developer.nvidia.com/events/recordings/siggraph-2020)

本文将成像表示为常微分方程的形式，从光源追踪光路直到观测者的正向传播类比于神经网络的前向传播，从观测者反向追踪光路直到光源的反向传播类比于神经网络的反向传播，可以理解为是某种伴随算子、逆算子。作者通过神经网络近似求解该逆渲染的过程，给出简单场景下的渲染效果和优化过程。



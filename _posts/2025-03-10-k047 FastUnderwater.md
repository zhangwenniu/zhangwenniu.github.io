---
layout: mypost
title: k047 Fast Underwater Scene Reconstruction using Multi-View Stereo and Physical Imaging
categories: [水下, MVS]
---



如果有研究水下场景的研究者，可以通过邮箱zhangwenniu@163.com联系我，我目前创建了一个水下场景的研究者交流群，欢迎感兴趣的研究者加入。

![FastUnderwater.fig.1](FastUnderwater.fig.1.png)

> 图1：概述。我们首先使用特征金字塔网络（FPN）从源视图中提取图像特征，并将它们扭曲为扭曲特征$\{F_i^w\}_{i = 1}^{N}$。将扭曲后的特征组合成一个代价体，然后使用三维卷积神经网络（3D CNN）对其进行处理以生成深度图。随后，使用池化网络整合预测深度处每个三维点的特征，然后将它们与源图像特征相结合。在此之后，使用颜色多层感知器（MLP）和一个中等大小的子网络来分析这些特征，得到去除介质后的优化图像。最后，再次使用介质子网络对图像进行额外处理，提取后向散射图像和衰减图像。随后将它们组合起来，得到最终的重建结果。

# 论文链接

- [https://arxiv.org/abs/2501.11884](https://arxiv.org/abs/2501.11884)

发表时间：[v1] Tue, 21 Jan 2025 04:35:27 UTC (3,747 KB)


# 论文重点难点介绍

## 1 **研究背景与问题**

- **背景**：水下场景重建在海洋科学、水下考古、生态学和地质学等领域具有重要应用。然而，由于水下环境中光与介质的复杂相互作用（如散射和吸收），导致深度估计和渲染更加困难。

- **问题**：现有的基于NeRF的方法虽然能够实现高质量的水下场景重建，但训练和渲染速度较慢。而传统的MVS方法在水下场景中表现不佳，因为它们没有考虑水下光传播的特性。

## 2 **研究方法**

- **方法概述**：本文提出了一种结合多视图立体（MVS）和基于物理的水下成像模型的新方法，用于快速水下场景重建。该方法包含两个分支：一个用于深度估计，另一个用于基于物理成像模型的渲染。

- **深度估计**：使用传统的MVS成本体积管道进行深度估计，通过特征金字塔网络（FPN）提取多尺度特征，并利用3D CNN进行正则化以生成深度图。

- **渲染**：通过设计一个介质子网络（Medium Subnet）来估计成像模型的参数，并结合颜色MLP进行高效渲染，从而恢复水下场景的真实颜色并提高几何表示的准确性。

## 3 **技术难点与解决方案**

- **难点1：水下成像模型的复杂性**

水下图像的质量受到光的衰减和散射的严重影响，不同波长的光在水中的衰减程度不同，导致图像颜色失真和对比度降低。

**解决方案**：采用改进的水下成像模型 [5]，将图像分解为直接成分和后向散射成分，并通过介质子网络估计衰减和散射参数，从而恢复清晰的场景颜色。

- **难点2：MVS在水下场景中的适用性**

传统的MVS方法依赖于清晰介质（如空气）中的光传播模型，在水下环境中表现不佳。

**解决方案**：通过将MVS与基于物理的成像模型结合，利用MVS的深度估计能力来增强水下场景的重建质量。该方法不需要依赖真实的深度信息，从而提高了训练和渲染效率。

- **难点3：训练和渲染效率**

基于NeRF的方法虽然能够生成高质量的水下场景重建结果，但训练和渲染速度较慢，限制了其实际应用。

**解决方案**：通过结合MVS的高效深度估计和物理成像模型的快速渲染能力，显著提高了训练和渲染速度。实验表明，该方法的训练时间比SeaThru-NeRF减少了约65倍。

## 4 **实验结果**

- **定量结果**：在SeaThru-NeRF数据集上，该方法在PSNR、SSIM和LPIPS等指标上均优于现有的MVS和NeRF方法，平均PSNR比SeaThru-NeRF提高了1.65dB，同时训练时间显著减少。

- **定性结果**：该方法能够生成高质量的水下场景新视图，并有效恢复场景的真实颜色。与SeaThru-NeRF相比，该方法在深度图的平滑性和一致性方面表现更好。

## 5 **未来工作方向**

- **改进特征提取**：当前方法在处理远处相似背景的物体时存在挑战，未来可改进FPN以提高特征提取的准确性。

- **数据集质量提升**：获取高质量的水下场景数据集（包含去除水后的图像）将有助于进一步提高颜色恢复的精度。

## 6 **总结**

本文通过结合MVS和基于物理的成像模型，提出了一种高效且高质量的水下场景重建方法。该方法不仅在渲染质量和训练效率上优于现有方法，还能够有效恢复水下场景的真实颜色，具有重要的实际应用价值。

# 论文详细介绍

## 1 **研究背景**

水下场景重建在海洋科学、水下考古、生态学等领域具有重要应用。然而，水下环境中的光传播特性（如散射和吸收）导致图像质量下降，增加了深度估计和渲染的复杂性。现有的基于NeRF的方法虽然能够实现高质量的水下场景重建，但存在训练和渲染速度慢的问题。而传统的MVS方法在水下环境中表现不佳，因为它们未考虑水下光传播的特性。

## 2 研究方法

### 2.1 方法概述

本文提出了一种结合多视图立体（MVS）和基于物理的水下成像模型的新方法，用于快速水下场景重建。该方法包含两个分支：

- **深度估计分支**：使用传统的MVS成本体积管道进行深度估计。

- **渲染分支**：基于物理成像模型进行渲染，通过估计介质参数恢复场景的真实颜色。

该方法不需要依赖真实的深度信息，从而提高了训练和渲染效率。

### 2.2 水下成像模型

水下图像质量受光的衰减和散射影响，衰减系数和散射系数随距离变化，导致颜色失真和对比度降低。本文采用改进的水下成像模型 [5]：

$$
I = J \cdot \exp(-\beta_D (v_D) \cdot z) + B_\infty \cdot \left(1 - \exp(-\beta_B (v_B) \cdot z)\right)
$$

其中，$J$ 是无介质时的清晰场景，$B_\infty$ 是无穷远处的背景光颜色，$\beta_D$ 和 $\beta_B$ 分别是直接光和散射光的衰减系数，$v_D$ 和 $v_B$ 是与距离、物体反射率、环境光谱和相机响应相关的向量。

### 2.3 深度估计

深度估计是通过MVS实现的，具体步骤如下：

`1.` **特征提取**：使用特征金字塔网络（FPN）从输入多视图图像中提取多尺度特征。

`2.` **成本体积构建**：通过可微同胚变换将源视图的特征映射到目标视图的深度平面上，构建成本体积。

`3.` **深度预测**：使用3D CNN对成本体积进行正则化，生成深度概率体积，并通过softmax计算深度分布。

`4.` **粗到细的预测**：采用级联结构逐步细化深度图，提高深度估计的准确性。

### 2.4 渲染过程

渲染过程基于物理成像模型，具体步骤如下：

`1.` **特征聚合**：从多视图源图像中提取目标视图的像素对齐特征，并通过池化算法聚合特征。

`2.` **颜色预测**：使用颜色MLP计算混合权重，结合源视图的颜色信息恢复目标视图的清晰图像。

`3.` **介质参数估计**：通过介质子网络估计衰减系数、散射系数和散射颜色。

`4.` **最终重建**：根据成像模型公式，结合衰减和散射效应重建水下图像。

## 3 实验

### 3.1 实验设置

- **数据集**：使用SeaThru-NeRF数据集，包含四个不同海域的多视图水下场景。

- **训练细节**：使用PyTorch实现，训练3000次迭代，采用Adam优化器。

- **基线方法**：与SeaThru-NeRF、3D-GS、ENeRF和MVS-Gaussian等方法进行比较。

- **评估指标**：使用PSNR、SSIM和LPIPS评估合成视图的质量。

### 3.2 实验结果

- **定性结果**：

本文方法在恢复水下场景的真实颜色和深度图的平滑性方面表现优于SeaThru-NeRF。在远处细节的渲染上，本文方法能够更好地保持几何特征的清晰度。

## 4 **结论**

本文提出了一种结合MVS和基于物理的成像模型的水下场景重建方法。该方法不仅提高了重建质量，还显著提高了训练和渲染效率。未来工作将集中在改进特征提取网络和获取更高质量的训练数据集，以进一步提升颜色恢复的精度。

# 论文方法部分详解


## 3 方法

本文提出了一种结合多视图立体（MVS）和基于物理的水下成像模型的方法，用于高效水下场景重建。该方法通过两个分支实现：**深度估计分支**和**渲染分支**。深度估计分支利用MVS的传统成本体积方法估计场景几何，而渲染分支则基于物理成像模型恢复场景的真实颜色并生成高质量的渲染结果。

### 3.1 水下成像模型

水下图像的形成受到光在水中的衰减和散射的影响。本文采用改进的水下成像模型 [5]，将图像分解为直接成分和后向散射成分：

$$
I = J \cdot \exp(-\beta_D (v_D) \cdot z) + B_\infty \cdot \left(1 - \exp(-\beta_B (v_B) \cdot z)\right)
$$

- $I$ 是观测到的图像；

- $J$ 是无介质时的清晰场景；

- $B_\infty$ 是无穷远处的背景光颜色；

- $\beta_D$ 和 $\beta_B$ 分别是直接光和散射光的衰减系数；

- $v_D$ 和 $v_B$ 是与距离、物体反射率、环境光谱和相机响应相关的向量。

### 3.2 方法概述

给定多视图图像，目标是从新的相机姿态合成目标图像。整体框架如图1所示，具体步骤如下：

`1.` 使用特征金字塔网络（FPN）从输入多视图图像中提取多尺度特征。

`2.` 通过可微同胚变换将特征映射到目标视图的深度平面上，构建成本体积。

`3.` 使用3D CNN对成本体积进行正则化，生成深度图。

`4.` 利用深度图聚合多视图和空间信息，通过颜色MLP和介质子网络恢复无介质的清晰图像。

`5.` 通过介质子网络进一步处理，生成最终的重建结果。

### 3.3 深度估计（MVS）

深度估计是整个流程的核心部分，具体步骤如下：

#### 3.3.1 特征提取

使用特征金字塔网络（FPN）从输入多视图图像中提取多尺度特征：

- 每个输入图像 $I_i \in \mathbb{R}^{H \times W \times 3}$ 被处理为低分辨率特征图 $F_{i,1} \in \mathbb{R}^{H/4 \times W/4 \times 32}$。

- 通过上采样和横向连接生成更高分辨率的特征图 $F_{i,2} \in \mathbb{R}^{H/2 \times W/2 \times 16}$ 和 $F_{i,3} \in \mathbb{R}^{H \times W \times 8}$。

- 通过平滑卷积提升特征质量，为后续深度预测提供更有效的特征表示。

#### 3.3.2 成本体积构建

通过可微同胚变换将源视图的特征映射到目标视图的深度平面上，构建成本体积：

- 对于每个采样深度 $z$，计算变换矩阵 $H_i(z)$：

$$
H_i(z) = K_i R_i \left(I + \frac{(R_i^{-1} t_i - R_t^{-1} t_t) a^T}{z}\right) R_t^{-1} K_t^{-1}
$$

- 使用 $H_i(z)$ 将源视图的特征 $F_i$ 映射到目标视图的深度平面上，得到扭曲特征 $F_w^i$：

$$
F_w^i(u, v, z) = F_i \cdot \left(H_i(z) \cdot [u, v, 1]^T\right)
$$

- 计算多视图特征的方差，构建成本体积 $C$：

$$
C = \frac{1}{N} \sum_{i=1}^N \| F_w^i - \bar{F}_w \|^2
$$

其中，$\bar{F}_w$ 是所有特征体积的平均值。

#### 3.3.3 深度预测

使用3D CNN对成本体积进行分析，生成深度概率体积 $\hat{C}$，并计算深度分布：

- 对于每个深度假设 $L_i$，计算其概率 $P_i = \text{softmax}(\hat{C})$。

- 计算深度预测值及其置信度：

$$
\hat{L}(u, v) = \sum_{i=1}^D P_i(u, v) \cdot L_i(u, v)
$$

$$
\hat{\sigma}(u, v) = \sqrt{\sum_{i=1}^D P_i(u, v) \cdot \left(L_i(u, v) - \hat{L}(u, v)\right)^2}
$$

- 基于置信区间确定初始深度范围：

$$
\hat{U}(u, v) = [\hat{L}(u, v) - \lambda \hat{\sigma}(u, v), \hat{L}(u, v) + \lambda \hat{\sigma}(u, v)]
$$

#### 3.3.4 粗到细的预测

采用级联结构逐步细化深度图：

- 初始深度图用于构建更高分辨率的成本体积。

- 通过3D CNN处理细化后的成本体积，生成更详细的深度图和3D特征体积。

- 所有深度图存储在缓冲区中，供后续步骤使用。

### 3.4 水下图像形成与渲染

传统MVS方法仅用于几何重建，而本文通过结合物理成像模型，利用MVS的深度估计能力重建整个场景外观。

#### 3.4.1 特征聚合

从多视图源图像中提取目标视图的像素对齐特征：

- 基于目标视图的深度图 $\hat{L}_{\text{tar}}$，计算每个3D点的世界坐标，并转换到源相机的图像坐标系。

- 使用双线性插值从源图像特征图中提取特征。

- 计算目标视图和源视图的单位方向向量，并标准化向量以计算光线方向差异。

- 使用池化算法聚合所有像素对齐特征，得到目标视图的图像特征 $f_{\text{img}}$：

$$
f_{\text{img}} = \text{Pooling}(f_1, \dots, f_N)
$$

#### 3.4.2 颜色预测

通过颜色MLP计算混合权重，并结合源视图的颜色信息恢复目标视图的清晰图像：

- 计算混合权重 $w_i$：

$$
w_i = \phi_{\text{color}}(f_{\text{img}}, f_{\text{grid}}, f_i)
$$

- 根据成像模型公式恢复每个像素的颜色：

$$
\hat{c} = c_{\text{clr}} \cdot \exp(-\sigma_{\text{atten}} \cdot \hat{L}) + c_{\text{bs}} \cdot \left(1 - \exp(-\sigma_{\text{bs}} \cdot \hat{L})\right)
$$

其中，$c_{\text{clr}}$ 是去除水介质后的清晰图像，$\sigma_{\text{atten}}$ 和 $\sigma_{\text{bs}}$ 分别是衰减和散射系数，$c_{\text{bs}}$ 是散射颜色。

#### 3.4.3 介质参数估计

通过介质子网络估计成像模型的参数：

- 使用球谐编码将输入方向图 $d$ 映射到高维空间：

$$
\hat{d} = \text{SHencoding}(d)
$$

- 将编码后的方向图输入介质解码器，得到基础输出 $s_{\text{base}}$：

$$
s_{\text{base}} = \text{MediumMLP}(\hat{d})
$$

- 应用不同的激活函数得到最终的介质参数：

$$
\sigma_{\text{atten}}, \sigma_{\text{bs}} = \text{Softplus}(s_{\text{base}})
$$

$$
c_{\text{bs}} = \text{Sigmoid}(s_{\text{base}})
$$

#### 3.4.4 最终重建

结合估计的介质参数和清晰图像，生成最终的水下图像：

- 恢复源视图的清晰图像 $c_{\text{clr}}^{\text{src}, i}$，并进行加权平均得到目标视图的清晰图像 $c_{\text{clr}}^{\text{tar}}$：

$$
c_{\text{clr}}^{\text{tar}} = \sum_{i=1}^N w_i \cdot c_{\text{clr}}^{\text{src}, i}
$$

- 应用衰减和散射效应重建水下图像：

$$
I_{\text{atten}} = c_{\text{clr}}^{\text{tar}} \cdot \exp(-\sigma_{\text{atten}} \cdot \hat{L}_{\text{tar}})
$$

$$
I_{\text{bs}} = c_{\text{bs}} \cdot \left(1 - \exp(-\sigma_{\text{atten}} \cdot \hat{L}_{\text{tar}})\right)
$$

$$
\hat{I} = I_{\text{atten}} + I_{\text{bs}}
$$

### 3.5 损失函数

损失函数结合了重建损失和结构相似性损失（D-SSIM），具体形式如下：

- 重建损失：

$$
L_{\text{recon}}(\hat{C}, C^*) = \left\|\frac{\hat{C} - C^*}{\text{sg}(\hat{C}) + \epsilon}\right\|^2
$$

其中，$\text{sg}(\cdot)$ 表示停止梯度，$\epsilon = 10^{-3}$。

- 最终损失函数：

$$
L = (1 - \lambda) L_{\text{recon}} + \lambda L_{\text{D-SSIM}}
$$

实验中，$\lambda$ 设为 0.2。

通过上述方法，本文有效地结合了MVS的深度估计能力和基于物理的成像模型，实现了高效且高质量的水下场景重建。

# 原文翻译

### 快速水下场景重建：多视图立体与物理成像的结合

**作者**：Shuyi Hua, Qi Liu

**单位**：华南理工大学未来技术学院，广州，511442，中国

**摘要**

水下场景重建由于光与介质之间的复杂相互作用（导致散射和吸收效应）而面临巨大挑战，这些效应使得深度估计和渲染变得更加复杂。尽管最近基于神经辐射场（Neural Radiance Fields, NeRF）的方法通过建模和分离散射介质实现了高质量的水下场景重建，但它们仍然存在训练和渲染速度慢的问题。为了解决这些限制，我们提出了一种将多视图立体（Multi-View Stereo, MVS）与基于物理的水下成像模型相结合的新方法。我们的方法包含两个分支：一个用于利用MVS的传统成本体积流程进行深度估计，另一个基于物理成像模型进行渲染。深度分支用于改善场景几何结构，而介质分支用于确定散射参数以实现精确的场景渲染。与依赖真实深度的传统MVSNet方法不同，我们的方法不需要真实深度信息，从而加快了训练和渲染过程。通过利用介质子网络估计介质参数，并结合颜色MLP进行渲染，我们恢复了水下场景的真实颜色，并实现了更高保真的几何表示。实验结果表明，我们的方法能够在散射介质中实现高质量的新视图合成，通过去除介质恢复清晰的视图，并在渲染质量和训练效率方面优于现有方法。

**关键词**：3D场景重建，新视图合成，多视图立体，水下场景重建

## 1 引言

水下场景重建是一个至关重要的研究领域，在海洋科学、水下考古学、生态学和地质学中有着广泛的应用。传统的水下探测方法主要依赖于潜水员和遥控潜水器（ROVs），然而这些方法常常受到操作限制，包括能见度低、成本高以及需要专业技能。通过开发精确的水下三维场景表示，研究人员能够更好地检查复杂的水下地形、研究海洋生物多样性以及追踪水下生态系统随时间的变化。

水下拍摄的图像通常由于水下环境的特殊挑战而质量下降。当光通过水时，会因距离而产生衰减，并且对不同波长敏感，同时还会受到后向散射的影响。衰减会导致某些颜色（尤其是红色）的丢失，而蓝色和绿色则会被增强。相反，后向散射会在图像上引入一层薄雾或霾，降低清晰度。这些效应的严重程度取决于拍摄对象与相机的距离以及其与海面的距离。由于介质的散射特性与空气不同，这些特性在尝试重建水下场景的几何结构时带来了独特的挑战。

在新视图合成（NVS）领域，神经辐射场（Neural Radiance Fields, NeRF）[1] 将场景表示为通过多层感知器（MLP）编码的连续体积场，通过体积渲染技术实现逼真的渲染。三维高斯绘制（3D Gaussian Splatting, 3D-GS）[2] 引入了可微分光栅化技术用于视图合成，利用各向异性的三维高斯原语进行显式场景表示。多视图立体（Multi-View Stereo, MVS）[3] 方法通过构建成本体积进行深度估计，将二维信息聚合为三维几何感知表示。然而，这些方法通常是为了像空气这样的清晰介质设计的，在这些介质中，光的传播基本不受介质影响。因此，这些方法在直接应用于水下场景时无法取得令人满意的结果，主要原因是三维重建过程中颜色和密度估计不准确。

最近，提出了一种基于NeRF的水下场景重建方法，称为SeaThru-NeRF [4]，通过分离介质实现了最先进的质量。然而，由于NeRF方法的固有限制，其训练和渲染速度过慢。对成像模型 [5] 的深入分析表明，深度先验起到了关键作用。MVSNet方法 [3] 是一种广泛认可的从多视角估计深度的方法，利用可微分同胚变换生成的成本体积。这表明将这两种方法结合起来具有良好的前景。

我们提出了一种将MVS与基于物理的水下成像模型相结合的新方法，该方法分为两个分支：一个分支基于MVS进行深度估计，另一个分支基于物理成像模型进行渲染。深度估计采用传统的成本体积构建流程 [3]。我们设计了一个介质子网络来估计成像模型的参数，并结合颜色MLP进行高效渲染。通过估计介质参数和场景深度，我们可以恢复场景的真实颜色，同时提高其几何表示的准确性，从而实现从新视角进行更高保真度和几何一致性的渲染。此外，我们提出的方法不依赖于MVSNet [3] 所需的真实深度，并且与SeaThru-NeRF相比，显著提高了训练和渲染速度。总之，本工作的贡献如下：

- 我们首次提出了一种用于水下场景重建的多视图立体新流程，能够在散射介质中合成新视图，并通过去除介质恢复清晰视图。

- 我们将基于物理的成像模型引入MVS，无需真实深度即可从图像中推断场景的完整外观，从而提高了水下环境重建质量和训练效率。

- 我们提出了一种能够独立检索成像模型介质参数的网络，用于水下场景重建。

## 2 相关工作

### 2.1 新视图合成（Novel View Synthesis）

新视图合成（NVS）旨在基于有限的输入视图生成场景或物体的新视角。多年来，研究人员开发了多种方法来解决这一问题，从传统的以几何为中心的方法到复杂的深度学习技术。光场方法 [6, 7] 从多个视角捕获密集的光线阵列，能够实现逼真的视图合成，但需要大量的输入数据。基于图像的渲染 [8, 9] 通过变形和混合插值视图，在中等视角变化下表现出色。然而，这两种方法在稀疏输入、遮挡和大视角偏差方面面临挑战，限制了它们在复杂场景中的适用性。深度学习的出现彻底改变了NVS，使得数据驱动的方法能够直接从图像中推断新视图 [10]。神经表示方法 [11, 12, 13, 14] 在新视图合成中被广泛使用。NeRF [1] 及其衍生方法在NVS中取得了重大突破。NeRF将场景表示为通过多层感知器（MLP）编码的连续体积场，通过体积渲染技术实现逼真的渲染 [15, 16]。效率提升一直是研究的重点，例如Mip-NeRF [15] 和Instant-NGP [17] 通过层次化采样和优化的神经表示显著加快了训练和渲染速度。PlenOctrees [18] 和Plenoxels [19] 通过预计算场景表示并将大规模环境划分为可管理的区域提高了效率。动态场景的扩展方法，如D-NeRF [20] 引入时间信息以处理运动，而NeRF-W [21] 和Block-NeRF [22] 在复杂真实场景中增强了鲁棒性。少样本和可泛化的方法，如PixelNeRF [23] 和深度监督的NeRF [24]，减少了对密集多视图数据的依赖，扩展了NeRF在稀疏视图场景中的适用性。尽管取得了这些进展，基于NeRF的方法仍然面临训练和渲染速度慢的问题，限制了其更广泛的应用。最近，3D-GS [2] 引入了可微分光栅化作为一种新的视图合成方法，利用一组各向异性的三维高斯原语进行显式场景表示。其快速的训练速度和高质量的实时渲染能力使3D-GS成为NVS领域的一个重要焦点。3D-GS方法也得到了许多改进，例如抗锯齿 [25, 26]、有效的密度控制 [27] 和稀疏视图重建 [28, 29]。然而，3D-GS的显式表示并不适合渲染半透明介质，例如存在光散射和吸收的水下场景。

### 2.2 多视图立体（Multi-View Stereo）方法

多视图立体（MVS）旨在从多个视图重建场景的密集三维表示。传统方法 [30, 31, 32, 33] 依赖于手工特征和相似性度量，例如基于体素和基于点的方法，为该领域奠定了基础。基于体素的方法 [34, 35] 评估离散三维单元的占据情况，但受限于高内存使用，而基于点的方法（如PMVS [36]）专注于扩展可靠的特征匹配，但在无纹理区域可能表现不佳。随着深度学习的发展，基于学习的方法因其灵活性成为主流，成本体积被广泛用于MVS方法中的深度估计 [37]。MVSNet [3] 首次提出了一个端到端的流程，通过构建成本体积将二维信息聚合为三维几何感知表示。后续工作在多视图三维重建中从成本体积估计深度，并从多个方面进行改进，例如通过循环平面扫描 [38, 39] 或粗到细的架构 [40, 41, 42] 降低内存消耗。然而，上述MVS网络需要真实深度作为几何监督进行训练。因此，在新视图合成领域，[43, 44, 45, 46, 47] 尝试将MVS方法与NeRF方法或3D-GS方法相结合。我们的模型从图像中直接外推场景的完整外观，无需真实深度，借鉴了这些不同方法的整合思路。

### 2.3 散射介质中的图像处理

散射介质（如水下环境）经常面临颜色偏移、图像变形和对比度降低等挑战，这些主要由复杂的光照条件引起，涉及光的散射和衰减。当前的水下图像增强（UIE）技术通常分为两类：非深度学习方法和基于深度学习的方法。基于CNN的模型 [48, 49, 50] 实现了端到端的水下图像恢复，而基于Transformer的架构 [51] 进一步改善了恢复效果。相比之下，非深度学习方法通常基于物理模型，并依赖于先验假设。例如，[52] 使用暗通道先验估计特定于水下条件的传输图，而 [5] 优化了大气散射模型以实现更准确的水下图像恢复。然而，这些非深度学习方法依赖于先验假设的准确性（如深度信息），限制了其进一步发展。这一问题可以通过三维视觉很好地解决。在水下图像处理方面，[53, 54, 55] 等工作利用成像模型取得了显著成果。最先进的基于NeRF的水下场景重建方法SeaThru-NeRF [4] 通过估计直接和后向散射分量，将成像模型 [5] 融入NeRF的渲染方程中。

## 3 方法

在本节中，我们详细介绍针对水下场景提出的多视图立体（MVS）方法，该方法利用水下图像形成模型，通过更好地考虑水下光学特性来提升MVS的性能。3.1节介绍预备知识。3.2节描述该方法的总体框架。在3.3节中，我们详细阐述使用多视图方法进行深度估计。3.4节解释水下图像形成模型的应用。最后，3.5节描述具体的实现细节。

### 3.1 预备知识

**水下图像形成模型**。水下成像系统所捕捉的图像质量会受到水下环境复杂特性的显著影响，这主要是因为水中存在光的衰减和散射现象。水下成像理论主要聚焦于这些现象，以阐明和改善水下图像质量的下降问题。

光在水中传播时强度的降低遵循指数关系，这是由两种不同的物理现象导致的：吸收和散射。吸收过程会导致光能的耗散，而散射则会使光的传播路径发生改变。光衰减是一个多方面的现象，受波长影响，在不同波长下表现出不同程度的衰减，具有选择性。在可见光谱中，红色、黄色和浅绿色比蓝色和绿色光的衰减程度更大，而蓝色和绿色光的衰减程度相对较小。因此，水下图像通常呈现蓝绿色调。光强的减弱限制了水下成像系统的工作范围。水中的悬浮颗粒和杂质是光散射的主要原因。散射效应可分为前向散射和后向散射。前向散射是指光在到达相机之前从目标表面反射时以小角度散射，从而导致图像模糊。相反，后向散射是指光线在被悬浮颗粒散射后，直接从自然或人造光源进入相机，导致图像对比度降低。

我们采用修订后的水下图像形成模型[5]作为环境光照下的通用模型。最终图像$I$可分解为直接分量和后向散射分量，如下所示：

$$
I = \overbrace{J \cdot exp(-\beta^{D}(\mathbf{v}_{D}) \cdot z)}^{\text{Direct}} + \overbrace{B^{\infty} \cdot (1 - exp(-\beta^{B}(\mathbf{v}_{B}) \cdot z))}^{\text{Backscatter}} \quad (1)
$$

其中，$J$是在深度$z$处无介质时捕捉到的清晰场景，$B^{\infty}$是无穷远处的后向散射水色。$\beta^{D}$和$\beta^{B}$分别是直接分量和后向散射分量的衰减系数。颜色将与衰减系数相乘，以表示介质对颜色的影响。向量$\mathbf{v}_{D}$和$\mathbf{v}_{B}$表示$\beta^{D}$和$\beta^{B}$与距离、物体反射率、环境光光谱、相机光谱响应以及水体的物理散射和光束衰减系数的相关性。

### 3.2 方法概述

![FastUnderwater.fig.1](FastUnderwater.fig.1.png)

> 图1：概述。我们首先使用特征金字塔网络（FPN）从源视图中提取图像特征，并将它们扭曲为扭曲特征$\{F_i^w\}_{i = 1}^{N}$。将扭曲后的特征组合成一个代价体，然后使用三维卷积神经网络（3D CNN）对其进行处理以生成深度图。随后，使用池化网络整合预测深度处每个三维点的特征，然后将它们与源图像特征相结合。在此之后，使用颜色多层感知器（MLP）和一个中等大小的子网络来分析这些特征，得到去除介质后的优化图像。最后，再次使用介质子网络对图像进行额外处理，提取后向散射图像和衰减图像。随后将它们组合起来，得到最终的重建结果。


给定多视图图像，我们的目标是从新的相机姿态在水下场景中合成目标图像。我们提出的方法的总体框架如图1所示。第一步是利用特征金字塔网络（Feature Pyramid Network, FPN）[56]从输入的多视图源图像中提取多尺度特征。随后，通过可微分同胚变换将这些特征转移到指定的相机视锥体中，构建成本体积。然后，利用三维卷积神经网络（3D CNN）对该体积进行正则化，以生成深度图。本系统的深度预测分支采用级联结构，按顺序系统地传播深度图。利用这些深度图，我们通过编码特征，为每个预测深度的三维点聚合多视图和空间信息。随后，我们利用颜色MLP和介质子网络分析编码后的特征，从而生成去除介质后的清晰图像。中间子网络对视图的视角进行编码，整合深度图，并解码中间参数。最终，再次利用介质子网络通过获取后向散射和衰减图像来增强清晰图像，随后将这些图像组合以产生最终的重建结果。

### 3.3 使用多视图立体（MVS）进行深度估计

MVS生成的深度图是我们工作流程中的一个重要组成部分，有助于与水下图像形成模型无缝集成。我们的方法基于基于学习的MVS方法的思路[3]。

**特征图**。我们使用特征金字塔网络（FPN）从输入的源多视图图像$\{I_i | i = 1, ..., N\}$中提取多尺度图像特征$\{F_i | i = 1, ..., N\}$。具体来说，每个输入图像$I_i \in \mathbb{R}^{H\times W\times 3}$通过网络处理，生成一个低分辨率特征图$F_{i,1} \in \mathbb{R}^{H/4\times W/4\times 32}$。随后，这些特征通过横向连接和上采样进行融合，生成另外两个更高分辨率的特征图：$F_{i,2} \in \mathbb{R}^{H/2\times W/2\times 16}$和$F_{i,3} \in \mathbb{R}^{H\times W\times 8}$。这种多分辨率特征表示在被传递到流水线的后续阶段之前，通过平滑卷积进行优化，以提高特征质量。多尺度特征提取器通过学习的上采样机制将高分辨率特征与低分辨率信息相结合。这种设计确保了多阶段深度预测过程中的每个阶段都能使用来自前一阶段的有意义的特征表示，从而更容易有效地提取高频特征[42]。

**代价体**。我们首先从初始场景范围深度$\{L_i | i = 1, ..., N\}$中划分出一组采样平面，然后将源视图$F_i$的图像特征扭曲到$D$个扫描平面上以构建代价体。这一步需要使用可微单应性，如下所述：

$$
\mathrm{H}_i(z) = \mathrm{K}_i\mathrm{R}_i \left( \mathrm{I} + \frac{(\mathrm{R}_i^{-1}\mathrm{t}_i - \mathrm{R}_t^{-1}\mathrm{t}_t)\mathrm{a}^T\mathrm{R}_t}{z} \right) \mathrm{R}_t^{-1} \mathrm{K}_t^{-1} \quad (2)
$$

其中$[\mathrm{K}_i, \mathrm{R}_i, \mathrm{t}_i]$和$[\mathrm{K}_t, \mathrm{R}_t, \mathrm{t}_t]$分别表示输入源视图和目标视图的相机内参、旋转和平移。$\mathrm{I}$是单位矩阵，$\mathrm{a}$是目标视图相机的主轴。我们使用矩阵$\mathrm{H}_i(z)$将源视图中$(u, v)$处的像素扭曲到采样深度$z$处的目标视图。最后，我们可以得到目标视图的扭曲特征图，其定义为：

$$
F_i^w(u, v, z) = F_i \cdot (\mathrm{H}_i(z) \cdot [u, v, 1]^T) \quad (3)
$$

为了构建代价体$C$，我们计算这些扭曲的多视图特征体$\{F_i^w | i = 1, ..., N\}$的方差，这在MVS的几何重建中被广泛使用[3, 42, 40]。对于$C$中位于坐标$(u, v, z)$处的每个体素，我们将其代价特征向量计算为：

$$
C = \frac{\sum_{i = 1}^{N} (F_i^w - \overline{F_i^w})^2}{N} \quad (4)
$$

其中$\overline{F_i^w}$是所有特征体的平均体。这个代价体是利用方差来表示不同输入视角下图像外观的差异。

这些差异是场景几何形状的变化以及与视角相关的阴影效果所导致的结果。

**从概率体进行深度预测**。在这一步中，使用三维卷积神经网络（3D CNN）来分析代价体，生成深度概率体$\hat{C}$。随后，可以基于这个概率体计算深度分布。最终的深度预测是通过使用深度概率分布对每个深度假设进行加权计算得出的。具体来说，我们通过对深度概率体进行softmax操作，得到在特定深度平面$L_i$处的概率$P_i$，即：

$$
P_i = Softmax(\hat{C}) \quad (5)
$$

目标视图中像素$(u, v)$处的深度值及其置信度定义为加权平均值$\hat{L}(u, v)$，标准差$\hat{\sigma}$通过以下公式计算：

$$
\hat{L}(u, v) = \sum_{i = 1}^{D} P_i(u, v) \cdot L_i(u, v) \quad (6)
$$

$$
\hat{\sigma}(u, v) = \sqrt{\sum_{i = 1}^{D} P_i(u, v) \cdot (L_i(u, v) - \hat{L}(u, v))^2} \quad (7)
$$

通过使用深度预测值及其方差进行计算以建立置信区间，我们可以衡量预测的不确定性，并确定物体所在的深度可能范围。即：

$$
\hat{U}(u, v) = [\hat{L}(u, v) - \lambda\hat{\sigma}(u, v), \hat{L}(u, v) + \lambda\hat{\sigma}(u, v)] \quad (8)
$$

其中$\lambda$是一个标量参数，用于设置置信区间的大小。这个深度范围将成为精细处理的初始深度范围。

**从粗到精的预测**。深度预测流水线采用级联结构开发，这使得深度图能够以从粗到精的方式传播。目标是使用多张图像准确表示场景的几何特征，并为特定视点生成详细的深度预测图。随后，我们利用先前估计的深度图构建高分辨率代价体。通过对这个细化后的体进行处理，我们能够生成更详细的深度图和三维特征体。所有深度图都存储在缓冲区中，以便在后续阶段使用。

### 3.4 水下图像形成

传统的多视图立体（MVS）方法通常仅将代价体用于几何重建，但最近的研究[44, 45, 46]表明，它也可用于推断场景的完整外观。受ENeRF[45]的启发，我们提议将水下成像模型应用于MVS，并重建整个水下场景。

在3.3节中，我们从输入源视图中获取图像特征$\{F_i | i = 1, ..., N\}$，以及目标视图的深度图$\hat{L}_{tar}$和源视图的深度图$\{\hat{L}_{src,i} | i = 1, ..., N\}$。在本节中，我们对这些内容进行进一步处理，以实现最终的重建。

为了有效地从多视图源图像特征中提取目标视图对应的像素对齐特征$\{f_i | i = 1, ..., N\}$，我们首先根据目标视图的深度图$\hat{L}_{tar}$计算三维坐标，并将它们从世界坐标系转换为源相机的图像坐标系。通过使用转换后的二维坐标，我们利用双线性插值从源图像的特征图中提取相应位置的特征。对于每个三维点，我们计算其相对于目标相机和源相机的单位方向向量。此过程包括对向量进行标准化，并计算光线方向的差异，以考虑光线方向的变化。将采样图像的提取特征与计算出的光线差异向量相结合，以创建一个连贯的特征表示。最后，我们使用池化算法聚合所有像素对齐的特征，以获得目标视图的图像特征$f_{img}$，即：

$$
f_{img} = Pooling(f_1, ..., f_N) \quad (9)
$$

类似地，我们将目标视图中每个三维点的坐标转换为二维采样网格，然后对扭曲的多视图特征体$\{F_i^w | i = 1, ..., N\}$进行采样，以在相应坐标处获得网格特征$f_{grid}$。通过获取不同位置的特征表示，我们增强了模型对场景几何结构的理解。三维点的颜色是通过将颜色从原始视点沿目标视点的路径进行扩展来确定的。具体来说，源视图的颜色由混合权重$w_i$进行调制，混合权重$w_i$通过颜色多层感知器$\varphi_{color}$计算得出：

$$
w_i = \varphi_{color}(f_{img}, f_{grid}, f_i) \quad (10)
$$

我们的模型源自3.1节中描述的图像形成模型，源视图和目标视图中每个像素的颜色遵循以下成像方程：

$$
\hat{c} = c^{\mathrm{clr}} \cdot \exp(-\sigma^{\mathrm{atten}} \cdot \hat{L}) + c^{\mathrm{bs}} \cdot (1 - \exp(-\sigma^{bs} \cdot \hat{L})) \quad (11)
$$

其中$\hat{c}$表示水中介质中的像素颜色，$c^{\mathrm{clr}}$是考虑去除水中介质后的清晰图像。方程中有三个介质参数，$\sigma^{\mathrm{atten}}$和$\sigma^{bs}$分别是衰减系数和散射系数，$c^{\mathrm{bs}}$是水中介质的散射颜色。

为了预测介质参数，我们使用一个被称为“介质模块”的独特子网络，根据图像中各个像素的方向（与世界坐标系对齐）来计算这些参数。该模块由一个编码器和一个解码器组成。介质编码器采用球谐编码[57]将输入方向图投影到更高维空间，从而增强其捕捉细微方向变化的能力，并提取不同频率的特征信息。

$$
\hat{d} = \mathrm{SHencoding}(d) \quad (12)
$$

其中$d$是输入方向图，$\hat{d}$是编码后的结果。随后，我们将$\hat{d}$输入到介质解码器中，得到一个基础输出$s_{base}$。我们对基础输出应用不同的激活函数，以获得最终的介质参数。

$$
s_{base} = \mathrm{MediumMLP}(\hat{d}), \quad (13)
$$

$$
\sigma^{\mathrm{atten}}, \sigma^{bs} = \mathrm{Softplus}(s_{base}), \quad (14)
$$

$$
c^{\mathrm{bs}} = \mathrm{Sigmoid}(s_{base}) \quad (15)
$$

基于公式(11)，我们首先恢复源视图的清晰图像$\{c_{src,i}^{\mathrm{clr}} | i = 1, ..., N\}$，然后进行加权平均操作，以获得目标视图的清晰图像$c_{tar}^{\mathrm{clr}}$。

$$
c_{tar}^{\mathrm{clr}} = \sum_{i = 1}^{N} w_i \cdot c_{src,i}^{\mathrm{clr}} \quad (16)
$$

我们继续使用公式(11)来重建水中介质的图像。具体来说，应用衰减系数来降低图像的清晰度，以获得衰减后的图像，然后将其与预测的散射图像相结合，以产生最终的重建结果。

$$
\hat{I} = I_{\mathrm{atten}} + I_{bs}, \quad (17)
$$

$$
I_{\mathrm{atten}} = c_{tar}^{\mathrm{clr}} \cdot \exp(-\sigma^{\mathrm{atten}} \cdot \hat{L}_{tar}), \quad (18)
$$

$$
I_{bs} = c^{bs} \cdot (1 - \exp(-\sigma^{\mathrm{atten}} \cdot \hat{L}_{tar})) \quad (19)
$$

其中$\hat{I}$表示最终的重建结果，而$I_{\mathrm{atten}}$和$I_{bs}$分别表示衰减图像和散射图像。

### 3.5 损失函数

我们的损失函数在3D - GS方法的基础上做了一些改进。在最初的3D - GS方法[2]中，损失函数通常由$L_1$损失和D - SSIM损失组成，如下所示：

$$
\mathcal{L} = (1 - \lambda)\mathcal{L}_1 + \lambda\mathcal{L}_{\text{D-SSIM}} \quad (20)
$$

我们一般将$\lambda$设为0.2。受文献[58]的启发，应用一个对暗区误差进行重度惩罚的损失函数，按照人类感知压缩动态范围的方式，能够提升低光照场景的重建质量。更具体地说，我们采用如下的重建损失：

$$
\mathcal{L}_{\text{recon}} (\hat{C}, C^*) = \left( \frac{\hat{C} - C^*}{\text{sg}(\hat{C}) + \epsilon} \right)^2 \quad (21)
$$

其中$\text{sg}(\cdot)$表示停止梯度，$\epsilon = 10^{-3}$，$\hat{C}$和$C^*$分别表示重建的像素颜色和真实的像素颜色。我们使用重建损失代替$L_1$损失来构建最终的损失函数，如下所示：

$$
\mathcal{L} = (1 - \lambda)\mathcal{L}_{\text{recon}} + \lambda\mathcal{L}_{\text{D-SSIM}} \quad (22)
$$

在我们所有的测试中，$\lambda$都设为0.2。

## 4 实验

### 4.1 实验设置

**SeaThru-NeRF数据集**

SeaThru-NeRF [4] 发布了一个真实前向视角 [1] 的数据集，包含四个在不同海域拍摄的多视图水下场景：IUI3红海、库拉索岛、日本花园红海和巴拿马。这四个场景分别包含29张、20张、20张和18张图像，其中25张、17张、17张和15张图像用于训练，剩余的4张、3张、3张和3张图像用于验证。所有图像均使用Nikon D850单反相机以RAW格式拍摄，并且相机被安装在带有穹顶端口的Nauticam水下外壳中，有效减少了可能干扰针孔相机模型的折射效应。RAW图像随后被下采样至大约900×1400的分辨率。在进一步处理之前，线性输入图像通过0.5%的通道剪辑进行白平衡处理，以消除极端噪声像素。最后，使用COLMAP [32] 估计相机姿态。

**实现细节**

我们的方法使用PyTorch实现，并在单个RTX 3090 GPU上进行训练。我们使用Adam [59] 优化器，并为每个场景训练3k次迭代。在实践中，我们在粗、细两级分别使用16个和8个深度平面构建成本体积。在训练期间，随机选择2个、3个或4个源视图作为输入，概率分别为0.1、0.8和0.1。对于介质子网络，方向编码器使用4级球谐编码。介质解码器是一个包含两个线性层的MLP：第一层包含128个隐藏单元，第二层包含64个隐藏单元，均使用ReLU激活函数。颜色MLP包含两个线性层，每个层有24个隐藏单元，使用ReLU激活函数。

**基线方法和评估指标**

为了展示针对水下场景的改进，我们首先将我们的方法与其他基于MVS的方法进行比较，例如ENeRF [45] 和MVSGaussian [46]，以及广泛使用的3D重建方法3D-GS。最重要的是，我们与SeaThru-NeRF [4] 进行了深入比较，这是最先进的基于NeRF的水下场景重建方法，以突出我们方法的进步。在评估中，我们采用了先前工作中建立的标准，包括ENeRF [45]、MVSNeRF [44] 和MVSGaussian [46]。对于真实前向视角 [1] 数据集，由于输入视图中通常看不到图像的边缘区域，我们专注于评估图像中心80%的区域。我们使用广泛使用的PSNR、SSIM [60] 和LPIPS [61] 指标来比较合成视图的质量。所有模型都在相同的GPU上使用相同的数据集进行训练，以确保公平性。

### 4.2 结果

![fig.2](FastUnderwater.fig.2.png)

> 图2：恢复结果和深度图。我们通过展示去除介质后的渲染结果，将我们的方法与SeaThru - NeRF进行对比。在每张图像下方，我们给出了相应的深度图。我们的恢复技术有效地保留了更丰富的颜色细节。与SeaThru - NeRF相比，深度渲染结果表现出更高的平滑度和一致性。

**定量结果**

![table.1](FastUnderwater.table.1.png)

> 表1：在SeaThru - NeRF数据集上的定量评估结果。$\uparrow$表示值越大越好，$\downarrow$表示相反情况。加粗的值表示最佳结果，下划线的值表示次佳结果。

为了评估我们方法的有效性，我们使用标准的SeaThru-NeRF数据集，将其渲染质量与几种基线方法进行了比较。表1展示了在四个不同场景中对新视图合成的PSNR、SSIM、LPIPS和平均训练时间的详细对比。结果清楚地展示了我们方法的优势，它在大多数场景中的表现超过了其他方法，并且在训练效率方面表现出显著提升。我们的方法表现优异，相比于领先的基于NeRF的水下场景重建方法SeaThru-NeRF，平均PSNR提升了1.65dB。结合图4中的场景，我们可以看到PSNR的提升主要体现在两个更深的场景中。最为重要的是，我们的方法显著减少了训练所需的时间。尽管我们的方法并非在所有指标中都表现最佳，但其稳定的性能使其紧随领先者之后，排名第二，且总体质量更为出色。此外，我们的方法超越了其他MVS方法，证明了将MVS与基于物理的成像模型相结合能够有效增强水下场景重建的能力。

![fig.3](FastUnderwater.fig.3.png)

> 图3：远处细节的渲染。我们将我们的方法与几种基线方法进行比较。我们的方法在渲染质量上优于它们，并且能更好地保留远处的几何细节。

图4展示了在水介质中的新视图渲染效果。为了进一步强调我们方法的优越性，我们放大了图3中标注为红色方框的区域。如图3所示，我们的方法在远处区域保持了清晰的几何特征，展示了其在深水和复杂环境中精确表示复杂场景几何结构的能力。此外，我们的方法成功地将物体从散射介质中分离出来。特别是，我们努力消除图像中的水的存在，从而呈现出更清晰的恢复结果，这些结果基于我们成像模型估计的介质参数。

![fig.4](FastUnderwater.fig.4.png)

> 图4：水中介质的新视图合成。从左到右的各列展示了“Cuecaço”、“IUI3红海”、“日本花园红海”和“巴拿马”场景的水下场景渲染结果。红色方框中突出显示的远处细节的渲染质量，已在图3中进行了比较和展示。

图2展示了我们方法在恢复水下场景颜色方面的潜力。尽管SeaThru-NeRF的恢复图像可能看起来是灰度的，但我们的方法保留了更多的颜色细节。此外，与SeaThru-NeRF生成的深度图相比，我们的深度图表现出更高的平滑度和一致性，从而暗示了我们在深度估计方面更高的准确性。这对于我们的基于物理的模型来说是一个特别的优势，因为将MVS的深度估计能力整合到物理成像模型中是我们方法成功的关键。

### 消融研究

![table.2](FastUnderwater.table.2.png)

> 表2：“IUI3红海”场景的消融研究。

我们在“IUI3红海”场景上进行了消融实验，以验证我们框架中三个关键组件的贡献：级联结构、重建损失和介质子网络。如表2所示，级联结构和重建损失均有助于提升重建质量，这通过PSNR指标得到了体现。然而，最大的优势来源于我们提出的介质子网络，它提供了最为显著的性能提升。这突出了介质子网络在增强我们方法有效性方面发挥的关键作用，尤其是在与基于物理的成像模型结合时。

## 5 结论

我们提出了一种将MVS与基于物理的成像模型相结合的水下场景重建方法。通过将MVS的深度估计能力直接嵌入成像模型，我们的方法实现了两个关键目标：在散射介质中生成高质量的新视图渲染，并准确恢复水下场景的真实颜色。这种双重能力不仅提升了渲染场景的视觉保真度，还为水下环境提供了更准确的表示，克服了与散射效应和颜色失真相关的挑战。除了在渲染质量上的进步，我们的方法相较于顶级的基于NeRF的方法，在训练效率上取得了显著提升。这种效率使得我们的方法能够在不牺牲结果的准确性或质量的前提下，实现更快的处理速度，从而使其在实际应用中既实用又高效。总之，我们的方法凭借其卓越的渲染质量、高效的训练以及对散射介质的适应性，成为了水下场景重建的一个极为有效的解决方案。

尽管我们的方法取得了进步，但水下场景重建仍有以下改进方向：首先，我们在处理远处具有相似背景的物体时仍面临挑战。例如，在深度图中，远处的蓝色水体特征被错误地识别为更靠近的位置（参见图2）。这一问题部分归因于特征提取阶段，尤其是通过FPN，它受到介质的影响，从而影响了特征提取的准确性。因此，未来的研究将致力于开发改进的FPN，以实现更准确的水下环境特征提取。其次，获取去除水后的高质量水下场景数据集存在挑战，这限制了训练数据的质量，并影响了颜色恢复的准确性。在未来的研究中，使用包含水下图像及其对应的去水图像的数据集，预计将显著提高颜色恢复的精度。
---
layout: mypost
title: k027 SeaSplat, Representing Underwater Scenes with 3D Gaussian Splatting and a Physically Grounded Image Formation Model
categories: [3DGS, 水下]
---

如果有研究水下场景重建的研究者，可以通过邮箱zhangwenniu@163.com联系我，我目前创建了一个水下场景重建的交流群，欢迎感兴趣的研究者加入。

# 论文链接

- [论文链接](https://arxiv.org/abs/2409.17345)

- [Github链接](https://github.com/dxyang/seasplat/)

- [项目主页](https://seasplat.github.io/)



## 论文的重点和难点内容

### **重点内容**

1. **方法的核心创新点**：

   - **结合3D高斯点渲染和水下图像形成模型**：SeaSplat将3D高斯点渲染（3DGS）与基于物理的水下图像形成模型相结合，通过同时学习场景的3D表示和水的物理参数（衰减和反向散射），恢复场景的真实颜色并生成高质量的新型视图。

   - **实时渲染能力**：该方法继承了3D高斯点渲染的高效性，能够在实时环境中快速渲染水下场景，同时显著优于其他基于神经辐射场（NeRF）的方法。

2. **实验验证**：

   - **多数据集验证**：作者在多个真实世界和模拟的数据集上验证了SeaSplat的性能，包括SeaThru-NeRF数据集（红海、加勒比海和太平洋的水下场景）、由水下机器人拍摄的水下场景，以及模拟的水下和雾场景。

   - **性能优势**：SeaSplat在新型视图合成任务中表现出色，与现有的水下图像恢复方法（如SeaThru-NeRF）相比，在所有数据集上都取得了更高的PSNR、SSIM和更低的LPIPS，同时保持了实时渲染的能力。

3. **实际应用前景**：

   - **水下机器人导航和环境监测**：SeaSplat能够恢复水下场景的真实颜色，并生成高质量的新型视图，这为水下机器人导航、环境监测和目标跟踪等任务提供了新的解决方案。

   - **动态场景和复杂光学现象的扩展**：尽管当前方法主要针对静态水下场景，但作者指出未来可以扩展到动态场景（如摇曳的珊瑚）和复杂的光学现象（如水面的透镜效应和阴影）。

### **难点内容**

1. **水下图像形成模型的复杂性**：

   - **物理过程建模**：水下图像形成模型需要准确描述光在水中传播时的衰减和反向散射过程。这些物理过程与距离、波长（颜色）密切相关，且反向散射会导致图像出现模糊或雾蒙蒙的效果。准确建模这些过程是恢复场景真实颜色的关键。

   - **参数估计**：模型中的衰减系数（βD）和反向散射系数（βB）需要从数据中估计。这些参数的估计不仅需要考虑图像的颜色和深度信息，还需要避免优化过程中的局部最优解。

2. **优化问题**：

   - **多目标优化**：SeaSplat需要同时优化3D高斯点的参数（位置、颜色、透明度等）和水的物理参数（衰减和反向散射系数）。优化过程中需要平衡多个目标，例如恢复场景的真实颜色、生成高质量的新型视图以及保持计算效率。

   - **深度估计的准确性**：在水下环境中，由于反向散射和衰减的影响，深度信息的估计变得更加困难。SeaSplat通过引入额外的损失函数（如深度加权重建损失、边缘感知平滑损失等）来约束深度估计，但如何在复杂的水下场景中准确估计深度仍然是一个挑战。

3. **实时性与计算效率**：

   - **实时渲染要求**：尽管3D高斯点渲染本身具有高效的渲染能力，但引入水下图像形成模型后，如何在保持实时渲染的同时，准确恢复场景的真实颜色是一个难点。SeaSplat通过优化算法和计算策略，在实时性和准确性之间取得了较好的平衡。

   - **内存和时间开销**：与基于NeRF的方法相比，SeaSplat显著降低了训练和推理的内存和时间开销。然而，如何进一步优化计算效率，使其能够应用于实际的水下机器人系统，仍然是一个需要解决的问题。

4. **动态场景和复杂光学现象的处理**：

   - **动态物体的建模**：当前的SeaSplat方法主要针对静态水下场景。如何扩展到动态场景（如摇曳的珊瑚、游动的鱼类等）是一个重要的研究方向。动态场景的建模需要考虑时间维度的信息，以及物体运动对光传播的影响。

   - **复杂光学现象的建模**：水下环境中还存在其他复杂的光学现象，如水面的透镜效应、阴影等。如何将这些现象纳入模型，并准确恢复场景的真实颜色，是未来工作的重点之一。

## 总结

SeaSplat通过结合3D高斯点渲染和水下图像形成模型，成功地恢复了水下场景的真实颜色，并生成了高质量的新型视图。然而，该方法在物理过程建模、优化问题、实时性与计算效率，以及动态场景和复杂光学现象的处理等方面仍面临一些挑战。这些难点为未来的研究提供了重要的方向。

# 详细介绍

### **1. 论文信息**

**标题：**  

SeaSplat: Representing Underwater Scenes with 3D Gaussian Splatting and a Physically Grounded Image Formation Model

**作者：**  

Daniel Yang¹², John J. Leonard¹, Yogesh Girdhar²  

¹ Massachusetts Institute of Technology (MIT)  
² Woods Hole Oceanographic Institution (WHOI)  

**联系方式：**  

{dxyang, jleonard}@mit.edu  
{dayang, yogi}@whoi.edu  

**代码和数据：**  

[https://seasplat.github.io](https://seasplat.github.io)

## **2. 研究背景**

水下视觉感知是海洋机器人技术中的一个重要领域，广泛应用于环境监测、目标跟踪和水下探索等任务。然而，水下图像的成像过程与陆地环境（空气）有很大不同，主要面临以下挑战：

- **光的衰减**：光在水中的传播会因距离和波长而衰减，导致图像颜色失真（红色光衰减快，蓝色光衰减慢）。  

- **反向散射**：水中的悬浮颗粒会使光散射到相机中，导致图像出现雾蒙蒙的效果。  

- **深度依赖效应**：这些效应随着相机与目标之间的距离和水深而变化。

传统的计算机视觉方法在水下环境中表现不佳，因此需要专门针对水下场景的解决方案。近年来，神经辐射场（NeRF）和3D高斯点渲染（3DGS）等技术在3D场景重建和新型视图合成中取得了显著进展。然而，这些方法大多假设成像环境为空气，不适用于水下场景。



## **3. 研究目标**

本文提出了一种名为 **SeaSplat** 的方法，旨在结合3D高斯点渲染和基于物理的水下图像形成模型，实现水下场景的真实颜色恢复和高质量新型视图合成。具体目标包括：

1. 恢复水下场景的真实颜色，去除水的干扰效应。

2. 生成高质量的新型视图，同时保持实时渲染能力。

3. 在真实世界和模拟数据集上验证方法的有效性。



## **4. 研究方法**

### **4.1 3D高斯点渲染（3DGS）**

3D高斯点渲染是一种高效的3D场景表示方法，通过一组3D高斯分布来表示场景。每个高斯分布具有以下参数：

- **均值（µ）**：表示位置。  

- **协方差（Σ）**：表示形状和方向。  

- **颜色（c）**：由球谐系数表示，无视角依赖。  

- **透明度（o）**：表示不透明度。

给定一个相机视角，可以通过以下步骤渲染图像：

1. 按照从前往后的顺序对高斯分布进行排序。  

2. 将高斯分布投影到相机平面上。  

3. 使用alpha合成计算每个像素的颜色。

3DGS的优化目标是通过最小化重建误差来优化高斯分布的参数。然而，传统3DGS假设成像环境为空气，不适用于水下场景。

### **4.2 水下图像形成模型**

水下图像形成模型描述了光在水中传播时的物理过程，包括衰减和反向散射。具体公式如下：

$$ I = J \cdot e^{-\beta_D \cdot Z} + B_\infty \cdot (1 - e^{-\beta_B \cdot Z}) $$

其中：

- **$I$** 是捕获的图像。  
- **$J$** 是场景的真实颜色（无水时的颜色）。  
- **$\beta_D$** 是衰减系数。  
- **$\beta_B$** 是反向散射系数。  
- **$B_{\infty}$** 是无穷远处的反向散射颜色。  
- **$Z$** 是深度图。

该模型表明，水下图像由两部分组成：

1. **直接图像（$D$）**：由真实颜色衰减得到。  

2. **反向散射图像（$B$）**：由反向散射光组成。

### **4.3 SeaSplat方法**

SeaSplat的核心思想是将3D高斯点渲染与水下图像形成模型相结合，通过学习场景的3D表示和水的物理参数，恢复场景的真实颜色并生成高质量的新型视图。具体步骤如下：

1. **学习场景的3D高斯表示**：给定一组输入图像、相机内参和外参，优化3D高斯分布的参数，以重建场景的真实颜色。  

2. **估计水的物理参数**：同时学习衰减系数（$\beta_D$）、反向散射系数（$\beta_B$）和反向散射颜色（$B_{\infty}$）。  

3. **生成水下图像**：通过将真实颜色与水的物理效应结合，生成水下图像，并通过光度误差优化模型参数。

### **4.4 优化策略**

为了优化模型参数，SeaSplat引入了多种损失函数：

1. **光度误差损失（L1 + D-SSIM）**：用于优化3D高斯分布的重建误差。  

2. **反向散射损失（$L_{bs}$）**：基于暗通道先验，优化反向散射参数。  

3. **颜色损失（$L_{gw} + L_{sat}$）**：基于灰世界假设和颜色饱和度约束，优化真实颜色的恢复。  

4. **深度加权重建损失（$L_{Z-recon}$）**：强调远距离区域的细节恢复。  

5. **边缘感知平滑损失（$L_{Z-smooth}$）**：约束深度图的平滑性。  

6. **背景损失（$L_{op}$）**：避免在水柱中生成不必要的高透明度高斯分布。

总损失函数为：

$$ L = L_{GS} + L_{bs} + L_{gw} + L_{sat} + L_{op} + L_{Z-smooth} + L_{Z-recon} $$



## **5. 实验验证**

### **5.1 数据集**

1. **SeaThru-NeRF数据集**：包含四个真实世界的水下场景，分别位于红海（Japanese Gardens和IUI3）、加勒比海（Curac¸ao）和太平洋（Panama）。  

2. **Salt Pond Bay数据集**：由水下机器人在美属处女岛拍摄的水下场景，包含向下拍摄的海底图像。  

3. **模拟数据集**：基于Mip-NeRF 360的户外花园场景，通过添加模拟的水和雾效果生成。

### **5.2 比较方法**

1. **SeaThru-NeRF**：将水下图像形成模型与神经辐射场（NeRF）结合的方法。  

2. **3D高斯点渲染（3DGS）**：传统的3D高斯点渲染方法。

### **5.3 评估指标**

1. **PSNR（峰值信噪比）**：越高越好。  

2. **SSIM（结构相似性指数）**：越高越好。  

3. **LPIPS（感知距离）**：越低越好。


### **5.4 实验结果**
1. **新型视图合成**：SeaSplat在所有数据集上均取得了优于SeaThru-NeRF和3DGS的结果。

2. **深度估计**：SeaSplat生成的深度图比3DGS更平滑，且避免了在水柱中生成不必要的高透明度高斯分布。  

3. **真实颜色恢复**：SeaSplat能够更好地恢复场景的真实颜色，尤其是在远距离区域，细节和颜色的恢复效果显著优于SeaThru-NeRF。

4. **计算效率**：SeaSplat保持了3D高斯点渲染的高效性，训练时间为1小时25分钟，推理时间为0.012秒，显存占用为4GB。相比之下，SeaThru-NeRF的训练时间长达21小时，推理时间为1
0.184秒，显存占用高达33.2GB。



## **6. 讨论与未来工作**

### **6.1 方法的优势**

1. **高效性**：SeaSplat继承了3D高斯点渲染的实时渲染能力，显著优于基于NeRF的方法。  

2. **准确性**：通过结合水下图像形成模型，能够更准确地恢复场景的真实颜色，并生成高质量的新型视图。  

3. **泛化能力**：在真实世界和模拟数据集上均表现出色，证明了方法的泛化能力。

### **6.2 方法的局限性**

1. **动态场景**：当前方法主要针对静态水下场景，尚未考虑动态物体（如摇曳的珊瑚、游动的鱼类）的影响。  

2. **复杂光学现象**：尚未建模水面的透镜效应和阴影等复杂光学现象。  

3. **实时性**：尽管SeaSplat已经显著降低了计算开销，但在实际水下机器人应用中，仍需进一步优化实时性。

### **6.3 未来工作**

1. **动态场景扩展**：考虑时间维度的信息，扩展方法以处理动态水下场景。  

2. **复杂光学现象建模**：将水面透镜效应和阴影等复杂光学现象纳入模型。  

3. **实时性优化**：进一步优化计算效率，使其能够实时应用于水下机器人系统。



## **7. 总结**

SeaSplat通过结合3D高斯点渲染和基于物理的水下图像形成模型，成功地恢复了水下场景的真实颜色，并生成了高质量的新型视图。该方法在多个真实世界和模拟数据集上表现出色，同时保持了高效的计算性能。尽管当前方法主要针对静态场景，但其在水下视觉感知领域的应用前景广阔，为未来的研究提供了重要的方向。

# 方法部分详解

论文的核心方法是 **SeaSplat**，它结合了 **3D 高斯点渲染（3D Gaussian Splatting, 3DGS）** 和 **基于物理的水下图像形成模型**，以实现水下场景的真实颜色恢复和高质量新型视图合成。以下是对方法部分的详细解析：



### **1. 方法概述**

SeaSplat 的目标是通过学习场景的 3D 高斯表示和水的物理参数（如衰减和反向散射），恢复水下场景的真实颜色，并生成高质量的新型视图。具体来说，该方法通过以下步骤实现：

1. 使用 3D 高斯点渲染（3DGS）表示场景的几何和颜色信息。

2. 结合水下图像形成模型，模拟光在水中传播时的衰减和反向散射效应。

3. 通过优化损失函数，同时学习场景的 3D 表示和水的物理参数，以恢复场景的真实颜色并生成高质量的新型视图。



### **2. 3D 高斯点渲染（3DGS）**

3D 高斯点渲染是一种高效的 3D 场景表示方法，通过一组 3D 高斯分布来表示场景。每个高斯分布具有以下参数：

- **均值（$\mu$）**：表示高斯分布的中心位置。

- **协方差（$\Sigma$）**：表示高斯分布的形状和方向。

- **颜色（$c$）**：由球谐系数表示，无视角依赖。

- **透明度（$o$）**：表示高斯分布的不透明度。

给定一个相机视角，可以通过以下步骤渲染图像：

1. **排序**：将高斯分布按照从前往后的顺序排序。

2. **投影**：将高斯分布投影到相机平面上。

3. **合成**：使用 alpha 合成计算每个像素的颜色：

   $$
   C(x) = \sum_{i=1}^{N} c_i \alpha_i(x) \prod_{j=1}^{i-1} (1 - \alpha_j(x))
   $$

   其中，$ c_i $ 和 $ \alpha_i $ 分别是第 $ i $ 个高斯分布的颜色和透明度。

3DGS 的优化目标是通过最小化重建误差来优化高斯分布的参数：

$$
L_{\text{original}} = (1 - \lambda) L_1 + \lambda L_{\text{D-SSIM}}
$$

其中，$ L_1 $ 是像素级的重建误差，$ L_{\text{D-SSIM}} $ 是结构相似性损失。



### **3. 水下图像形成模型**

水下图像形成模型描述了光在水中传播时的物理过程，包括衰减和反向散射。具体公式如下：

$$
I = J \cdot e^{-\beta_D \cdot Z} + B_\infty \cdot (1 - e^{-\beta_B \cdot Z})
$$

其中：
- **$ I $** 是捕获的图像。

- **$ J $** 是场景的真实颜色（无水时的颜色）。

- **$ \beta_D $** 是衰减系数，表示光在水中传播时的衰减。

- **$ \beta_B $** 是反向散射系数，表示水中的悬浮颗粒对光的散射。

- **$ B_\infty $** 是无穷远处的反向散射颜色。

- **$ Z $** 是深度图，表示相机到场景的距离。

该模型表明，水下图像由两部分组成：

1. **直接图像（$ D = J \cdot e^{-\beta_D \cdot Z} $）**：由真实颜色衰减得到。

2. **反向散射图像（$ B = B_\infty \cdot (1 - e^{-\beta_B \cdot Z}) $）**：由反向散射光组成。



### **4. SeaSplat 方法的核心**

SeaSplat 的核心思想是将 3D 高斯点渲染与水下图像形成模型相结合，通过学习场景的 3D 表示和水的物理参数，恢复场景的真实颜色并生成高质量的新型视图。具体步骤如下：

1. **学习场景的 3D 高斯表示**：给定一组输入图像、相机内参和外参，优化 3D 高斯分布的参数，以重建场景的真实颜色 $ \hat{J} $ 和深度图 $ \hat{Z} $。

2. **估计水的物理参数**：同时学习衰减系数 $ \beta_D $、反向散射系数 $ \beta_B $ 和反向散射颜色 $ B_\infty $。

3. **生成水下图像**：通过将真实颜色与水的物理效应结合，生成水下图像 $ \hat{I} $，并使用光度误差优化模型参数。



### **5. 优化策略**

为了优化模型参数，SeaSplat 引入了多种损失函数，以确保学习到的 3D 高斯表示和水的物理参数能够准确恢复场景的真实颜色并生成高质量的新型视图。

#### **5.1 损失函数**

1. **光度误差损失（Photometric Loss）**：

   $$
   L_{\text{original}} = (1 - \lambda) L_1 + \lambda L_{\text{D-SSIM}}
   $$

   用于优化 3D 高斯分布的重建误差。

2. **反向散射损失（Backscatter Loss）**：

   $$
   L_{\text{bs}} = \sum_{(i, j)} \sum_{c} \left( \max \{ \hat{D}_c(i, j), 0 \} + k \min \{ \hat{D}_c(i, j), 0 \} \right)
   $$

   其中，$ \hat{D} = I - \hat{B} $ 是去除反向散射后的直接图像，$ \hat{B} $ 是反向散射图像，$ k > 1 $ 是超参数。

3. **颜色损失（Color Loss）**：

   - **灰世界假设（Gray World Hypothesis）**：

     $$
     L_{\text{gw}} = \frac{1}{3} \sum_{c} \left( \frac{1}{N} \sum_{i, j} \hat{J}_c(i, j) - 0.5 \right)^2
     $$

   - **颜色饱和度损失（Saturation Loss）**：

     $$
     L_{\text{sat}} = \sum_{i, j} \sum_{c} \max \{ \hat{J}_c(i, j) - T_{\text{sat}}, 0 \}
     $$

     其中，$ T_{\text{sat}} $ 是颜色饱和度阈值。

4. **深度加权重建损失（Depth-Weighted Reconstruction Loss）**：

   $$
   L_{\text{Z-recon}} = \| Z_{\text{detach}} \cdot (I - \hat{I}) \|_1
   $$

   用于强调远距离区域的细节恢复。

5. **边缘感知平滑损失（Edge-Aware Smoothness Loss）**：

   $$
   L_{\text{Z-smooth}} = \sum_{i, j} \left( e^{-\vert \nabla_x I\vert } \vert \nabla_x \hat{Z}\vert  + e^{-\vert \nabla_y I\vert } \vert \nabla_y \hat{Z}\vert  \right)
   $$

   用于约束深度图的平滑性。

6. **背景损失（Background Loss）**：

   $$
   L_{\text{op}} = \sum_{i, j} \alpha(i, j) \cdot \mathbb{1}_{\|I(i, j) - B_\infty\|_2^2 < T_{\text{sim}}}
   $$

   用于避免在水柱中生成不必要的高透明度高斯分布。

最终的总损失函数为：

$$
L = L_{\text{GS}} + L_{\text{bs}} + L_{\text{gw}} + L_{\text{sat}} + L_{\text{op}} + L_{\text{Z-smooth}} + L_{\text{Z-recon}}
$$



### **6. 实现细节**

1. **深度图渲染**：通过将高斯分布的颜色替换为深度值，实现深度图的渲染。

2. **衰减和反向散射系数的实现**：使用卷积核实现衰减和反向散射系数的计算。

3. **优化策略**：交替优化 3D 高斯分布的参数和水的物理参数，以确保全局最优解。



### **7. 方法的优势**

1. **高效性**：继承了 3D 高斯点渲染的实时渲染能力，显著优于基于 NeRF 的方法。

2. **准确性**：通过结合水下图像形成模型，能够更准确地恢复场景的真实颜色。

3. **泛化能力**：在多个真实世界和模拟数据集上表现出色，证明了方法的泛化能力。



### **8. 方法的局限性**

1. **动态场景**：当前方法主要针对静态水下场景，尚未考虑动态物体（如摇曳的珊瑚、游动的鱼类）的影响。

2. **复杂光学现象**：尚未建模水面的透镜效应和阴影等复杂光学现象。

3. **实时性**：尽管 SeaSplat 已经显著降低了计算开销，但在实际水下机器人应用中，仍需进一步优化实时性。


通过以上方法，SeaSplat 成功地将 3D 高斯点渲染与水下图像形成模型相结合，实现了水下场景的真实颜色恢复和高质量新型视图合成。

# 原文翻译

## 海下场景表示：结合3D高斯点绘制与基于物理的图像形成模型的SeaSplat

**丹尼尔·杨¹²，约翰·J·莱昂纳德¹，以及约格什·吉尔达²**

**摘要**  
我们介绍了SeaSplat，这是一种利用3D辐射场最新进展实现水下场景实时渲染的方法。水下场景是具有挑战性的视觉环境，因为通过水这种介质进行渲染会引入与距离和颜色相关的图像捕获效应。我们通过基于物理的水下图像形成模型约束了3D高斯点绘制（3DGS），3DGS是辐射场的最新进展，能够快速训练并实时渲染完整的3D场景。我们将SeaSplat应用于SeaThru-NeRF数据集的真实场景、美属处女岛水下机器人采集的场景以及模拟退化的现实场景中，不仅在存在介质的情况下从场景中渲染新视点的定量性能上有所提升，还能够恢复场景的底层真实颜色，并将渲染结果恢复到没有介质干扰的状态。我们展示了水下图像形成有助于学习场景结构，生成更好的深度图，并且我们的改进保持了利用3D高斯表示所赋予的显著计算优势。代码、数据和可视化内容可在[https://seasplat.github.io](https://seasplat.github.io)获取。

## I. 引言

配备成像能力的水下机器人越来越多地被部署用于复杂和适应性任务，包括环境监测 [1]–[3]、视觉目标跟踪 [4]、[5] 和水下环境探索 [6]、[7]。然而，水下视觉感知与在空气中收集的图像有显著不同的特征，这使得直接将计算机视觉领域的许多进展应用到这一领域变得具有挑战性 [8]、[9]。

由于通过介质（水）成像的固有影响，水下收集的图像会出现退化。光在水中的传播受到距离依赖和光谱敏感的衰减以及反向散射的影响。实际上，衰减导致水下图像缺乏某些颜色，特别是红色，而蓝色和绿色则过多，而反向散射导致水下图像呈现出一种薄雾或朦胧的效果。这些效应的大小随着相机拍摄对象的距离以及海面深度的变化而变化。

辐射场方法，通过使用神经网络参数化体积密度和颜色（如NeRF [10]），在重建3D场景和实现高质量新视点合成方面取得了显著进展。最近，3D高斯点绘制（3D Gaussian Splatting）[11] 在实现这些3D环境的实时、逼真渲染方面取得了成功。然而，这些方法通常假设数据是在大气条件下（例如，通过空气成像）收集的，其中颜色随距离变化相对恒定，当应用于水下环境时会导致不理想的效果，如浮动物和其他几何不一致性。我们提出了SeaSplat，这是一种将3D高斯点绘制与基于物理的水下图像形成模型相结合的方法。通过同时学习介质的参数以及底层3D表示，我们可以恢复场景的真实颜色，同时更准确地估计场景的几何，如通过更高保真度和几何一致的新视点渲染所示。我们强调了我们的方法在由潜水员在各种珊瑚环境中收集的真实水下数据、在户外环境中模拟的场景以及由自主水下机器人收集的数据上估计场景真实颜色的能力。

## II. 相关工作

### A. 辐射场

神经辐射场（NeRF）[10] 在 3D 重建和新视点合成方面取得了显著进展。NeRF 使用多层感知机（MLP）隐式地编码场景的体积密度和颜色。为了渲染一个视点，通过沿射线对体积进行密集采样，得到的样本用于查询 MLP，然后沿每条射线聚合。可以在这种可微分渲染的图像上应用光度误差损失，以学习 MLP 的权重。这一过程计算成本高昂，尤其是在对 MLP 进行密集采样和查询时。许多 NeRF 的扩展方法在提高质量和速度方面取得了成功 [12]–[16]。对于这些方法的更深入综述，我们推荐读者参考 [17] 和 [18]。与这种范式不同，最近的一项进展——3D 高斯点绘制（3D Gaussian Splatting）[11]，明确地使用 3D 高斯分布对辐射场进行参数化，可以通过光栅化高效地渲染，同时保持最先进的视觉质量。


### B. 水下色彩恢复

水下图像的色彩校正 [19]、[20] 依赖于地面图像的去雾方法 [21]、[22]。去雾方法试图将观测图像分解为场景辐射、大气光和传输项，其中传输项随深度指数衰减，由一个与波长无关的衰减系数控制。然而，[23] 指出这种模型对于水下领域是不足够的。相反，他们提出了一个修正模型，其中衰减系数是波长依赖的，即随颜色通道变化，且控制衰减和大气光的传输系数也不同。然而，即使是一个能够捕捉水下图像形成细微差别的更复杂模型，参数估计仍然具有挑战性。如果在多个距离处放置色卡，可以估计模型的参数 [23]。进一步的研究实现了在有深度信息时的参数估计 [24]。还有一些研究使用基于学习的方法进行色彩校正 [25]、[26]。这些方法将问题表述为学习两个数据分布之间的映射，使用生成对抗网络 [27]。然而，这些方法不能保证在不同场景或从训练数据中得到的环境条件下介质效应的一致性。最近的一些工作将水下色彩恢复与神经辐射场相结合。SeaThru-NeRF [28] 将水下图像形成模型整合到神经辐射场框架中 [16]。他们通过按视图方向采样介质参数，而不是像通常那样按射线密集采样，来增强 NeRF，实现了图像的色彩校正以及新视点合成。另一项工作 [29] 也类似地将水下图像形成模型整合到神经辐射场框架中，并学习场景的物理属性，如反照率。然而，[29] 关注的是由与相机移动的光源引起的反向散射效应，更适合于捕捉水下的目标物体或小区域，而不是 SeaThru-NeRF [28] 那样进行场景尺度的重建。


## III. 预备知识

### A. 3D 高斯点绘制

3D 高斯点绘制（3DGS）[11] 使用一组3D高斯分布对场景进行参数化，每个高斯分布具有均值$\mu$、由尺度$S$和旋转$R$导出的协方差$\Sigma$、不透明度$o$以及由球谐系数导出的颜色$c$。给定一个视点$T_{\text{cam}}^{\text{world}}$，我们可以通过将高斯分布从前到后排序，将它们投影到相机平面上，并对投影后的高斯分布进行alpha合成，从而高效地渲染出图像。对于一个像素$x$，其颜色可以计算为：

$$
C(x) = \sum_{i=1}^{N} c_i \alpha_i(x) \prod_{j=1}^{i-1} (1 - \alpha_j(x)) \quad (1)
$$

其中，$c_i$和$\alpha_i$分别是第$i$个3D高斯分布的颜色和密度。这种表示方法通过结合$L_1$重建损失和D-SSIM项进行优化：

$$
L_{\text{original}} = (1 - \lambda) L_1 + \lambda L_{\text{D-SSIM}} \quad (2)
$$

### B. 水下图像形成

在水下环境中，两个与波长和距离相关的物理过程会影响光的传播以及图像的捕获。首先，光在水中的传播会因波长而选择性地衰减，红色光在相同距离上的衰减远大于蓝色光 [30]。其次，水柱中的颗粒会将来自各种光源的光反射到相机中，导致反向散射，也称为薄雾光。这两种效应会随着相机与被成像物体之间的距离而变化：物体越远，其反射向相机的光就越衰减，且物体与相机之间的水柱中的颗粒越多，反向散射就越强。这些过程可以用以下公式建模 [23]：

$$
I = J \cdot e^{-\beta_D \cdot Z} + B_\infty \cdot (1 - e^{-\beta_B \cdot Z}) \quad (3)
$$

其中，$I$是捕获的图像，$J$是如果没有介质时会被捕获的底层颜色，$\beta_D$和$\beta_B$分别是衰减系数和反向散射系数，$B_\infty$是无穷远处的反向散射水色，$Z$是深度图像。尽管从水面的深度也会影响可见光的量，但这里的深度指的是从相机到物体的距离。衰减后的真彩色图像$J \cdot e^{-\beta_D \cdot Z}$也被称为直接图像$D$。

## IV. SeaSplat：水下3D高斯方法

### A. 我们的方法

我们提出了SeaSplat，这是一种基于3D高斯分布的方法，能够在合成新视点的同时，估计环境中底层的真实颜色，就像不存在介质（水）一样。给定一组图像$I \in \mathbb{R}^{H \times W \times 3}$、相机内参$K \in \mathbb{R}^{3 \times 3}$以及通过运动恢复结构（例如COLMAP [31]）得到的每一帧的世界到相机的外参$T_{\text{cam}}^{\text{world}} \in \text{SE}(3)$，我们的目标是学习一个底层、校正颜色的场景的3D高斯表示，同时学习介质效应（反向散射和衰减）的参数。该模型的概览如图2所示。通过建模校正颜色的场景，我们可以利用3D高斯表示的深度以及在公式(3)中描述的物理模型，估计深度和波长依赖的效应，并将这些效应与底层颜色相结合。直观上，用物理模型约束3D高斯表示，可以推动3D高斯分布在三维空间中更好地定位，并具有更符合物理规律的深度。具体来说，对于给定的相机视点，我们可以渲染底层真实颜色图像$\hat{J}$以及3D场景的估计深度图$\hat{Z}$。利用图像形成模型，我们可以计算每个通道的衰减图$\hat{A} = e^{-\beta_D \cdot \hat{Z}}$以及反向散射图像$\hat{B} = B_\infty \cdot (1 - e^{-\beta_B \cdot \hat{Z}})$。最终，我们重建的水下图像简单地表示为$\hat{I} = \hat{J} \cdot \hat{A} + \hat{B}$。

### B. 优化

尽管我们的3D高斯表示受到水下图像形成模型的约束，但仅使用[11]中的标准损失函数是不足以得到有意义的非平凡解的。例如，一个不添加反向散射且不进行衰减的模型仍然可以满足优化目标。同样，由于我们同时学习深度和这些介质参数，如果介质参数估计不佳，优化过程可能会导致深度变得不合理。因此，我们引入了额外的损失约束，并交替优化介质参数和底层3D高斯表示。

为了优化反向散射参数，我们采用了DeepSeeColor [32]开发的反向散射损失，这是用于去雾的暗通道先验损失的一个变体：

$$
L_{\text{bs}} = \sum_{(i, j)} \sum_{c} \left( \max\{\hat{D}_c(i, j), 0\} + k \min\{\hat{D}_c(i, j), 0\} \right), \quad (4)
$$

其中，超参数$k > 1$，直接图像$\hat{D}$为去除反向散射后的观测图像，即$\hat{D} := I - \hat{B}$，而$\hat{D}_c$为特定颜色通道$c \in \{r, g, b\}$中的直接图像。$\hat{B}$是利用3D高斯表示估计的深度$\hat{Z}$计算得到的，且$\hat{Z}$被分离以防止梯度回传。因此，梯度不会影响底层的3D高斯表示，但学到的反向散射参数会约束其他损失中使用的图像形成模型。

与其它水下色彩校正工作[32]、[33]类似，我们鼓励每个颜色通道的平均值接近颜色范围的中间值，即灰世界假设[34]，并惩罚颜色过饱和：

$$
L_{\text{gw}} = \frac{1}{3} \sum_{c} \left( \frac{1}{N} \sum_{i, j} \hat{J}_c(i, j) - 0.5 \right)^2, \quad (5)
$$

$$
L_{\text{sat}} = \sum_{i, j} \sum_{c} \max\{\hat{J}_c(i, j) - T_{\text{sat}}, 0\}, \quad (6)
$$

其中，$N$是$\hat{J}$中的像素数，$T_{\text{sat}}$是一个经验设置的阈值，为0.7。同样，为了强调恢复远处细节（反向散射和衰减可能对视觉外观影响最大），我们增加了一个深度加权的重建损失，且不允许梯度通过深度图像回传。这补充了3DGS [11]的原始重建损失：

$$
L_{Z\text{-recon}} = \| Z_{\text{detach}} \cdot (I - \hat{I}) \|_1, \quad (7)
$$

我们使用边缘感知的总变分损失[35]来正则化估计的深度图，推动深度图在观测相机图像的高梯度区域（例如边缘）变化更平滑：

$$
L_{Z\text{smooth}} = \sum_{i, j} \left( e^{-\vert \nabla_x I\vert } \vert \nabla_x \hat{Z}\vert  + e^{-\vert \nabla_y I\vert } \vert \nabla_y \hat{Z}\vert  \right), \quad (8)
$$

由于重建损失在引导3D高斯表示的优化中占主导地位，常常会在不一定代表水下物体的区域（例如水柱）添加高不透明度的高斯分布。为了抵消这一点，我们增加了一个损失，将与估计的$B_\infty$颜色在欧几里得距离上相似的像素推向低alpha掩码值：

$$
L_{\text{op}} = \sum_{i, j} \alpha(i, j) \cdot \mathbb{1}_{\vert \vert I(i, j) - B_\infty\vert \vert _2^2 < T_{\text{sim}}}, \quad (9)
$$

我们的总损失如下，其中$L_{\text{GS}}$对应于原始的3D高斯点绘制[11]损失：

$$
L = L_{\text{GS}} + L_{\text{bs}} + L_{\text{gw}} + L_{\text{sat}} + L_{\text{op}} + L_{Z\text{smooth}} + L_{Z\text{-recon}}, \quad (10)
$$

### C. 实现

我们在3D高斯点绘制[11]的代码基础上进行了修改，以实现可微分的深度图和alpha掩码渲染[36]、[37]。深度渲染是通过第二次渲染通道实现的，将传递到渲染函数中的颜色替换为每个3D高斯的$z$值。我们使用零阶球谐函数，使得每个高斯的颜色不依赖于视角。与[32]类似，衰减和反向散射系数$\beta_D$和$\beta_B$分别实现为一个$(1, 1, 1, 3)$的卷积核，可以通过卷积高效地应用于深度图像$D$。$B_\infty$被实现为一个可微分的参数。

## V. 实验

### A. 实验设置

**a) 数据集**  
我们使用了与SeaThru-NeRF [28]一同发布的多视角水下场景数据集，其中包含四个场景，分别来自红海（日本花园和IUI3）、加勒比海（库拉索）和太平洋（巴拿马），这些场景的相机姿态是通过COLMAP [31]提取的。这些场景涵盖了多种地理区域和水况。RAW图像使用带有圆顶端口的水下外壳中的DSLR相机拍摄，并进行了白平衡处理。此外，我们还使用了一个由遥控水下机器人CUREE [2]在美属处女岛的盐湾海底拍摄的水下场景，该场景包含一个放置在海底的颜色图表。机器人在低海拔和高海拔之间移动，以突出显示随着距离增加而加剧的图像退化。值得注意的是，与SeaThru数据集中包含水柱的前向图像不同，该数据集的图像是向下拍摄的海底图像。我们同样使用COLMAP [31]来提取相机姿态。此外，我们还基于Mip-NeRF 360 [16]的户外花园场景生成了一个合成的模拟数据集。我们使用3DGS [11]估计深度图，并模拟了一个雾蒙蒙和水下的场景，类似于[28]。根据公式(3)，我们添加了水，其中衰减系数$\beta_D = [2.6, 2.4, 1.8]$，反向散射系数$\beta_B = [1.9, 1.7, 1.4]$，无穷远处的反向散射颜色$B_\infty = [0.07, 0.2, 0.39]$。我们还添加了雾，其中$\beta_D = \beta_B = 2.4$，去除了波长依赖性以及反向散射和衰减项之间的差异 [21]、[22]。

**b) 对比方法**  
与我们的工作最接近的是SeaThru-NeRF [28]，该方法通过将水下图像形成模型 [24]整合到神经辐射场框架（特别是Mip-NeRF360 [16]）中，与我们的方法类似。尽管SeaThru-NeRF通过学习的MLP按视图方向估计水参数，而SeaSplat假设整个场景的水参数是一致的，不需要如此密集的采样。我们还将我们的方法与传统的3D高斯点绘制（3DGS）[11]进行了对比。对于这两种方法，我们都使用了作者公开发布的代码。

**c) 评估指标**  
由于我们无法获取水下场景的真实底层颜色（除非排干海洋），我们展示了带有介质的图像的新视点合成的定量结果。我们通过计算峰值信噪比（PSNR）、结构相似性指数（SSIM）[38]和感知距离（LPIPS）[39]，将保留的测试帧的渲染结果与真实帧进行比较，以评估视觉保真度。我们还评估了训练所学表示和渲染一帧所需的计算要求，包括时间和内存，所有这些都在一致的硬件集上进行。

### B. 结果

我们在表I中报告了对原始含介质图像进行新视点合成的定量结果。与SeaThru-NeRF [28]相比，我们在所有数据集和所有指标上均显示出更高的性能。与3D高斯点绘制（3DGS）[11]相比，我们在所有指标上也显示出更高或相似的性能。这表明我们在渲染新视点的质量上达到了与3DGS [11]相当或更好的水平，同时在估计水下场景底层颜色方面也优于其他类似方法。从定性角度来看，我们在图3中展示了含介质的新视点合成结果。在查看彩色图像时，所有方法的结果看起来相当。然而，在查看渲染的深度图像时，我们看到了更多的变化。值得注意的是，3DGS倾向于在低纹理区域放置3D高斯分布，特别是靠近相机的水柱区域。这种效果可以在库拉索的右上角、日本花园的左上角、巴拿马的右上角以及IUI3的顶部的深度图像渲染中看到。这些“漂浮物”区域会导致在光度上看似合理的渲染结果，但从更极端的视点来看可能会出现问题。与SeaThru-NeRF一样，SeaSplat能够在这些场景中推断出合理的深度估计，这得益于水下图像形成模型所施加的额外约束。与3DGS相比，SeaSplat的深度图更加平滑，而3DGS的深度图有许多尖锐的、嘈杂的伪影；与SeaThru-NeRF相比，SeaSplat在某些区域的小周期性伪影也较少。在盐池数据集中，相机轨迹仅在水柱内上下移动，且为向下拍摄的图像，SeaSplat与3DGS在深度估计上的差异最为明显。图4从极端视点展示了每种方法所学习的3D高斯表示（左侧为SeaSplat，右侧为3DGS）。虽然SeaSplat仅学习到海底的表示，但3DGS在水柱中放置了许多“漂浮物”，以产生原始输入图像中存在的介质退化图像质量。为了了解SeaSplat与SeaThru-NeRF在恢复场景真实颜色方面的能力，我们在图5中可视化了推断的恢复结果。在SeaThru-NeRF数据集中，两种方法的外观看起来都较为合理，尽管SeaSplat显示出更明亮、更生动的恢复效果。在查看被衰减和薄雾效应覆盖的区域时，我们发现SeaSplat能够恢复更多的细节和颜色。在对花园场景添加合成的雾和水时，SeaSplat能够减轻雾和水的影响，使背景中的植被细节和颜色更加清晰，而SeaThru-NeRF则难以恢复场景的合理估计。在盐池数据集上，这是一个向下拍摄的、有界的场景，我们强调了SeaSplat增强图像的能力，如图所示，即使在如此高的高度下，颜色图表也更加清晰。我们还在表II中展示了不同方法之间的计算需求比较，这些数据是在SeaThru-NeRF数据集上进行训练和评估时平均得出的。我们的方法几乎没有增加额外的推理时间和内存限制，与3D高斯点绘制相当，这表明我们额外的介质估计过程对性能的影响可以忽略不计。渲染时间的增加主要归因于获取深度所需的第二次渲染通道。这保留并强化了从NeRF到3D高斯的显著范式改进，正如Sea-Thru NeRF的高计算需求所见。此外，我们的方法不需要在每个像素处密集查询或采样介质参数，而是采用一组全局介质参数。

### C. 消融实验

最后，在表III中，我们还对设计决策进行了消融实验，特别是我们所增加的额外损失目标的作用。需要注意的是，这些消融实验量化了在含介质的新视点合成任务上的性能，而我们同样关注颜色恢复的任务。我们可以看到，背景损失是有帮助的，而许多其他损失组件单独来看并没有带来显著的性能提升。然而，当将它们组合在一起时，我们发现我们的总损失函数确实带来了性能的提升。

## VI. 结论

我们提出了SeaSplat，这是一种结合了3D辐射场的最新进展与基于物理的水下图像形成模型的方法，能够实现实时渲染水下场景。在一系列真实世界和模拟的数据集上，SeaSplat不仅能够在介质中生成高质量的新视点，还能恢复这些场景在没有介质时的真实底层颜色。与此同时，SeaSplat几乎没有增加额外的计算开销，保留了3D高斯点绘制的实时渲染能力，并且显著优于其他能够恢复退化介质影响下颜色的辐射场方法。

### 局限性与未来工作

在将该方法部署到任何水下机器人之前，需要将其适应于实时环境，随着新数据的观测逐步构建3D高斯表示。这为水下导航、碰撞避免以及对视觉上有趣的自然现象进行自适应采样开辟了潜在的应用前景。在完全建模水下光的效应方面，本工作没有考虑光与表面之间的相互作用（例如，像其他最近的工作[40]中所做的那样，考虑水面的透镜效应），也没有考虑由潜水员或水下机器人引起的场景中的阴影，如盐池数据集中所呈现的那样。最后，水下世界充满了动态的、移动的物体（例如，随水流摇曳的扇珊瑚），而我们的工作目前专注于静态的水下环境。




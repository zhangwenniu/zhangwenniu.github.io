---
layout: mypost
title: k063 Image-Based Relocalization and Alignment for Long-Term Monitoring of  Dynamic Underwater Environments
categories: [水下]
---

# 论文重点与难点

## 重点

`1.` **提出了一种用于长期监测动态水下环境的图像重定位与对齐方法**：

   - 结合视觉位置识别（VPR）、特征匹配和图像分割，实现了对水下生态系统的长期监测。

   - 该方法能够在多年的尺度上识别重访区域、估计刚性变换，并分析生态系统的变化。

`2.` **构建了SQUIDLE+ VPR基准测试**：

   - 这是首个大规模水下VPR基准测试，利用多个机器人平台收集的大量非结构化数据，涵盖从几天到几年的时间跨度。

   - 数据集包含多种轨迹、任意重叠和多样化的海底类型，且在不同的环境条件下（如深度、光照和浑浊度）拍摄。

`3.` **技术方法**：

   - **全局检索阶段**：使用VPR技术提取图像描述符，通过计算图像对之间的L2范数来评估相似性矩阵，并选择最相似的前K个匹配。

   - **局部细化阶段**：利用LightGlue建立关键点对应关系，并基于内点数量重新排序匹配结果。

   - **单应性估计与异常值过滤**：估计单应性矩阵，并计算重投影误差以过滤掉不符合几何一致性的匹配。

   - **2D分割掩码的变换**：使用Segment-Anything 2（SAM2）提取分割掩码，并通过单应性矩阵将掩码变换到公共图像空间，使用交并比（IoU）作为相似性代理。

`4.` **实验验证**：

   - 在多个水下数据集上验证了VPR方法的性能，包括Eiffel Tower、Okinawa、Tasman Fracture和St Helens数据集。

   - 通过Recall@K和Precision-Recall曲线评估了不同VPR方法的性能，并展示了所提出方法的有效性。

## 难点

`1.` **水下环境的复杂性**：

   - 水下图像受到光照、浑浊度、深度变化等因素的影响，导致图像之间存在显著的视觉差异。这使得传统的视觉定位方法难以直接应用于水下环境。

   - 例如，Okinawa数据集中的图像受到台风Trami的影响，环境破坏导致图像变化显著，增加了重定位的难度。

`2.` **大规模数据的处理与分析**：

   - SQUIDLE+基准测试包含大量的数据，涵盖了多种机器人平台和复杂的轨迹模式。如何高效地处理和分析这些数据是一个挑战。

   - 例如，全局检索阶段需要在大规模数据库中快速准确地找到与查询图像最相似的匹配项，而局部细化阶段需要处理大量的关键点对应关系，以提高匹配的准确性。

`3.` **精确的图像对齐与变化检测**：

   - 在图像对齐过程中，需要精确估计单应性矩阵，以确保分割掩码能够准确地变换到公共图像空间。任何微小的误差都可能导致分割掩码的不准确对齐，从而影响变化检测的性能。

   - 例如，在Tasman Fracture数据集中，由于视角变化较大，图像分割结果可能存在不一致性，这进一步降低了交并比（IoU）分数。

`4.` **计算效率与性能的平衡**：

   - 虽然暴力匹配方法（如SuperPoint暴力匹配）可以提供较高的检索性能，但其计算成本极高。如何在保持较高性能的同时显著降低计算成本是一个关键问题。

   - 论文提出的分层方法（结合MegaLoc和SuperPoint）在保持较高性能的同时，将计算速度提高了100倍，但如何进一步优化计算效率仍是一个需要解决的问题。

# 论文详细讲解

## 1 研究背景与动机

论文的核心目标是开发一种用于长期监测动态水下环境的自动化方法，以应对水下生态系统监测中的复杂挑战。水下生态系统（如珊瑚礁、海草床和巨藻林）对海洋生物多样性、渔业和海岸保护至关重要，但这些生态系统正面临气候变化、污染和人类活动（如过度捕捞和海岸扩张）的威胁。因此，长期监测这些生态系统的变化对于理解人类活动的影响、改善生态系统的长期健康和韧性至关重要。

然而，传统的水下监测方法（如潜水员监测）成本高昂且效率低下，而自主水下机器人（AUV）配备人工智能和计算机视觉技术，为高效、非侵入性的监测提供了一种可扩展的替代方案。然而，水下图像的复杂性（如有限的能见度、光学畸变、浑浊度、光的非线性衰减等）给传统的视觉定位方法带来了巨大挑战。

## 2 研究方法

论文提出了一种集成的图像重定位与对齐方法，结合了视觉位置识别（VPR）、特征匹配和图像分割技术，用于从视频图像中识别重访区域、估计刚性变换，并分析生态系统的变化。具体方法如下：

### 2.1 视觉位置识别（VPR）

VPR是该方法的核心部分，用于从数据库中检索与查询图像最相似的图像。论文使用了多种先进的VPR技术（如NetVLAD、CosPlace、MegaLoc等），通过计算图像对之间的L2范数来评估相似性矩阵，并选择最相似的前K个匹配：


$$
M_K = \{(Q_i, D_k) \mid D_k \in \text{TopK}(S(Q_i, D), K)\}
$$

其中，$$S(Q, D) \in \mathbb{R}^{q \times d}$$ 是查询图像集$$Q$$和数据库图像集$$D$$之间的相似性矩阵，$$\text{TopK}(\cdot, K)$$表示选择最相似的K个匹配。

### 2.2 特征匹配与局部细化

为了进一步提高匹配精度，论文使用了LightGlue来建立关键点对应关系。给定初始匹配$$M_K$$，使用SuperPoint特征提取关键点，并通过LightGlue计算关键点之间的对应关系：


$$
K_{qd} = \{(k_q, k_d) \mid k_q \in K_q, k_d \in K_d\}
$$

其中，$$K_q$$ 和 $$K_d$$ 分别是查询图像和数据库图像中的关键点。然后，根据内点数量重新排序匹配结果，选择最佳匹配：


$$
M_1 = \{(Q_i, D_1) \mid D_1 = \text{TopK}(\text{inliers}(Q_i, D_k), 1)\}
$$

### 2.3 单应性估计与异常值过滤

从最佳匹配的关键点对应关系中估计单应性矩阵$$H \in \mathbb{R}^{3 \times 3}$$，并计算重投影误差：


$$
e_r = \frac{1}{2} \left( \frac{1}{\vert K_{qd}\vert } \sum_{(k_q, k_p) \in K_{qd}} \|k_q - Hk_p\|^2 + \sum_{(k_q, k_p) \in K_{qd}} \|H^{-1}k_q - k_p\|^2 \right)
$$

其中，$$\vert K_{qd}\vert $$ 是匹配关键点的数量。为了过滤掉不符合几何一致性的匹配，论文丢弃了重投影误差大于10像素的图像对：


$$
M_\chi = \{(Q_i, D_1) \mid e_r(Q_i, D_1) \leq \chi\}
$$

### 2.4 分割掩码的变换与变化检测

为了模拟变化检测，论文使用Segment-Anything 2（SAM2）提取每个图像的分割掩码，并将分割掩码变换到公共图像空间。通过计算变换后的分割掩码之间的交并比（IoU），可以近似地检测生态系统的变化：


$$
\text{IoU} = \frac{\text{intersection}(Q, D)}{\text{union}(Q, D)}
$$

## 3 实验与结果

论文在多个水下数据集上进行了实验，包括Eiffel Tower、Okinawa、Tasman Fracture和St Helens数据集。这些数据集涵盖了不同的环境条件、轨迹模式和时间跨度。

### 3.1 VPR方法的性能评估

论文使用Recall@K和Precision-Recall曲线来评估不同VPR方法的性能。实验结果表明，MegaLoc在大多数数据集上表现最佳，且计算效率较高。例如，在Okinawa数据集上，MegaLoc的Recall@10达到了79.7%，而暴力匹配方法SuperPoint的Recall@10为71.3%。

### 3.2 分割掩码对齐与变化检测

通过变换分割掩码并计算IoU，论文展示了该方法在变化检测方面的潜力。例如，在St Helens数据集中，由于环境变化较小，IoU值较高（0.86），而在Okinawa数据集中，由于台风导致的显著变化，IoU值较低（0.65）。

## 4 结论与未来工作

论文提出了首个大规模水下VPR基准测试SQUIDLE+，并开发了一种结合全局描述符和局部特征细化的分层方法，用于长期水下生态系统的监测。该方法在多个数据集上表现出色，能够有效地检测生态系统的变化。未来的工作将集中在进一步优化计算效率，并开发更可靠的变化检测工具，以实现水下生态系统的自动化监测。

# 论文方法部分详细讲解

论文提出了一种用于长期监测动态水下环境的图像重定位与对齐方法，该方法结合了视觉位置识别（VPR）、特征匹配和图像分割技术。以下是方法部分的详细讲解：

## 1 技术流程概述

![fig.1](VPR.fig.1.png)

> 图1：我们的基于图像的水下生态系统监测重定位与对齐方法概述。我们使用分层视觉地点识别（VPR）技术，从自由导航机器人捕获的多年时间跨度的视频图像中稳健地识别共同位置。这些图像在光照、浊度和深度等环境条件上表现出显著变化。我们通过图像关键点之间的对应关系估计图像之间的刚性变换，然后将其用于在共同的像素空间中注册分割掩码。最后，我们应用交并比（IoU）指标来检测随时间变化的生态系统变化。

整体技术流程如图1所示，主要包括以下步骤：

`1.` **全局检索阶段**：使用视觉位置识别（VPR）技术，从数据库中检索与查询图像最相似的图像。

`2.` **局部细化阶段**：通过特征匹配技术，进一步优化全局检索的结果，提高匹配精度。

`3.` **单应性估计与异常值过滤**：估计图像之间的单应性矩阵，并过滤掉不符合几何一致性的匹配。

`4.` **分割掩码的变换与对齐**：使用分割掩码进行变换和对齐，以便进行像素级的变化检测。

## 2 全局检索阶段（Global Retrieval Stage）

- **目标**：从数据库中快速检索出与查询图像最相似的前$$K$$个图像。

- **方法**：使用多种先进的VPR技术（如NetVLAD、CosPlace、MegaLoc等），提取图像的全局描述符，并计算查询图像与数据库图像之间的相似性矩阵$$S(Q, D)$$。

- **相似性矩阵计算**：通过计算图像对之间的L2范数来评估相似性矩阵：

  
$$
S(Q, D) \in \mathbb{R}^{q \times d}
$$

  其中，$$q$$和$$d$$分别是查询图像集和数据库图像集的大小。

- **选择最相似的匹配**：对于每个查询图像$$Q_i$$，选择相似性分数最高的前$$K$$个数据库图像$$D_k$$作为初始匹配：

  
$$
M_K = \{(Q_i, D_k) \mid D_k \in \text{TopK}(S(Q_i, D), K)\}
$$

  其中，$$\text{TopK}(\cdot, K)$$表示选择最相似的$$K$$个匹配。

## 3 局部细化阶段（Local Refinement Stage）

- **目标**：通过特征匹配进一步优化全局检索的结果，提高匹配精度。

- **方法**：使用LightGlue建立关键点对应关系。首先提取SuperPoint特征，然后通过LightGlue计算关键点之间的匹配。

- **关键点对应关系**：对于每个初始匹配$$(Q_i, D_k)$$，提取关键点$$K_q$$和$$K_d$$，并建立关键点对应关系：

  
$$
K_{qd} = \{(k_q, k_d) \mid k_q \in K_q, k_d \in K_d\}
$$

  其中，$$k_q$$和$$k_d$$分别表示查询图像和数据库图像中的关键点。

- **重新排序匹配**：根据内点数量重新排序匹配结果，选择最佳匹配：

  
$$
M_1 = \{(Q_i, D_1) \mid D_1 = \text{TopK}(\text{inliers}(Q_i, D_k), 1)\}
$$

## 4 单应性估计与异常值过滤（Homography Estimation and Outlier Filtering）

- **目标**：估计图像之间的单应性矩阵，并过滤掉不符合几何一致性的匹配。

- **方法**：从最佳匹配的关键点对应关系中估计单应性矩阵$$H \in \mathbb{R}^{3 \times 3}$$，并计算重投影误差。

- **重投影误差计算**：计算关键点在对方图像中的投影误差，采用双向误差平均以考虑尺度变化：

  
$$
e_r = \frac{1}{2} \left( \frac{1}{\vert K_{qd}\vert } \sum_{(k_q, k_p) \in K_{qd}} \|k_q - Hk_p\|^2 + \sum_{(k_q, k_p) \in K_{qd}} \|H^{-1}k_q - k_p\|^2 \right)
$$

  其中，$$\vert K_{qd}\vert $$是匹配关键点的数量。

- **过滤异常值**：丢弃重投影误差大于10像素的图像对，以确保保留的匹配具有几何一致性：

  
$$
M_\chi = \{(Q_i, D_1) \mid e_r(Q_i, D_1) \leq \chi\}
$$

## 5 分割掩码的变换与对齐（2D Warping of Segmentation Masks）

- **目标**：通过变换分割掩码进行像素级的变化检测。

- **方法**：使用Segment-Anything 2（SAM2）提取每个图像的分割掩码，并将分割掩码变换到公共图像空间。

- **分割掩码提取**：使用SAM2为每个图像生成分割掩码，并将所有分割实例合并为一个统一的掩码。

- **掩码变换**：使用估计的单应性矩阵$$H$$将查询图像的分割掩码变换到数据库图像的分割掩码空间。

- **变化检测**：通过计算变换后的分割掩码之间的交并比（IoU）来检测变化：

  
$$
\text{IoU} = \frac{\text{intersection}(Q, D)}{\text{union}(Q, D)}
$$

  其中，$$\text{intersection}(Q, D)$$表示查询掩码和数据库掩码之间的共享像素数，$$\text{union}(Q, D)$$表示两个掩码覆盖的总像素数。

通过上述方法，论文提出了一种鲁棒的图像重定位与对齐方法，能够有效应对水下环境的复杂性和长期变化检测的需求。

# 原文翻译

基于图像的长期动态水下环境重定位与对齐  

**作者**: Beverley Gorry, Tobias Fischer, Michael Milford, Alejandro Fontan  

**摘要**: 有效监测水下生态系统对于追踪环境变化、指导保护工作和确保生态系统长期健康至关重要。然而，由于水下图像的复杂性，传统视觉定位方法面临显著挑战，使得利用机器人平台自动化管理水下生态系统仍然困难。我们提出了一种集成管道，结合了视觉地点识别（VPR）、特征匹配和视频衍生图像的图像分割。该方法能够稳健地识别重访区域、估计刚性变换，并进行生态系统变化的下游分析。此外，我们引入了SQUIDLE+ VPR基准——这是首个大规模水下VPR基准，旨在利用来自多个机器人平台的大量非结构化数据，涵盖从几天到数年的时间间隔。该数据集包括多样化的轨迹、任意重叠和不同海底类型，并在不同的环境条件下捕获，包括深度、光照和浊度的差异。我们的代码可在以下网址获取：[https://github.com/bev-gorry/underloc](https://github.com/bev-gorry/underloc)。

## I. 引言  

![fig.1](VPR.fig.1.png)

> 图1：我们的基于图像的水下生态系统监测重定位与对齐方法概述。我们使用分层视觉地点识别（VPR）技术，从自由导航机器人捕获的多年时间跨度的视频图像中稳健地识别共同位置。这些图像在光照、浊度和深度等环境条件上表现出显著变化。我们通过图像关键点之间的对应关系估计图像之间的刚性变换，然后将其用于在共同的像素空间中注册分割掩码。最后，我们应用交并比（IoU）指标来检测随时间变化的生态系统变化。

水下生态系统如珊瑚礁、海草床和海藻森林支撑着海洋生物多样性、渔业和海岸保护。这些水下栖息地至关重要，但面临气候变化、污染和人类活动（如过度捕捞和海岸扩张）的日益增长威胁[1][2]。长期监测这些栖息地对于检测和量化不同物种分布和丰度的变化至关重要，帮助研究人员更好地理解人类活动的影响，并改善这些关键生态系统的长期健康和恢复力[3]。由于潜水员监测的成本和局限性，自主水下车辆提供了一种可扩展的替代方案[4]。配备人工智能和计算机视觉的它们能够实现高效、非侵入性的监测，并在导航[5]、3D地图构建[6][7]和图像分割[8]方面取得进展，以进行高分辨率数据收集。可靠的多年度变化检测使得对这些动态和脆弱生态系统的长期监测成为可能，需要高度精确的配准以捕捉珊瑚礁等栖息地的厘米级变化[9]。在陆地和水上监测场景中，变化检测受益于GPS位置先验、明确的地标以及数十年来针对这些场景的机器人视觉技术。然而，这些方法不能直接应用于水下调查，后者必须应对独特的挑战，如有限的能见度、光学畸变、浊度、光斑和深度的非线性光衰减。Eustice等人在大规模船体映射方面的早期工作为水下同步定位与地图构建（SLAM）技术奠定了基础，使用基于视觉和声纳的方法进行基础设施检查[10][11]。这些努力针对船体检查[12]、石油钻井平台监测和管道调查[13]等应用，虽然与生态系统监测不同，但分享了长期特征跟踪[14]、传感器退化和环境可变性[9]方面的挑战。在本文中，我们专注于由视觉地点识别（VPR）和语义分割的最新进展支持的长期环境监测。VPR使自主系统能够基于一组参考图像识别先前访问过的位置。尽管VPR具有精确、可重复定位的潜力，但我们表明，当前最先进的技术在具有挑战性的新数据集（如水下领域）中面临挑战，而该领域迄今受到的关注非常有限。我们的方法仅需要一组预先收集的图像作为参考——不需要SLAM、结构从运动（SfM）或几何地图构建。这使其更简单且更具可扩展性，适用于长期监测。如图1所示，我们的系统通过使用来自自主海洋机器人的单目视频在多年时间尺度上注册位置，实现了成本效益高且可扩展的水下生态系统监测。它识别重访区域、估计刚性变换并分析视觉变化，即使在轨迹重叠变化的情况下也是如此。通过将VPR与特征匹配和图像扭曲相结合，我们的方法增强了空间一致性和随时间变化的配准精度。虽然我们的方法建立在现有技术之上，但我们是第一个以这种方式结合它们的人，实现了对生态学家有价值的关键监测能力。最后，我们引入了SQUIDLE+ VPR基准，这是首个用于评估水下VPR的大规模基准，利用来自SQUIDLE+的公开数据。通过广泛的实验，我们为水下环境的最先进VPR技术进行了基准测试，推进了长期生态监测的精确配准，并提高了自主海洋调查的可靠性、可扩展性和有效性，以支持科学研究和海洋保护。

## II. 相关工作  

我们回顾了图像配准、3D地图构建和视觉位置识别（VPR）作为长期水下生态系统监测的关键组成部分。传统的图像配准（第II-A节）依赖于GPS或多传感器SLAM进行粗略定位，然后再进行对齐细化，而我们的方法使用VPR从单目视频中高效检索图像以实现精确配准。在3D地图构建中，现有方法通常在单一轨迹内传播语义信息，而我们将分割扩展到多年数据集中的不同条件下（第II-B节）。水下VPR仍然未被充分探索，大多数技术是为结构化陆地环境设计的（第II-C节）。最后，我们在第II-D节中回顾了水下定位基准测试。

### A. 用于变化检测的图像配准  

Delaunoy等人[9]使用SURF特征对相隔10个月拍摄的小珊瑚礁场景图像进行配准和变化检测。类似地，Williams等人[15]应用了基于立体视觉的SLAM技术，使用SIFT特征对自主水下航行器（AUV）在网格调查期间收集的多个3D图像地图进行配准。他们的方法在12小时内比较了同一区域，估计了AUV在重叠网格调查中的轨迹。Bryson等人[16]推进了多年重复调查图像处理和精确配准，用于监测海底海洋栖息地的长期变化。通过SLAM和3D重建对立体图像和导航数据进行后处理，生成了海底的纹理模型，并将其正交投影到地理参考的镶嵌图中。他们的方法包括互信息优化，以提高对多年间颜色和亮度变化的鲁棒性。  

这些研究依赖于GPS或多传感器SLAM进行粗略定位，然后再应用图像配准方法。相比之下，我们的方法从未校准的单目相机视频中高效检索同一位置的图像线索。这使得我们能够实现监测应用所需的精确图像配准。

### B. 珊瑚礁的3D地图构建  

最近的水下3D地图构建进展改善了珊瑚礁重建和语义解释。Sauder等人[17]提出了一种管道，将语义分割从图像传播到通过SfM从单轨迹视频流估计的3D点云。Sethuraman等人[18]利用神经辐射场（NeRFs）进行物理信息的新视图合成和图像恢复，解决了水柱效应如衰减和后向散射。Wang等人[7]将视觉惯性里程计与实时3D重建相结合，在资源受限的AUV上生成密集地图，取得了与离线方法相当的结果。Song等人[19]引入了TURTLMap，专为低纹理水下区域的实时定位和地图构建而设计。与这些通常专注于单次地图构建、需要受控相机路径或解决特定可见性挑战的方法不同，我们的方法能够在不同年份、不同条件下捕获的图像中传播语义分割，且具有任意轨迹重叠。我们的管道优先考虑图像到图像的配准，以便仅使用单目视频输入进行直接生态比较，使其更适用于实际的长期监测应用。

### C. 视觉位置识别  

尽管具有潜力，水下VPR仍然未被充分探索，并且很少在实际环境中应用。大多数VPR管道[20]–[25]是在结构化环境中开发和测试的，很少应用于非结构化环境——几乎没有任何针对水下图像的。对陆地位置识别技术的全面回顾超出了本文的范围，但感兴趣的读者可以参考一些优秀的综述[26]–[29]。本文感兴趣的是AnyLoc[30]，这是少数在埃菲尔铁塔水下数据集[31]上评估的VPR技术之一，以及MegaLoc[32]，这是一种多功能图像检索模型，通过利用多样化的数据和训练技术，在VPR、视觉定位和地标检索中实现了最先进的性能。位置识别通常用于分层管道中，其中前K个检索到的数据库图像使用计算更密集的局部特征匹配进行重新排序。一个广泛使用的例子是hloc[33]，它使用NetVLAD[20]进行图像检索，并使用在户外场景上预训练的SuperPoint[34]和SuperGlue[35]进行局部匹配。有几项工作专门针对水下环境中的位置识别。Maldonado-Ramirez等人[36]提出了一种无监督的VPR方法，使用卷积自编码器从视觉注意力算法检测到的显著地标中学习紧凑表示。Burguera等人[37]引入了一种深度网络，用于快速、稳健的水下环路检测，使用聚类SIFT特征和无监督训练。虽然这些方法专注于水下VPR并评估自记录的机器人序列，但我们的方法在涵盖多样化序列、机器人平台和相机设置的大型公开基准上进行了测试，更接近一般VPR评估。

### D. 水下定位基准测试  

Boittiaux等人[31]引入了埃菲尔铁塔数据集，这是一个用于长期视觉定位的深海数据集。Ferrera等人[38]提出了AQUALOC，以支持水下航行器的视觉-惯性-压力SLAM，提供了来自SfM的离线轨迹，用于与实时定位方法进行比较。Joshi等人[39]提出了一种视觉-惯性SLAM管道，并在南卡罗来纳州的人工沉船和佛罗里达州的洞穴中进行了评估。Singh等人[40]引入了一个用于基准测试折射相机模型估计的数据集，其特点是在受控的池环境中使用时间同步的5相机/IMU设置在远程操作车辆（ROV）上。Angelakis等人[41]利用澳大利亚海狮的动物携带视频和运动数据来绘制海底栖息地地图，解决了基于船只调查的挑战，这些调查成本高、耗时、依赖天气，并且在深海环境中不切实际。Joshi等人[42]提出了一种用于浅水区的3D水质测绘系统，使用带有GPS和水质传感器的BlueROV2，通过在发生错误时重新浮出水面进行位置校正。在本文中，我们朝着水下场景中VPR方法的标准化基准测试迈出了一步，类似于陆地VPR中的努力[43]，[44]。具体来说，我们建议利用https://squidle.org/上免费提供的序列。SQUIDLE+托管了最大的公开可访问的地理参考海洋图像库，使得能够构建针对特定科学问题的数据集，并从各种平台、活动和全球分布的部署中获取数据。通过利用这一庞大的数据源，我们的工作为专门的水下VPR基准测试奠定了基础，促进了VPR管道的开发和严格评估。

## 三、技术方法

本节详细介绍了我们使用未校准的单目相机视频流在水下场景中进行变化检测的方法。

给定视频流中的两个连续图像集$$Q$$和$$D$$，我们首先使用通过最先进的视觉地点识别（VPR）技术[20]、[22]、[23]、[25]、[30]、[32]获得的全局描述符，为每个$$Q_i$$在$$D$$中找到前$$K$$个匹配项。使用LightGlue，我们通过内点数量对这些匹配项重新排序，并过滤掉重投影误差较大的图像对，以避免错误匹配。然后，我们从匹配的图像中提取语义分割掩码，并使用单应矩阵对图像进行变形。我们使用交并比（IoU）度量来近似位置相似度。

### A. 视觉地点识别 - 全局检索阶段

我们使用标准的VPR技术提取图像描述符，并通过计算所有图像对之间的L2范数来计算相似度矩阵：$$S(Q, D) \in \mathbb{R}^{q \times d}$$。

对于每个查询图像$$Q_i$$，我们选择相似度得分最高的前$$K$$个数据库图像$$D_k$$，形成初始匹配集：


$$
\mathcal{M}_K = \{(Q_i, D_k) \vert  D_k \in \text{TopK}(S(Q_i, D), K)\}, \quad (1)
$$

其中$$\text{TopK}(\cdot, K)$$表示选择$$K$$个最相似匹配项的函数。

### B. 关键点对应关系 - 局部细化阶段

我们使用LightGlue在$$\mathcal{M}_K$$中的SuperPoint特征之间的图像空间中建立关键点对应关系$$\mathcal{K}_{qd}$$来细化匹配：


$$
\mathcal{K}_q = \{ \boldsymbol{k}_q \vert  \boldsymbol{k}_q \in \mathbb{R}^3 \}, \quad \mathcal{K}_d = \{ \boldsymbol{k}_d \vert  \boldsymbol{k}_d \in \mathbb{R}^3 \}, \quad (2)
$$

其中$$\boldsymbol{k}_q$$和$$\boldsymbol{k}_d$$表示用齐次坐标表示的关键点。

然后，我们基于内点数量将其作为新的相似度度量对匹配项重新排序，为每个查询选择最佳匹配：


$$
\mathcal{M}_1 = \{ (Q_i, D_1) \vert  D_1 = \text{TopK}(\text{inliers}(Q_i, D_k), 1) \}. \quad (3)
$$ 

### C. 单应性估计和异常值过滤

从最佳匹配子集$$\mathcal{M}_1$$中的图像对应关系$$\mathcal{K}_{qd}$$，我们估计单应矩阵$$H \in \mathbb{R}^{3 \times 3}$$，并将一个图像中的关键点投影到另一个图像中的对应位置。重投影误差$$e_r$$计算为投影到相反图像上的关键点之间的均方根误差（RMSE）距离的平均值：


$$
\begin{align*}
e_r = \frac{1}{2\sqrt{\vert \mathcal{K}_{qd}\vert }} & \left(\sqrt{\sum_{(\boldsymbol{k}_q, \boldsymbol{k}_p)\in\mathcal{K}_{qd}} \|\boldsymbol{k}_q - H\boldsymbol{k}_p\|^2} \right.\\
& \left. + \sqrt{\sum_{(\boldsymbol{k}_q, \boldsymbol{k}_p)\in\mathcal{K}_{qd}} \|H^{-1}\boldsymbol{k}_q - \boldsymbol{k}_p\|^2} \right), \quad (4)
\end{align*}
$$

其中$$\vert \mathcal{K}_{qd}\vert $$表示匹配关键点的数量。为了考虑尺度变化（例如，当查询图像是数据库图像的放大版本，反之亦然），我们通过计算两个方向的变换来平均双向误差。

最后，我们丢弃重投影误差大于$$\chi = 10$$像素的图像对，确保仅保留几何上一致的匹配：


$$
\mathcal{M}_{\chi} = \{ (Q_i, D_1), \ e_r(Q_i, D_1) \leq \chi \}. \quad (5)
$$

### D. 分割掩码的二维变形

为了模拟一种用于对配准图像对$$\mathcal{M}_{\chi}$$进行视觉监测的潜在变化检测方法，我们使用Segment-Anything 2（SAM2）[46]自动为每个图像提取分割掩码。为了在我们的实验中简化操作，图像内的所有分割实例被合并为一个统一的掩码。然后，我们使用单应矩阵将掩码变形到一个公共图像空间中，从而能够使用像素交并比（IoU）作为相似度代理进行像素级比较。 

## IV. 实验设置  

### A. VPR 基线  

对于全局检索，我们评估了在[47]中实现的最先进的视觉地点识别（VPR）方法，包括MixVPR[25]、CosPlace[22]、NetVLAD[20]、AnyLoc[30]、MegaLoc[32]和CricaVPR[23]。虽然我们的管道支持[47]中引用的所有模型，但由于它们在陆地VPR中的出色表现和广泛应用，我们选择了这个子集。对于局部特征细化，我们使用SuperPoint[34]关键点与LightGlue[45]结合。为了评估基于GPS衍生地面真值的检索结果是否对应于有意义的地点识别，我们将评估的VPR方法与随机猜测器进行比较，后者作为性能下限。随机猜测器实现为蒙特卡罗实验：对于每个查询图像$$Q_i$$，我们随机选择$$K$$个数据库匹配，每次查询重复此选择$$n$$次。然后，Recall@K指标定义为在$$K$$个选定的数据库图像中至少检索到一个正确匹配的迭代比例，归一化为总试验次数$$\vert Q\vert  \cdot n$$，其中$$\vert Q\vert $$是查询图像的数量，$$n$$选择为足够大。为了建立性能上限，我们使用暴力基线，该基线使用LightGlue对每个查询-数据库图像对进行穷举局部特征匹配。在这种情况下，关键点匹配的数量作为图像对之间的相似性度量。  

### B. SQUIDLE+ VPR 基准  

![table.1](VPR.table.1.png)

> 表1：SQUIDLE+ VPR基准源自各种机器人平台，遵循具有任意重叠的轨迹，时间跨度从几天到几年不等。数据包括在不同场景条件下（例如深度、光照、浊度）捕获的多样化海底类型。

![fig.2](VPR.fig.2.png)

> 图2：水下数据集的GPS轨迹（以米为单位）和环境差异。可视化轨迹展示了四个水下数据集的查询序列（橙色）和数据库序列（蓝色）：(a) 来自大西洋中脊热液喷口的埃菲尔塔（2018–2020），(b) 冲绳（2016–2017–2018），捕捉台风“潭美”前后的中光珊瑚礁环境，(c) 塔斯曼断裂（2018），展示深海底栖栖息地，以及(d) 圣海伦斯（2011–2013），记录了浅滩荒芜区域过渡运动期间的图像。为了分析覆盖相同区域但遵循不同轨迹的序列之间的独特重叠，我们在冲绳数据集中包含了第三条轨迹。每个轨迹的RGB图像对展示了相应的位置，突出了对自动化VPR系统具有挑战性的显著外观变化。值得注意的是，即使正确匹配的位置也表现出由于视角、光照条件和实际生态系统随时间变化而导致的显著视觉差异，进一步强调了水下环境中长期视觉地点识别的复杂性。

在这项工作中，我们建议利用SQUIDLE+的数据来推进水下VPR，通过利用大量非结构化数据。该数据集源自执行多样化轨迹模式的各种机器人平台，具有任意重叠，时间跨度从几天到几年不等。数据包括在不同场景分布外条件下捕获的多样化海底类型（例如深度、光照、浊度）。表1提供了选定序列的详细信息，而图2展示了机器人轨迹并呈现了描述场景变化的RGB图像样本。我们通过选择仅使用向下视角相机捕获的序列来处理数据，这是探索海底环境最常见的设置。这使得我们的基准与大多数使用前向相机捕获的结构化环境数据集不同，后者是大多数VPR技术的训练对象[30]，使其更类似于航空数据集[48]。为了确保跨数据集的处理时间一致，我们将RGB图像下采样到大约640×480像素的分辨率，调整以保持原始宽高比。我们包括具有GPS信号的AUV序列，以使用图像独立的地面真值评估我们的检索管道，用于视觉定位评估[49]。我们的数据集包括以下序列：  

**Okinawa1**：该数据集捕捉了冲绳濑底岛附近的中光珊瑚海底，使用AUV Tuna-Sand在两年期间记录[50]。2018年的序列是在台风“潭美”（Paeng）影响后几天记录的。环境损坏导致的视觉变化阻碍了重定位，强调了需要强大的管道来记录损坏和恢复。  

**Tasman Fracture2**：该数据集由底栖立体静态图像组成，用于绘制塔斯马尼亚附近海山上的表底动物和栖息地分布图[51]。图像在885米深度使用深拖相机在两天内捕获。  

**St Helens3**：该数据集包括使用AUV Sirius进行的底栖立体成像调查，用于监测塔斯马尼亚东北部圣海伦斯长期海胆种群动态[52]。作为每两年一次的监测计划的一部分，AUV调查了浅滩荒芜/海藻过渡带，这些数据包括在我们的基准中。  

我们还将**Eiffel Tower4**[31]数据集与SQUIDLE+序列一起包括在内，以提供一个专门用于基准长期水下视觉定位的精选参考。该数据集包括在五年期间对同一热液喷口建筑进行四次访问时捕获的图像。使用导航数据和SfM估计相机姿态和公共场景几何。与[30]相比，其中仅使用了一小部分图像（约1%），我们包含了整个序列以进行更全面的评估。对于所有序列，我们获得了地面真值矩阵，其中每对图像被认为描绘了同一地点，如果它们的空间距离小于预定义的定位半径（见表I）。

### C. 评估指标  

我们使用公认的指标评估VPR方法，因为这些指标在该领域广泛认可，并为评估性能提供了清晰的框架。这种方法使我们能够在研究社区的标准背景下理解这些方法的行为，同时突出它们在具有挑战性的水下数据集上的有效性。  

**Recall@K**：对于给定查询，按相似性排序的前$$K$$个匹配中至少有一个正确匹配（即真阳性）的概率。  

**精确率-召回率曲线**：精确率-召回率曲线评估VPR性能，仅考虑每个查询的最高排名匹配。该分析突出了精确率和召回率之间的权衡，提供了关于最高排名匹配在正确识别重访位置方面的可靠性的见解。

## V. 结果与讨论  

### A. 分层图像检索  

![fig.3](VPR.fig.3.png)

> 图3：VPR方法在水下数据集上的Recall@K性能。实验结果展示了正确匹配出现在前$$K$$个检索候选中的概率。我们比较了六种VPR方法——MixVPR、CosPlace、NetVLAD、AnyLoc、MegaLoc和CricaVPR——与随机猜测器，以评估基于GPS地面真值的检索结果是否对应于有意义的位置识别。此外，我们还包括了暴力SuperPoint方法（对所有可能的图像对进行特征匹配）以建立性能上限。†在冲绳数据集中，由于割草机轨迹模式（见图2），随机猜测器的性能在$$K \gtrsim 10$$时接近VPR方法，这增加了图像密度。这使得随机选择偶尔能够检索到正确匹配，即使没有视觉对应关系。虽然这不影响基于GPS的地面真值，但它强调了VPR方法在较低$$K$$值时最具区分性（见第V-A节）。‡由于其显著更高的计算成本，我们在圣海伦斯数据集中未包括SuperPoint暴力方法。

![fig.2](VPR.fig.2.png)

> 图2：水下数据集的GPS轨迹（以米为单位）和环境差异。可视化轨迹展示了四个水下数据集的查询序列（橙色）和数据库序列（蓝色）：(a) 来自大西洋中脊热液喷口的埃菲尔塔（2018–2020），(b) 冲绳（2016–2017–2018），捕捉台风“潭美”前后的中光珊瑚礁环境，(c) 塔斯曼断裂（2018），展示深海底栖栖息地，以及(d) 圣海伦斯（2011–2013），记录了浅滩荒芜区域过渡运动期间的图像。为了分析覆盖相同区域但遵循不同轨迹的序列之间的独特重叠，我们在冲绳数据集中包含了第三条轨迹。每个轨迹的RGB图像对展示了相应的位置，突出了对自动化VPR系统具有挑战性的显著外观变化。值得注意的是，即使正确匹配的位置也表现出由于视角、光照条件和实际生态系统随时间变化而导致的显著视觉差异，进一步强调了水下环境中长期视觉地点识别的复杂性。

![fig.4](VPR.fig.4.png)

> 图4：上图：最佳单匹配VPR的精确率-召回率曲线。SuperPoint-H方法优于单阶段VPR方法，在显著降低计算成本的同时实现了接近SuperPoint暴力方法的性能（见表III）。下图：背景以黑白表示二进制地面真值，绿色十字表示使用SuperPoint-H方法每个查询的最佳匹配的真阳性，红色十字表示假阳性。阳性匹配被过滤以排除重投影误差大于10像素的匹配，对应于埃菲尔塔数据集的精确率为39%，冲绳为99%，塔斯曼断裂为22%，圣海伦斯为72%。

![table.2](VPR.table.2.png)

> 表2：VPR方法在水下数据集上的Recall@K性能。我们使用绿色突出显示每个数据集的最佳R@10结果，橙色显示不包括SuperPoint暴力方法的第二佳R@10结果。AnyLoc、MegaLoc和CricaVPR表现相当。我们选择MegaLoc进行后续实验，因为它比AnyLoc显著更快，并且在平均性能上略优于CricaVPR。†我们使用R@10比较结果，因为这是基于GPS地面真值提供有意义比较的最大值（见图3中的解释）。

全局检索阶段。图3展示了八种基线的Recall@K性能，包括随机猜测器和暴力SuperPoint方法（见第IV-A节）。在冲绳数据集中，一个显著的观察结果是，当$$K \gtrsim 10$$时，随机猜测器的性能开始接近专门的VPR方法。这种现象源于结构化的割草机轨迹模式（见图2），这种模式创造了图像的高空间密度。在这种密集模式中，即使随机选择也偶尔能检索到我们定义的定位半径内的图像，尽管缺乏VPR方法识别的视觉对应关系。这并不表明我们的地面真值存在问题，而是表明对于高度结构化的调查模式，VPR方法的判别优势在较低的$$K$$值时最为明显。图4中的精确率-召回率曲线进一步说明了这一区别，表明在评估最高排名的匹配时，VPR方法实现了比随机选择高得多的精确率。表2突出了每个数据集的最佳Recall@10结果和第二佳结果，提供了跨方法的比较评估。MegaLoc比AnyLoc快得多，并且在平均表现上略优于CricaVPR。我们使用R@10进行评估，因为更高的值会由于基于GPS的地面真值的限制而引入歧义，如上所述。暴力SuperPoint在几乎所有数据集中建立了检索性能的上限，但带来了显著更高的计算成本。鉴于这种开销，暴力SuperPoint被排除在圣海伦斯数据集的实验之外。  

![fig.4](VPR.fig.4.png)

> 图4：上图：最佳单匹配VPR的精确率-召回率曲线。SuperPoint-H方法优于单阶段VPR方法，在显著降低计算成本的同时实现了接近SuperPoint暴力方法的性能（见表III）。下图：背景以黑白表示二进制地面真值，绿色十字表示使用SuperPoint-H方法每个查询的最佳匹配的真阳性，红色十字表示假阳性。阳性匹配被过滤以排除重投影误差大于10像素的匹配，对应于埃菲尔塔数据集的精确率为39%，冲绳为99%，塔斯曼断裂为22%，圣海伦斯为72%。

![table.3](VPR.table.3.png)

> 表3：每个VPR方法的每查询平均计算时间（毫秒）。所有单阶段VPR技术都比暴力SuperPoint方法显著更快。我们的分层方法（结合MegaLoc和SuperPoint）比暴力SuperPoint方法快×100倍。

局部优化阶段。基于先前的实验，我们选择MegaLoc作为分层方法中的全局图像检索方法。图4展示了八种基线中最佳单匹配结果的精确率-召回率曲线。我们的分层方法将MegaLoc与SuperPoint结合，实现了与暴力SuperPoint相当的性能（我们的分层方法的平均精确率为16%，而暴力方法为18%），同时显著减少了计算开销（快100倍）。一个关键优势是它能够在基于特征的优化之前首先缩小候选匹配范围，从而节省大量计算资源（见表3）。图4的底部行展示了地面真值评估，其中正确识别的匹配用绿色标记，误报用红色标记。为了确保可靠的评估，我们设置了重投影误差小于10像素的阈值，以认为匹配有效。这种过滤导致埃菲尔铁塔数据集的精确率为39%，冲绳为99%，塔斯曼断裂为22%，圣海伦斯为72%。为了量化计算效率，表3展示了每种VPR方法的每次查询的平均计算时间。通过以分层方式将MegaLoc与SuperPoint结合，我们的方法比暴力SuperPoint方法实现了100倍的加速，展示了准确性和效率之间的有利权衡。所有实验均在配备NVIDIA RTX 4090 GPU的Dell Precision 3680 Tower上进行。

### B. 分割变形与定性结果  

![fig.5](VPR.fig.5.png)

> 图5：分割掩码扭曲和重投影误差的定性结果，以及对齐掩码的交并比（IoU）分数。最左列：每个SQUIDLE+数据集的查询RGB图像，叠加了SAM2分割掩码。第二列：使用我们的MegaLoc + SuperPoint分层方法获得的数据库匹配，叠加了SAM2分割掩码。第三列：使用从LightGlue关键点估计的单应性将查询图像扭曲到选定的数据库图像上。数据库图像的实际关键点以蓝色圆圈表示，查询图像的投影关键点以橙色十字表示。重投影距离用黄色虚线标记。最右列：使用估计的单应性扭曲的查询掩码，并叠加到数据库掩码上。交集区域以绿色表示，而非重叠的查询和数据库掩码分别保持橙色和蓝色。

图5展示了使用从LightGlue关键点对应关系估计的单应性变换对SAM2生成的分割掩码进行变形后的结果。我们使用交并比（IoU）指标评估两个变形掩码之间的对齐情况，其中交集定义为查询掩码和数据库掩码之间共享的像素数，并集表示变形图像中两个掩码覆盖的总像素数。这种简单的评估提供了定性比较，展示了如何将用于水下生态系统监测的变化检测方法与我们的基于图像的配准方法无缝集成。然而，某些限制影响了我们变化检测的准确性：（1）分割结果的不一致性，（2）无法区分变形不准确性、外观变化和场景中的实际结构修改。定性观察表明，变化最小的场景（如圣海伦斯的岩石环境）获得了较高的IoU值。相比之下，台风Trami对冲绳序列造成的显著外观变化导致较低的IoU值，捕捉了环境干扰的影响。最后，塔斯曼断裂序列展示了我们的方法在强视角变化下配准图像的能力。然而，这些变化也引入了图像分割的不一致性，导致IoU分数进一步降低。这凸显了将我们的方法与更简化的比较方法结合以在极端视角变化下实现变化检测的潜力。  

## VI. 结论与未来工作  

我们提出了第一个全面的水下环境VPR基准测试，解决了长期海洋监测中的一个关键空白。我们的分层方法将全局描述符与局部特征优化相结合，在从几天到几年的不同时间跨度内，在多样化的水下场景中实现了稳健的性能。此外，精确的图像配准使得能够在时间上分离的观测之间进行像素级比较——这对于海洋保护至关重要，因为水下栖息地的广阔规模使得手动评估不切实际。未来的工作将集中于通过利用SQUIDLE+中的所有可用资源，完全自动化基准测试过程，从而实现对水下定位管道的最广泛评估。此外，我们激励开发与自动变化检测工具集成的可靠时间地图，确保对水下生态系统的可扩展和高效监测。
---
layout: mypost
title: k050, G058 Seathru-NeRF, Neural Radiance Fields in Scattering Media
categories: [NeRF, 水下]
---

如果有研究水下场景的研究者，可以通过邮箱zhangwenniu@163.com联系我，我目前创建了一个水下场景的研究者交流群，欢迎感兴趣的研究者加入。

# 论文重点难点讲解

## 1 **研究背景与动机**

- **背景**：神经辐射场（NeRF）在新型视图生成领域取得了巨大进展，但现有方法主要假设场景是在无散射或吸收的“清晰空气”中拍摄的，忽略了介质（如雾、霾、水下环境）对物体外观的影响。

- **动机**：在水下或雾天等场景中，介质的散射和吸收效应显著影响图像质量，导致远处物体的颜色失真和对比度降低。因此，将散射介质的建模纳入NeRF框架中，能够更真实地重建和渲染这些复杂场景。

## 2 **SeaThru-NeRF框架的核心贡献**

- **核心思想**：通过将散射介质的成像模型（SeaThru模型）融入NeRF的体积渲染方程中，实现了对场景和介质的联合建模。该方法能够将场景分解为“干净”部分和“后向散射”部分，从而在渲染时可以选择性地去除介质影响。

- **关键创新点**：

  - 提出了一种新的渲染模型，将场景和介质分别用颜色和密度参数表示，并在NeRF框架内进行学习。

  - 通过模拟和真实场景的实验，展示了该方法在水下和雾天场景中的有效性，能够恢复出清晰的场景视图，并重建出更准确的3D场景结构。

## 3 **模型构建与优化**

- **模型推导**：

  - 在传统的NeRF体积渲染方程中引入介质的散射和吸收效应，将光线沿路径的颜色贡献分为“对象”和“介质”两部分。具体公式为：

    $$
    C(r) = \int_{t_n}^{t_f} T(t) \left[ \sigma_{\text{obj}}(t) c_{\text{obj}}(t) + \sigma_{\text{med}}(t) c_{\text{med}}(t) \right] dt
    $$

    其中，$$T(t)$$表示光线从$$t_n$$到$$t$$的累积透射率，$$\sigma_{\text{obj}}$$和$$\sigma_{\text{med}}$$分别表示对象和介质的密度，$$c_{\text{obj}}$$和$$c_{\text{med}}$$分别表示对象和介质的颜色。

  - 通过离散化和简化，得到了适用于优化的离散形式。

- **网络架构**：

  - 对象的颜色和密度由标准的NeRF MLP网络计算，而介质的颜色和密度则由一个独立的MLP网络计算，该网络仅依赖于光线的方向。

  - 通过约束介质参数在每条光线上的恒定性，减少了模型的复杂度。

- **优化策略**：

  - 采用了包含重建损失、提案损失和先验损失的复合损失函数，确保模型能够准确地分离对象和介质，并重建出高质量的图像。

## 4 **实验与结果**

- **实验设置**：

  - 使用了真实水下场景（红海、加勒比海、太平洋）和模拟场景（基于LLFF数据集的雾天和水下效果）。

  - 比较了MIP-NeRF-360、NeRF-W、NeRFReN等现有方法。

- **关键结果**：

  - 在真实水下场景中，SeaThru-NeRF在PSNR、SSIM和LPIPS等指标上优于现有方法，尤其是在远处物体的细节恢复上表现更好。

  - 在模拟场景中，该方法能够很好地分离场景的“干净”部分和“后向散射”部分，即使在复杂的散射介质条件下也能保持较高的重建质量。

- **局限性**：

  - 当前模型基于现有的成像模型，不支持多次散射或人工照明条件。

  - 在场景范围变化较小的情况下，介质参数的估计可能不够准确。

  - 依赖于预先提取的相机姿态信息，这在能见度较差的场景中可能难以获得。

## 5 **难点与挑战**

- **介质建模的复杂性**：在水下等散射介质中，介质的散射和吸收特性与波长密切相关，需要精确建模才能恢复出真实的场景颜色。

- **联合优化的难度**：同时优化场景和介质参数是一个复杂的优化问题，需要设计合适的网络架构和损失函数来确保模型能够正确分离对象和介质。

- **真实场景的多样性**：真实水下场景的光照条件、物体反射率和介质特性变化较大，模型需要具备足够的泛化能力以适应不同的场景。

## 6 **未来工作**

- **扩展模型能力**：考虑加入多次散射和人工照明的建模，以支持更复杂的场景。

- **改进优化策略**：进一步优化网络架构和损失函数，提高模型在复杂场景下的性能。

- **探索更多应用场景**：将该方法应用于其他类型的散射介质（如雾霾、烟雾）或动态场景。

# 论文详细讲解

## 1 **研究背景与动机**

NeRF（Neural Radiance Fields）作为一种强大的新型视图合成技术，已经在多种场景中取得了优异的效果。然而，现有NeRF及其变体主要假设场景是在无散射或吸收的“清晰空气”中拍摄的，忽略了介质（如雾、霾、水下环境）对物体外观的影响。在水下或雾天等场景中，介质的散射和吸收效应显著，导致远处物体的颜色失真和对比度降低。因此，将散射介质的建模纳入NeRF框架中，能够更真实地重建和渲染这些复杂场景。

## 2 **SeaThru-NeRF框架的核心贡献**

SeaThru-NeRF通过将散射介质的成像模型（SeaThru模型）融入NeRF的体积渲染方程中，实现了对场景和介质的联合建模。该方法能够将场景分解为“干净”部分和“后向散射”部分，从而在渲染时可以选择性地去除介质影响。具体贡献包括：

- **新的渲染模型**：将散射介质的成像模型与NeRF的体积渲染方程结合，允许同时建模场景和介质。

- **网络架构设计**：提出了一种适合学习场景信息和介质参数的网络架构，能够处理复杂的散射介质场景。

- **实验验证**：通过模拟和真实世界的水下场景，展示了该方法在恢复场景颜色和深度信息方面的优势。

## 3 **模型构建与优化**

### 3.1 **模型推导**

SeaThru-NeRF的核心是将散射介质的成像模型融入NeRF的体积渲染方程中。传统的NeRF渲染方程为：

$$
C(r) = \int_{t_n}^{t_f} T(t) \sigma(t) c(t) \, dt
$$

其中，$$T(t) = \exp \left( -\int_{t_n}^t \sigma(s) \, ds \right)$$ 是累积透射率，$$\sigma(t)$$ 和 $$c(t)$$ 分别是场景在点 $$t$$ 的密度和颜色。

在散射介质中，光线的颜色贡献分为两部分：来自场景对象的颜色和来自介质的后向散射颜色。因此，SeaThru-NeRF的渲染方程扩展为：

$$
C(r) = \int_{t_n}^{t_f} T(t) \left[ \sigma_{\text{obj}}(t) c_{\text{obj}}(t) + \sigma_{\text{med}}(t) c_{\text{med}}(t) \right] dt
$$

其中，$$\sigma_{\text{obj}}$$ 和 $$c_{\text{obj}}$$ 表示场景对象的密度和颜色，$$\sigma_{\text{med}}$$ 和 $$c_{\text{med}}$$ 表示介质的密度和颜色。

### 3.2 **网络架构**

SeaThru-NeRF的网络架构分为两部分：

`1.` **对象网络**：使用标准的NeRF MLP网络，将场景对象的密度 $$\sigma_{\text{obj}}$$ 作为位置 $$(x, y, z)$$ 的函数，颜色 $$c_{\text{obj}}$$ 作为视图方向 $$(\theta, \phi)$$ 的函数。

`2.` **介质网络**：使用独立的MLP网络，将介质的颜色 $$c_{\text{med}}$$ 和密度 $$\sigma_{\text{med}}$$ 作为视图方向的函数，并假设介质参数在每条光线上的恒定性。

### 3.3 **优化策略**

SeaThru-NeRF的优化目标是最小化重建误差，同时约束介质参数的合理性。损失函数包括：

- **重建损失**：采用RawNeRF的损失函数，计算预测颜色与真实颜色的平方误差。

$$
L_{\text{recon}}(\hat{C}, C^*) = \left\| \frac{\hat{C} - C^*}{\text{sg}(\hat{C}) + \epsilon} \right\|^2
$$

- **先验损失**：通过Laplacian分布约束每个点的透射率 $$T_{\text{obj}}$$，确保场景对象的不透明性。

$$
L_{\text{acc}}(w) = -\log \left( e^{-|T_{\text{obj}}| / 0.1} + e^{-|1 - T_{\text{obj}}| / 0.1} \right)
$$

## 4 **实验与结果**

### 4.1 **实验设置**

- **真实场景**：在红海、加勒比海和太平洋采集了多视角水下图像，使用Nikon D850相机拍摄，并通过COLMAP提取相机姿态。

- **模拟场景**：基于LLFF数据集的“Fern”场景，模拟了雾天和水下效果。

- **基线方法**：与MIP-NeRF-360、NeRF-W、NeRFReN等现有方法进行比较。

### 4.2 **关键结果**

![table.2](seathru-nerf.table.2.png)

> 表2. 与基线的比较：PSNR↑ / SSIM↑ / LPIPS↓。

- **PSNR、SSIM和LPIPS指标**：SeaThru-NeRF在真实水下场景和模拟场景中均优于现有方法，尤其是在远处物体的细节恢复上表现更好。见表2。

- **场景恢复**：SeaThru-NeRF能够更好地分离场景的“干净”部分和“后向散射”部分，即使在复杂的散射介质条件下也能保持较高的重建质量。

### 4.3 **局限性**

- 当前模型基于现有的成像模型，不支持多次散射或人工照明条件。

- 在场景范围变化较小的情况下，介质参数的估计可能不够准确。

- 依赖于预先提取的相机姿态信息，这在能见度较差的场景中可能难以获得。

## 5 **未来工作**

- **扩展模型能力**：考虑加入多次散射和人工照明的建模，以支持更复杂的场景。

- **改进优化策略**：进一步优化网络架构和损失函数，提高模型在复杂场景下的性能。

- **探索更多应用场景**：将该方法应用于其他类型的散射介质（如雾霾、烟雾）或动态场景。

通过上述设计和实验，SeaThru-NeRF为处理散射介质中的场景建模和渲染提供了一个强大的工具，显著提升了在复杂环境下的视图合成和场景恢复能力。

# 论文方法部分详解

## 1 **SeaThru-NeRF模型的核心思想**

SeaThru-NeRF通过将散射介质的成像模型（SeaThru模型）融入NeRF的体积渲染方程中，实现了对场景和介质的联合建模。该方法能够将场景分解为“干净”部分和“后向散射”部分，从而在渲染时可以选择性地去除介质影响，恢复出清晰的场景视图。

## 2 **模型推导**

### 2.1 **基本模型**

在传统NeRF中，体积渲染方程为：

$$
C(r) = \int_{t_n}^{t_f} T(t) \sigma(t) c(t) \, dt
$$

其中，$$T(t) = \exp \left( -\int_{t_n}^t \sigma(s) \, ds \right)$$ 是累积透射率，$$\sigma(t)$$ 和 $$c(t)$$ 分别是场景在点 $$t$$ 的密度和颜色。

在散射介质中，光线的颜色贡献分为两部分：来自场景对象的颜色和来自介质的后向散射颜色。因此，SeaThru-NeRF的渲染方程扩展为：

$$
C(r) = \int_{t_n}^{t_f} T(t) \left[ \sigma_{\text{obj}}(t) c_{\text{obj}}(t) + \sigma_{\text{med}}(t) c_{\text{med}}(t) \right] dt
$$

其中，$$T(t) = \exp \left( -\int_{t_n}^t \left[ \sigma_{\text{obj}}(s) + \sigma_{\text{med}}(s) \right] ds \right)$$，$$\sigma_{\text{obj}}$$ 和 $$c_{\text{obj}}$$ 表示场景对象的密度和颜色，$$\sigma_{\text{med}}$$ 和 $$c_{\text{med}}$$ 表示介质的密度和颜色。

### 2.2 **离散化与简化**

在实际实现中，积分被离散化为多个区间，每个区间内的密度和颜色被视为常数。离散化的渲染方程为：

$$
\hat{C}(r) = \sum_{i=1}^N \hat{C}_{\text{obj}, i}(r) + \sum_{i=1}^N \hat{C}_{\text{med}, i}(r)
$$

其中，

$$
\hat{C}_{\text{obj}, i}(r) = T_{\text{obj}, i} \left[ 1 - e^{-(\sigma_{\text{obj}, i} + \sigma_{\text{med}, i}) \delta_i} \right] \frac{\sigma_{\text{obj}, i}}{\sigma_{\text{obj}, i} + \sigma_{\text{med}, i}} c_{\text{obj}, i}
$$

$$
\hat{C}_{\text{med}, i}(r) = T_{\text{obj}, i} \left[ 1 - e^{-(\sigma_{\text{obj}, i} + \sigma_{\text{med}, i}) \delta_i} \right] \frac{\sigma_{\text{med}, i}}{\sigma_{\text{obj}, i} + \sigma_{\text{med}, i}} c_{\text{med}}
$$

$$T_{\text{obj}, i} = \exp \left( -\sum_{j=0}^{i-1} (\sigma_{\text{obj}, j} + \sigma_{\text{med}, j}) \delta_j \right)$$ 是区间 $$i$$ 的累积透射率。

### 2.3 **模型简化与约束**

为了简化模型，假设介质参数在每条光线方向上是恒定的，但允许在不同颜色通道上有所不同。此外，假设场景对象是不透明的，其密度在对象位置处显著高于介质密度，从而实现对象和介质的有效分离。

## 3 **网络架构**

### 3.1 **对象网络**

对象的颜色和密度由标准的NeRF MLP网络计算：

- 密度 $$\sigma_{\text{obj}}$$ 仅依赖于位置 $$(x, y, z)$$。

- 颜色 $$c_{\text{obj}}$$ 依赖于位置和视图方向 $$(\theta, \phi)$$。

### 3.2 **介质网络**

介质的颜色和密度由独立的MLP网络计算：

- 介质的颜色 $$c_{\text{med}}$$ 和密度 $$\sigma_{\text{med}}$$ 仅依赖于视图方向。

- 该网络输出每个颜色通道的介质密度 $$\sigma_{\text{attn}}$$ 和 $$\sigma_{\text{bs}}$$，分别用于对象颜色和后向散射颜色的计算。

## 4 **损失函数**

SeaThru-NeRF的优化目标是最小化重建误差，同时约束介质参数的合理性。损失函数包括：

### 4.1 **重建损失**

采用RawNeRF的损失函数，计算预测颜色与真实颜色的平方误差：

$$
L_{\text{recon}}(\hat{C}, C^*) = \left\| \frac{\hat{C} - C^*}{\text{sg}(\hat{C}) + \epsilon} \right\|^2
$$

其中，$$\text{sg}(\cdot)$$ 表示停止梯度传播，$$\epsilon = 10^{-3}$$。

### 4.2 **先验损失**

通过Laplacian分布约束每个点的透射率 $$T_{\text{obj}}$$，确保场景对象的不透明性：

$$
L_{\text{acc}}(w) = -\log \left( e^{-|T_{\text{obj}}| / 0.1} + e^{-|1 - T_{\text{obj}}| / 0.1} \right)
$$

### 4.3 **总损失**

总损失函数为：

$$
L = L_{\text{recon}}(\hat{C}, C^*) + L_{\text{prop}}(s, w) + \lambda L_{\text{acc}}(w)
$$

其中，$$L_{\text{prop}}(s, w)$$ 是MIP-NeRF-360中的“提案”损失，用于优化采样分布；$$\lambda = 0.0001$$ 是超参数。

## 5 **优化与实现**

SeaThru-NeRF基于MIP-NeRF-360的代码实现，使用Nvidia A100 GPU进行训练。网络训练250,000次迭代，每次迭代处理16,384条光线。优化过程中，损失函数和评估指标均在未经过后处理的线性图像上计算。

通过上述方法，SeaThru-NeRF能够有效地处理散射介质中的场景建模和渲染问题，显著提升了在复杂环境下的视图合成和场景恢复能力。

# 重点问题讲解

## 公式20推导

以下是对这些公式推导步骤的讲解：

### 从第一步到第二步

原始公式为：

$$\sum_{i = 0}^{k - 1} \hat{C}_{i}^{\text{med}}(\mathbf{r}) = \sum_{i = 0}^{k - 1} T_{i} \cdot (1 - e^{-\boldsymbol{\sigma}^{\text{med}} \delta}) \cdot \mathbf{c}^{\text{med}}  \tag{20}$$

这里进行近似推导，假设 $$T_{i} \approx e^{-i\boldsymbol{\sigma}^{\text{med}} \delta}$$ 。这通常是基于某种物理场景下的近似条件，比如在光传播或辐射传输模型中，当满足一定的介质特性和传播距离条件时，透射率 $$T_{i}$$ 可以用这样的指数形式近似。

将 $$T_{i} \approx e^{-i\boldsymbol{\sigma}^{\text{med}} \delta}$$ 代入上式，就得到了第二步：

$$\approx \sum_{i = 0}^{k - 1} e^{-i\boldsymbol{\sigma}^{\text{med}} \delta} \cdot (1 - e^{-\boldsymbol{\sigma}^{\text{med}} \delta}) \cdot \mathbf{c}^{\text{med}}$$
 
### 从第二步到第三步

第二步公式为：

$$\sum_{i = 0}^{k - 1} e^{-i\boldsymbol{\sigma}^{\text{med}} \delta} \cdot (1 - e^{-\boldsymbol{\sigma}^{\text{med}} \delta}) \cdot \mathbf{c}^{\text{med}}$$

这里可以把 $$(1 - e^{-\boldsymbol{\sigma}^{\text{med}} \delta}) \cdot \mathbf{c}^{\text{med}}$$ 看作一个常数项，提取到求和符号外面，得到：

$$(1 - e^{-\boldsymbol{\sigma}^{\text{med}} \delta}) \cdot \mathbf{c}^{\text{med}} \sum_{i = 0}^{k - 1} e^{-i\boldsymbol{\sigma}^{\text{med}} \delta}$$

对于等比数列求和 $$\sum_{i = 0}^{k - 1} e^{-i\boldsymbol{\sigma}^{\text{med}} \delta}$$ ，首项 $$a = 1$$，公比 $$q = e^{-\boldsymbol{\sigma}^{\text{med}} \delta}$$ ，根据等比数列求和公式 $$S_n=\frac{a(1 - q^n)}{1 - q}$$（这里 $$n = k$$），可得：

$$\sum_{i = 0}^{k - 1} e^{-i\boldsymbol{\sigma}^{\text{med}} \delta}=\frac{1 - (e^{-\boldsymbol{\sigma}^{\text{med}} \delta})^k}{1 - e^{-\boldsymbol{\sigma}^{\text{med}} \delta}}=\frac{1 - e^{-\boldsymbol{\sigma}^{\text{med}} \delta \cdot k}}{1 - e^{-\boldsymbol{\sigma}^{\text{med}} \delta}}$$

将其代入上式：

$$(1 - e^{-\boldsymbol{\sigma}^{\text{med}} \delta}) \cdot \mathbf{c}^{\text{med}} \cdot \frac{1 - e^{-\boldsymbol{\sigma}^{\text{med}} \delta \cdot k}}{1 - e^{-\boldsymbol{\sigma}^{\text{med}} \delta}}$$

分子分母的 $$(1 - e^{-\boldsymbol{\sigma}^{\text{med}} \delta})$$ 约掉，就得到了第三步：

$$=(1 - e^{-\boldsymbol{\sigma}^{\text{med}} \delta \cdot k}) \cdot \mathbf{c}^{\text{med}}$$ 

## 线性图像

在论文中提到的“线性图像”通常指的是图像的像素值与实际的光强度成线性关系，即图像数据没有经过伽马校正或其它非线性色调映射处理。这种图像在计算机视觉和图形学中被称为“线性空间”图像。

在数字摄影中，相机传感器捕捉到的原始数据通常是线性的，即像素值与入射光强度成正比。然而，为了适应人眼的感知特性，大多数显示设备（如显示器、打印机）会对接收到的图像数据进行伽马校正，将线性空间的图像转换为非线性空间的图像。伽马校正是一种幂律变换，通常用于调整图像的对比度，使得图像在显示设备上看起来更自然。

在NeRF（Neural Radiance Fields）及其相关研究中，使用线性图像作为输入数据是很重要的，因为这些方法通常基于物理的渲染模型，需要准确的光强度信息来进行场景的重建和渲染。如果使用经过伽马校正的非线性图像，可能会导致渲染结果的不准确。

在论文的实验部分，作者提到他们使用了线性图像作为输入，并在处理之前对图像进行了白平衡调整。白平衡是一种常见的图像预处理步骤，用于校正图像的色彩平衡，使得白色物体在不同光照条件下看起来是白色的。这一步骤有助于提高模型对场景颜色的准确恢复能力。

总结来说，论文中提到的“线性图像”是指未经伽马校正的原始图像数据，这种图像更符合物理渲染模型的要求，有助于提高NeRF模型在场景重建和渲染中的准确性和质量。

# 原文翻译

## 1 引言  

Mildenhall等人[25]关于神经辐射场（NeRFs）的开创性工作极大地推动了神经渲染领域的发展，得益于其灵活性和前所未有的合成图像质量。然而，原始NeRF[25]及其后续变体的公式假设图像是在清洁空气中获取的，即介质不会显著散射或吸收光线，且获取的图像仅由物体辐射组成。NeRF公式基于考虑沿3D光线采样点的体渲染方程。在清洁空气环境的假设下，一个隐含的假设（通常通过专门的损失组件[5]显式强制执行）是每条光线只遇到一个不透明（高密度）物体，相机与物体之间的密度为零。  

与清洁空气情况形成鲜明对比的是，当介质具有吸收和/或散射特性时（例如雾、烟雾、雾霾以及所有水生环境），体渲染方程具有真正的体意义，因为整个体积而不仅仅是物体对图像强度有贡献。由于NeRF模型估计场景中每个点的颜色和密度，因此它非常适合通用的体渲染，前提是使用适当的渲染模型。在这里，我们通过SeaThru-NeRF填补了这一空白，该框架结合了考虑散射介质的渲染模型。这是通过在NeRF框架内为物体（场景）和介质分配独立的颜色和密度参数来实现的。我们的方法采用SeaThru水下图像形成模型[1, 3]来处理散射介质。SeaThru是标准波长无关衰减（例如雾）图像形成模型的推广，其中使用两个不同的宽带系数来表示介质，这在衰减与波长相关时（如所有水体及某些大气条件下）更为准确。  

![fig.1](seathru-nerf.fig.1.png)

> 图1. NeRF尚未解决介质强烈影响物体外观的场景，例如水下图像。通过将散射图像形成模型纳入NeRF渲染方程，我们能够将场景分离为“干净”和后向散射组件。因此，我们可以渲染带有或不带参与介质的光线真实新视角，在后一种情况下恢复颜色，仿佛图像是在清洁空气中拍摄的。库拉索场景的结果：RAW图像（左）经过增亮和白平衡（WB）处理以进行可视化，显示出更多细节，而距离相机较远的区域（右上角）被严重的后向散射遮挡和衰减——在我们的恢复图像中，这些后向散射被有效移除。请放大以观察细节。

在我们的模型中，介质参数按颜色通道分离，并且是视角的学习函数，仅在场景中的3D光线上强制保持恒定。尝试在具有散射介质的场景上优化现有的NeRF会导致空间中漂浮的云状物体，而我们的公式使网络能够学习整个3D体积的正确表示，其中包括场景和介质。我们的实验表明，SeaThru-NeRF在具有散射介质的模拟和具有挑战性的真实场景（见图1）上实现了最先进的逼真新视角合成，这些场景包括复杂的几何形状和外观。此外，它还实现了：  

`1.` 场景的颜色恢复，仿佛它们不是通过介质成像的，因为我们的建模允许将物体外观与介质效应完全分离。  

`2.` 3D场景结构的估计，超越了运动结构重建（SFM）或当前NeRF的结果，尤其是在能见度较差的远距离区域，因为我们联合重建并推理几何和介质。  

`3.` 宽带介质参数的估计，这些参数是捕获环境的有用特性，并可能允许在不同条件下进行模拟。

## 2 相关工作  

**神经辐射场（NeRFs）：** 原始NeRF工作[25]为大量后续研究铺平了道路，在许多相关方面取得了快速而显著的进展。为简洁起见，以下我们仅关注与我们的工作密切相关的文献，并建议读者参考NeRF引入前对该领域的全面综述[43]，以及最新的综述[44]。NeRF最近在多图像设置中表现出极强的能力，这些设置涉及计算成像任务，包括HDR[50]、去模糊[20]、超分辨率[49]、低光增强[23]和去噪[32]。同样，恢复因散射和衰减效应而退化的“无介质”干净图像版本的需求也可以从神经渲染方法中受益。SeaThru-NeRF旨在建模这种退化，并能够恢复其参数，重建干净的底层场景和新视角图像。  

我们的工作与最近在挑战性环境中提高NeRF质量和鲁棒性的努力相关（例如[14, 21, 29]）。Nerf-W[21]学习每张图像的潜在嵌入，以捕捉复杂场景中的外观变化。它将场景分解为图像依赖和共享组件，以将瞬态元素与静态场景分离。NeRFReN[14]专为具有反射的场景设计，将场景分离为透射和反射分量，并分别建模为独立的NeRF。Ref-NeRF[47]引入了新的视图依赖外观参数化和结构化方法，可以表示具有镜面反射和反射的场景。  

我们的方法与其中一些方法有许多共同点，最显著的是将目标图像重建为组件的组合方式：在我们的案例中是“直接”和“后向散射”，在[21]中是“瞬态”和“静态”，在[14]中是“透射”和“反射”。然而，正如我们展示的，这些方法在散射介质场景中存在局限性，因为它们没有显式建模介质的影响。我们的方法不同之处在于能够建模一个连续的介质组件，而不是物体。  

**散射介质中的图形学与视觉：** 光在介质中的传播由辐射传输方程[10]控制。该方程描述了介质中每个粒子的光相互作用，需要大量的蒙特卡罗模拟来获得完整解[12, 16, 30, 31]，参见[30]的出色综述。在许多情况下，可以对介质进行简化假设以简化渲染[22]，其中最主要的假设是单次散射[33, 42, 48]。真实渲染需要知道介质的散射特性，这些特性可以在实验室[13, 26]或从现场图像[6]中估计。[17]提出了一种用于渲染合成大气云的多层感知器（MLP）。  

在计算机视觉中，不良天气[28]和水下[37]的环境光照下的图像形成模型在水平视角下具有相同的通用结构。水下[46]和雾中使用的人工照明有额外的项来解释光源的非均匀性。在水下，介质参数表现出强烈的波长依赖性，这被证明会影响宽带相机通道的模型精度。SeaThru模型[1–3]被提出来解决这一问题。  

散射介质中的场景重建是一个不适定问题，最初通过多帧图像解决[27,37,38,46]。后来，单图像方法提出了多种图像先验来克服问题的不适定性，参见[19,51]分别对单图像去雾和水下重建的全面综述。深度学习的引入导致了单图像去雾和水下重建网络的爆炸式增长。参见[41]和[4]分别对深度学习去雾和水下重建工作的最新综述。在水下，有人建议在图像恢复之前先求解场景的3D结构，使用雾模型[9, 35]和修订模型[2]。在SeaThruNeRF中，我们同时重建场景及其3D结构，展示了多种优势。  

WaterNeRF[40]提出了一种水下神经渲染器，仅基于直方图均衡化图像估计介质参数，与渲染分离。在这里，我们展示了在渲染方程中建模散射介质对各种不同场景的巨大优势。与我们的工作同时且独立，[11]提出了一种用于雾的NeRF。

## 3 科学背景

### 3.1 神经辐射场（NeRFs）

原始的NeRF公式[25]通过一个可训练的连续函数隐式地表示一个三维场景。它通常由一个多层感知器（MLP）$$f_{\Theta} : (\mathbf{x}, \mathbf{d}) \to (\mathbf{c}, \sigma)$$进行参数化，该函数对三维点$$\mathbf{x} = (x, y, z)$$处的密度$$\sigma$$以及从该点沿观察方向$$\mathbf{d} = (\theta, \phi)$$（通常表示为一个二维单位归一化三维向量）发出的颜色$$\mathbf{c} = (r, g, b)$$进行编码。

这种简单的表示方法用于模拟基于图像的经典渲染，通过对从已定位相机反向投影的光线进行颜色累积来实现。如果我们将沿相机光线$$\mathbf{r}$$的点参数化为$$\mathbf{r}(t) = \mathbf{o} + \mathbf{d}(t)$$，其中$$\mathbf{o}$$是相机中心，$$t \in \mathbb{R}_+$$，那么沿光线的预期（图像）颜色$$C(\mathbf{r})$$可以写成：

$$
C(\mathbf{r}) = \int_{t_n}^{t_f} T(t)\sigma(t)\mathbf{c}(t)dt \tag{1}
$$

其中：沿光线的积分限制在范围$$t \in [t_n, t_f]$$（场景内容的近边界和远边界）内；$$\sigma(t)$$和$$\mathbf{c}(t)$$是点$$\mathbf{r}(t)$$处的密度及其朝向相机中心发出的颜色的简写；$$T(t)$$表示从$$t_n$$到$$t$$沿光线的累积透射率（即光线从$$t_n$$传播到$$t$$而不与沿途其他粒子碰撞的概率），其表达式为：

$$
T(t) = \exp\left(- \int_{t_n}^{t} \sigma(s)ds\right) \tag{2}
$$

在实际应用中，公式(1)中的渲染颜色$$C(\mathbf{r})$$使用求积法则进行近似，即将范围$$[t_n, t_f]$$离散化为一组$$N$$个区间$$I_i = [s_i, s_{i + 1}]$$（其中$$t_n = s_0 < \cdots < s_N = t_f$$），假设在每个区间内密度$$\sigma$$和颜色$$\mathbf{c}$$是恒定的（例如，在每个区间的中心点对模型进行一次查询）。

在公式(1)的离散化版本中：

$$
\hat{C}(\mathbf{r}) = \sum_{i = 1}^{N} C_i(\mathbf{r}) \tag{3}
$$

区间$$I_i$$的贡献$$C_i(\mathbf{r})$$由下式给出：

$$
C_i(\mathbf{r}) = \int_{s_i}^{s_{i + 1}} T(t)\sigma_i\mathbf{c}_idt = T(s_i)\left(1 - e^{-\sigma_i\delta_i}\right)\mathbf{c}_i \tag{4}
$$

其中$$\sigma_i$$和$$\mathbf{c}_i$$是第$$i$$个区间上的（恒定）密度和颜色，区间长度为$$\delta_i = s_{i + 1} - s_i$$，区间开始处的透射率$$T(s_i)$$为：

$$
T(s_i) = \exp\left(- \sum_{j = 0}^{i - 1} \sigma_j\delta_j\right) \tag{5}
$$

这个完全可微的NeRF模型（采用离散化渲染方案）使用简单的重建损失进行训练：

$$
L = \sum_{\mathbf{r} \in R} \|\hat{C}(\mathbf{r}) - C(\mathbf{r})\|^2 \tag{6}
$$

将每个渲染的训练图像像素$$\hat{C}(\mathbf{r})$$与其真实颜色$$C(\mathbf{r})$$进行比较。然后，该模型可用于合成逼真的新视角图像。 

### 3.2 散射介质中的图像形成

![fig.2](seathru-nerf.fig.2.png)

> 图2. 我们的光线模型示意图。我们假设每条光线最多有一个不透明物体的场景。介质是半透明的，每条光线的密度恒定。控制后向散射和水中物体的密度不同[1]。

在雾、霾或水下的图像形成与在清晰空气中的图像形成主要在两个方面存在差异。首先，从物体发出的直接信号会根据距离和波长而衰减。其次，该信号会被后向散射（也称为路径辐射或杂散光）遮挡——这种辐射是由于沿视线（LOS）的粒子的内散射而产生的，如图2所示。遮挡的后向散射层的强度和颜色与场景内容无关，并且其强度沿视线累积，随距离增加。因此，远处物体的可见度和对比度会显著降低，并且它们的颜色会失真。

我们采用修订后的模型[1]作为环境光照下散射介质中图像形成的通用模型。图像强度（每像素，每个颜色通道）由下式给出：

$$
I = \underbrace{\underbrace{J}_{\text{color}} \cdot \underbrace{(e^{-\beta^{D}(\mathbf{v}_D)\cdot z})}_{\text{attenuation}}}_{\text{direct}} + \underbrace{\underbrace{B^{\infty}}_{\text{color}} \cdot \underbrace{(1 - e^{-\beta^{B}(\mathbf{v}_B)\cdot z})}_{\text{attenuation}}}_{\text{backscatter}} \tag{7}
$$

其中，$$I$$ 是相机拍摄的距离为 $$z$$ 的场景的线性图像，$$J$$ 是在距离 $$z$$ 上没有介质时原本会拍摄到的清晰场景，$$B^{\infty}$$ 是无穷远处的后向散射水色，即在不包含物体的区域的后向散射。最后，$$\beta^{D}$$ 和 $$\beta^{B}$$ 分别是衰减系数和后向散射系数，这两个参数描述了介质的影响。向量 $$\mathbf{v}_{D}$$ 和 $$\mathbf{v}_{B}$$ 表示 $$\beta^{D}$$ 和 $$\beta^{B}$$ 与距离、物体反射率、环境光光谱、相机光谱响应以及水体的物理散射和光束衰减系数之间的相关性，所有这些都是波长的函数。文献[2]表明，在一幅图像中可以假定 $$\beta^{B}$$ 是恒定的，而 $$\beta^{D}$$ 主要取决于物体距离，对物体反射率的依赖较弱，因此求解完整模型至少需要 6 个未知数。通常假定场景中 $$B^{\infty}$$ 的值是均匀的，但如文献[6]中所讨论的，由于太阳的方向性等因素，它几乎从来都不是真正均匀的。

该模型也适用于雾霾和雾气场景，在这种情况下，衰减对波长的依赖非常小。然后，成像过程会大大简化，因为可以假定只有一个在各个颜色通道上均匀的介质参数，用一个标量表示（$$\beta^{D} = \beta^{B} = \beta$$）[38]。在水下，通常会做出简化（不太准确）的假设，即 $$\beta^{D} = \beta^{B}$$，这样未知数就减少到 3 个[6, 7, 37]。 

## 4 SeaThru - NeRF

### 4.1 基本模型推导

在这里，我们考虑比原始NeRF[25]更一般的场景，在这种场景中，光穿过散射介质而不是自由空间，这对拍摄到的颜色产生显著影响。根据文献[22]，我们建议在公式(1)和(2)中加入介质因素：

$$
C(\mathbf{r}) = \int_{t_n}^{t_f} T(t)\left(\sigma^{\text{obj}}(t)\mathbf{c}^{\text{obj}}(t) + \sigma^{\text{med}}(t)\mathbf{c}^{\text{med}}(t)\right)dt \tag{8}
$$

其中：

$$
T(t) = \exp\left(- \int_{t_n}^{t} \left(\sigma^{\text{obj}}(s) + \sigma^{\text{med}}(s)\right)ds\right) \tag{9}
$$

对物体和介质分别使用不同的颜色和密度参数。注意，当介质密度$$\sigma^{\text{med}}$$为零时，这些公式就简化为公式(1)和(2)。

转换到离散化版本，类似地，我们得到公式(4)和(5)的扩展形式如下<sup>3</sup>：

$$
C_i(\mathbf{r}) = T(s_i)\left(1 - e^{-(\sigma_i^{\text{obj}} + \sigma_i^{\text{med}})\delta_i}\right)\frac{\sigma_i^{\text{obj}}\mathbf{c}_i^{\text{obj}} + \sigma_i^{\text{med}}\mathbf{c}_i^{\text{med}}}{\sigma_i^{\text{obj}} + \sigma_i^{\text{med}}} \tag{10}
$$

透射率为

$$
T(s_i) = \exp\left(- \sum_{j = 0}^{i - 1} (\sigma_j^{\text{obj}} + \sigma_j^{\text{med}})\delta_j\right) \tag{11}
$$

将离散化的颜色渲染公式(3、10和11)拆分为“物体”和“介质”两部分，我们得到：

$$
\hat{C}(\mathbf{r}) = \sum_{i = 1}^{N} \hat{C}_i^{\text{obj}}(\mathbf{r}) + \sum_{i = 1}^{N} \hat{C}_i^{\text{med}}(\mathbf{r}) \tag{12}
$$

$$
\hat{C}_i^{\text{obj}}(\mathbf{r}) = T(s_i)\left(1 - e^{-(\sigma_i^{\text{obj}} + \sigma_i^{\text{med}})\delta_i}\right)\frac{\sigma_i^{\text{obj}}\mathbf{c}_i^{\text{obj}}}{\sigma_i^{\text{obj}} + \sigma_i^{\text{med}}} \tag{13}
$$

$$
\hat{C}_i^{\text{med}}(\mathbf{r}) = T(s_i)\left(1 - e^{-(\sigma_i^{\text{obj}} + \sigma_i^{\text{med}})\delta_i}\right)\frac{\sigma_i^{\text{med}}\mathbf{c}_i^{\text{med}}}{\sigma_i^{\text{obj}} + \sigma_i^{\text{med}}} \tag{14}
$$ 

作为对模型进行约束和简化的第一步，我们将介质参数约束为沿三维视线方向保持恒定。我们去掉相应的区间索引，只保留仅依赖于光线$$\mathbf{r}$$的$$\sigma^{\text{med}}$$和$$\mathbf{c}^{\text{med}}$$。$$\mathbf{c}^{\text{med}}$$的一致性源于沿视线（LOS）均匀相位函数（散射辐射与散射角的依赖关系）的常见假设[22, 30]。对于密度$$\sigma^{\text{med}}$$，我们将其设定为每个颜色通道相互独立，但每条光线保持恒定。与假设在每张图像甚至每个场景中都保持恒定的模型相比，这限制要小得多[2]。这些约束将通过网络中的相应结构选择来实现。

![fig.2](seathru-nerf.fig.2.png)

> 图2. 我们的光线模型示意图。我们假设每条光线最多有一个不透明物体的场景。介质是半透明的，每条光线的密度恒定。控制后向散射和水中物体的密度不同[1]。

此外，我们假设场景中的物体是不透明的，因此除了在物体所在位置有一个高峰值外，沿光线的物体密度接近于零。另一方面，介质是半透明的，其特征是具有一个较低的非零密度。这意味着在物体之前$$\sigma^{\text{med}} \gg \sigma^{\text{obj}}$$，而在物体处$$\sigma^{\text{med}} \ll \sigma^{\text{obj}}$$，如图2所示。因此，公式(13 - 14)简化为：

$$
\hat{C}_i^{\text{obj}}(\mathbf{r}) = T_i \cdot \left(1 - e^{-\sigma_i^{\text{obj}}\delta_i}\right) \cdot \mathbf{c}_i^{\text{obj}} \tag{15}
$$

$$
\hat{C}_i^{\text{med}}(\mathbf{r}) = T_i \cdot \left(1 - e^{-\sigma^{\text{med}}\delta_i}\right) \cdot \mathbf{c}^{\text{med}} \tag{16}
$$

$$
T_i = \exp\left(- \sum_{j = 0}^{i - 1} \sigma_j^{\text{obj}}\delta_j\right) \cdot \exp\left(-\sigma^{\text{med}}s_i\right) \tag{17}
$$ 

### 4.2 与水下图像形成的关系

在本节中，我们将展示我们的模型可以简化为3.2节中描述的图像形成模型，该模型并非基于体渲染。为此，我们考虑沿着大小恒定为$$\delta$$的区间对光线进行采样，并且不透明物体出现在深度$$z$$处，即在区间$$I_k = [s_k, s_{k + 1}]$$的起始位置，其中$$k$$为某个整数（即：对于每个$$i$$，$$s_i = i \cdot \delta$$，特别地，$$z = k \cdot \delta$$）。

在这种情况下，对于所有$$i < k$$，$$\sigma_i^{\text{obj}} \approx 0$$，且$$\sigma_k^{\text{obj}} \gg \sigma^{\text{med}}$$。这意味着$$C_k^{\text{med}}(\mathbf{r}) \ll C_k^{\text{obj}}(\mathbf{r})$$且对于$$i < k$$，$$C_i^{\text{obj}}(\mathbf{r}) \approx 0$$，因此我们可以将渲染公式(12)写为：

$$
\hat{C}(\mathbf{r}) \approx \hat{C}_k^{\text{obj}}(\mathbf{r}) + \sum_{i = 0}^{k - 1} \hat{C}_i^{\text{med}}(\mathbf{r}) \tag{18}
$$

此外，由于物体是不透明的，我们可以假设密度$$\sigma_k^{\text{obj}}$$足够大，使得第$$k$$个区间末尾的透射率$$T(s_{k + 1})$$几乎降为零，即：$$e^{-\sigma_k^{\text{obj}}\delta} \approx 0$$。因此：

$$
\begin{align*}
\hat{C}_k^{\text{obj}}(\mathbf{r}) &= T_i \cdot \left(1 - e^{-\sigma_i^{\text{obj}}\delta}\right) \cdot \mathbf{c}_i^{\text{obj}} \tag{19}\\
&\approx e^{-k\sigma^{\text{med}}\delta} \left(1 - e^{-\sigma_k^{\text{obj}}\delta}\right)\mathbf{c}_k^{\text{obj}} \approx e^{-\sigma^{\text{med}}\delta \cdot k}\mathbf{c}_k^{\text{obj}}
\end{align*}
$$

并且：

$$
\begin{align*}
\sum_{i = 0}^{k - 1} \hat{C}_i^{\text{med}}(\mathbf{r}) &= \sum_{i = 0}^{k - 1} T_i \cdot \left(1 - e^{-\sigma^{\text{med}}\delta}\right) \cdot \mathbf{c}^{\text{med}} \tag{20}\\
&\approx \sum_{i = 0}^{k - 1} e^{-i\sigma^{\text{med}}\delta} \cdot \left(1 - e^{-\sigma^{\text{med}}\delta}\right) \cdot \mathbf{c}^{\text{med}}\\
&= \left(1 - e^{-\sigma^{\text{med}}\delta \cdot k}\right) \cdot \mathbf{c}^{\text{med}}
\end{align*}
$$

将公式(19)和(20)代入公式(18)（回想深度$$z$$等于$$k \cdot \delta$$）可得：

$$
\hat{C}(\mathbf{r}) \approx e^{-\sigma^{\text{med}}z} \cdot \mathbf{c}_k^{\text{obj}} + \left(1 - e^{-\sigma^{\text{med}}z}\right) \cdot \mathbf{c}^{\text{med}} \tag{21}
$$

这恰好得到了常用的散射介质图像形成模型[8, 37]。通过与公式(7)比较可以看出，其中$$\sigma^{\text{med}} = \beta^{D} = \beta^{B}$$起到衰减系数的作用（对直接信号和后向散射相同），$$\mathbf{c}^{\text{med}}$$是杂散光$$B^{\infty}$$（无穷远处的后向散射颜色），而$$\mathbf{c}_k^{\text{obj}}$$是清晰图像颜色$$J$$ 。 

### 4.3 最终模型

我们对4.1节中提出的基本模型进行了一些改进。在4.2节中，我们表明我们的渲染公式会得到一个图像形成模型，其中直接信号和后向散射部分的衰减系数相同。根据3.2节中的讨论，以及文献[1]中所示，当将这些公式应用于具有宽带颜色通道的相机时，在$$C_i^{\text{obj}}(\mathbf{r})$$中相机所“感受”到的有效$$\sigma^{\text{med}}$$与在$$C_i^{\text{med}}(\mathbf{r})$$中所“感受”到的不同。因此，在我们的最终模型中，我们在每个部分对$$\sigma^{\text{med}}$$使用不同的参数，对于$$C_i^{\text{obj}}(\mathbf{r})$$和$$C_i^{\text{med}}(\mathbf{r})$$分别将它们称为$$\sigma^{\text{attn}}$$和$$\sigma^{\text{bs}}$$。我们的最终公式如下：

$$
\begin{align*}
\hat{C}_i^{\text{obj}}(\mathbf{r}) &= T_i^{\text{obj}} \cdot \exp\left(-\sigma^{\text{attn}}s_i\right) \cdot \left(1 - \exp\left(-\sigma_i^{\text{obj}}\delta_i\right)\right) \cdot \mathbf{c}_i^{\text{obj}}\\
\hat{C}_i^{\text{med}}(\mathbf{r}) &= T_i^{\text{obj}} \cdot \exp\left(-\sigma^{\text{bs}}s_i\right) \cdot \left(1 - \exp\left(-\sigma^{\text{bs}}\delta_i\right)\right) \cdot \mathbf{c}^{\text{med}}\\
T_i^{\text{obj}} &= \exp\left(- \sum_{j = 0}^{i - 1} \sigma_j^{\text{obj}}\delta_j\right) \tag{22}
\end{align*}
$$

![fig.3](seathru-nerf.fig.3.png)

> 图3. SeaThru-NeRF架构。“物体”输出（密度和颜色，绿色部分）的计算遵循标准NeRF架构，而“介质”组件（紫色部分）由依赖于视角方向的单独子网（“介质MLP”）每条光线计算一次。

基于我们的推导，我们提出以下架构（图3）。如文献[25]中所述，物体属性由一对多层感知器（MLP）计算，使得密度$$\sigma^{\text{obj}}$$仅是位置$$(x, y, z)$$的函数，而颜色$$\mathbf{c}^{\text{obj}}$$还由观察方向$$(\theta, \phi)$$决定。介质参数$$\mathbf{c}^{\text{med}}$$、$$\sigma^{\text{bs}}$$、$$\sigma^{\text{attn}}$$由一个单独的多层感知器处理，该感知器仅以方向作为输入，这符合我们将它们约束为每个观察方向保持恒定的决定。 

### 4.4 损失函数

我们将沿光线的样本序列记为$$\mathbf{s} = \{s_i\}_{i = 0}^{N}$$，在第$$i$$个区间$$[s_i, s_{i + 1}]$$上的物体“权重”为：

$$
w_i^{\text{obj}} = T_i^{\text{obj}} \cdot \left(1 - \exp\left(-\sigma_i^{\text{obj}}\delta_i\right)\right) \tag{23}
$$

这些权重的序列记为$$\mathbf{w} = \{w_i^{\text{obj}}\}_{i = 0}^{N - 1}$$，真实（监督）像素颜色记为$$C^{*}$$。

我们的损失是以下各项的组合：

$$
\mathcal{L} = \mathcal{L}_{\text{recon}}(\hat{C}, C^{*}) + \mathcal{L}_{\text{prop}}(\mathbf{s}, \mathbf{w}) + \lambda\mathcal{L}_{\text{acc}}(\mathbf{w}) \tag{24}
$$

其中，$$\lambda = 0.0001$$是通过交叉验证选择的。由于我们处理的是线性图像，我们采用RawNeRF[23]的重建损失：

$$
\mathcal{L}_{\text{recon}}(\hat{C}, C^{*}) = \left(\frac{\hat{C} - C^{*}}{\text{sg}(\hat{C}) + \epsilon}\right)^2 \tag{25}
$$

其中，$$\text{sg}(\cdot)$$表示停止梯度，$$\epsilon = 10^{-3}$$。“提议”损失$$\mathcal{L}_{\text{prop}}(\mathbf{s}, \mathbf{w})$$是Mip - NeRF - 360[5]的固有部分。它对“原始”和“提议”采样中物体权重分布之间的差异进行惩罚，其中只有后者用于渲染[5]。

为了在包含物体的空间点和仅包含介质的空间点之间进行二元区分，我们对光线上每个点的透射率$$T_i^{\text{obj}}$$添加一个先验条件，使其要么为0要么为1，不允许存在半透明物体。这被建模为两个拉普拉斯分布的混合，模式分别为0和1（参考文献[34]）：

$$
\mathbb{P}(x) \propto e^{-\frac{|x|}{0.1}} + e^{-\frac{|1 - x|}{0.1}} \tag{26}
$$

并使用负对数似然损失：

$$
\mathcal{L}_{\text{acc}}(\mathbf{w}) = -\log \mathbb{P}(T_i^{\text{obj}}) \tag{27}
$$ 

### 4.5 实现与优化

我们的实现基于Mip - NeRF - 360[5]中发布的代码，在我们的场景中选择性能最佳的基线，即使用归一化设备坐标（NDC）的前向配置。对于介质多层感知器（mediumMLP），我们使用6个具有256个特征的线性层和一个软加（softplus）激活函数，然后是3个分支的全连接层，使用sigmoid激活函数来预测$$c^{\text{med}}$$，使用softplus激活函数来预测$$\sigma^{\text{attn}}$$和$$\sigma^{\text{bs}}$$。

在渲染方案中，[5]将最远的$$\delta_i$$初始化为无穷大，使网络能够为不与任何物体相交的光线预测背景颜色。我们禁用了这一设置，因为它会妨碍我们的方法解释对沿光线渲染颜色有贡献的介质。我们保持与[5]中相同的学习率和优化参数。网络训练250,000次迭代，批量大小为16384条光线，在英伟达A100 GPU上大约需要10小时。损失函数和指标在任何后处理之前根据输出进行计算。 

## 5 实验与结果

### 5.1 实验

**真实世界场景**：我们通过在三个不同海域潜水获取水下多图像场景，分别是红海（以色列埃拉特）、加勒比海（库拉索岛）和太平洋（巴拿马），图像数量分别为20张、20张和18张，每组各预留3张用于验证。这些数据涵盖了各种不同的水质条件和成像条件。图像是使用尼康D850单反相机，搭配Nauticam水下相机壳和穹顶端口拍摄的RAW格式图像，以避免危及小孔成像模型的折射问题[45]，并下采样至平均尺寸900×1400。在处理之前，对输入的线性图像进行白平衡处理，每个通道裁剪0.5%以去除极端噪声像素。最后，使用COLMAP[39]提取相机位姿。

**模拟场景**：我们使用LLFF数据集[24]中的Fern场景进行模拟。我们运行了MIP - NeRF - 360[5]，并使用预测的深度图来模拟水下和雾天场景。根据公式(7)添加水体，参数值为：$$\beta^{D} = [1.3, 1.2, 0.9]$$，$$\beta^{B} = [0.95, 0.85, 0.7]$$，$$B^{\infty} = [0.07, 0.2, 0.39]$$。雾的模拟基于文献[36]，其中$$\beta = 1.2$$。 

**基线方法。** 所有方法的输入都是同一组白平衡的线性图像。在渲染介质中的场景任务中，我们与以下NeRF方法进行比较：MIP-NeRF-360，使用NDC的前向视角（MIP360）[5]、NeRF-W[21]和NeRFReN[14]。在重建无介质的干净图像任务中，我们与NeRF-W进行比较，其中“瞬态”组件可视为干净图像，“静态”图像可视为后向散射，并与NeRFReN进行比较，其中“反射”和“透射”组件分别视为干净图像和后向散射图像。我们另外与单图像场景重建方法进行比较：普通白平衡、SeaThru[2]、Bekerman等人的领先“经典”方法[6]，以及最近的基于深度学习的方法[52]（以及[15, 52]，其结果在项目网站上提供）。SeaThru需要深度图，我们按照原始建议使用SFM（Agisoft Metashape）生成这些深度图。这些深度图用于与不同方法的结果进行比较。  

**照片后期处理。** 由于输入和输出图像是线性的，我们对所有线性重建场景应用照片后期处理以增强场景对比度和外观，使用[18]中的数码相机管道。这是为了改善可视化效果，便于定性比较，而PSNR是在未进行照片后期处理的原始线性图像上计算的。

### 5.2 结果

![fig.4](seathru-nerf.fig.4.png)

> 图4. 在LLFF数据集[24]的Fern场景上进行的合成实验。顶部：我们的方法可以处理“空气中”场景，生成均匀的零后向散射图像，重建PSNR接近基线[5]的23.73。中部和底部：我们的方法在模拟雾和水下效果中分离干净和后向散射组件。重建质量下降得较为平缓。

我们首先对干净图像进行合理性检查。我们的方法正确估计了零后向散射，并且不会强制估计介质（图4顶部）。在我们的模拟雾和水下场景中（图4中部和底部），我们的方法很好地分离了场景的组件，仅在水下场景中PSNR略有下降。  

![fig.1](seathru-nerf.fig.1.png)

> 图1. NeRF尚未解决介质强烈影响物体外观的场景，例如水下图像。通过将散射图像形成模型纳入NeRF渲染方程，我们能够将场景分离为“干净”和后向散射组件。因此，我们可以渲染带有或不带参与介质的光线真实新视角，在后一种情况下恢复颜色，仿佛图像是在清洁空气中拍摄的。库拉索场景的结果：RAW图像（左）经过增亮和白平衡（WB）处理以进行可视化，显示出更多细节，而距离相机较远的区域（右上角）被严重的后向散射遮挡和衰减——在我们的恢复图像中，这些后向散射被有效移除。请放大以观察细节。

![fig.5](seathru-nerf.fig.5.png)

> 图5. 在“红海”场景中进行介质中的场景渲染。从左到右：白平衡输入图像，我们的结果，MIP360[23]，NerfW[21]，NeRFReN[14]。[前两行]训练图像和放大区域。即使在过拟合训练图像的任务中，各方法之间也存在显著差异，从放大区域（红色方块）可以看出，我们能够在更远的区域重建精细细节（尽管PSNR较低）。[后两行]新视角和放大区域。我们的方法取得了最高的PSNR（见表1、2），并在更远的区域提供了更好的细节（见红色方块放大区域）。我们的深度重建在更远的区域提供了更多细节。

![table.1](seathru-nerf.table.1.png)

> 表1. 红海数据集上介质中渲染的平均PSNR：我们的方法，3种消融变体（文中描述：I– 1个参数，II– 3个参数，III– 公式13、14）以及其他方法。

![table.2](seathru-nerf.table.2.png)

> 表2. 与基线的比较：PSNR↑ / SSIM↑ / LPIPS↓。

**图像渲染和新视角合成（在介质中）。** 定性结果总结在图1和图5中。表1总结了红海场景验证集上的平均PSNR，我们的方法在其中取得了最高的PSNR。需要注意的是，NeRF-W[21]和NeRFReN[14]都基于NeRF[25]模型，而我们的代码基于Mip-NeRF-360[5]，后者通常在更远的区域中改善了重建细节。因此，我们真正的比较对象是[5]，我们在验证集上平均提高了约0.8dB。表2比较了所有真实世界和模拟场景验证集上的PSNR、SSIM和LPIPS与[5]的结果。我们的方法在大多数情况下表现更好，尤其是在更远的区域（红色方块）中表现尤为突出。在训练集上，[5]通过错误地将水建模为附近的蓝色物体，在PSNR方面实现了合理的场景渲染，这可以从深度图中看出，表明顶部区域有一个近处物体。即便如此，在我们的结果中，更远的物体重建得更加详细。NeRFReN[14]的深度图在中距离区域变得平坦，而我们的深度图在更远的区域提供了更多信息。SFM由于缺乏特征，无法在这些区域估计深度。  

![fig.6](seathru-nerf.fig.6.png)

> 图6. 在“红海”场景中进行场景重建。第1-3列：比较我们的方法（顶行）和其他两种NeRF——Nerf-W[21]（中行）和NeRFReN[14]（底行）。尽管它们并非为散射介质开发，但仍成功将训练图像分离为两个组件（第1、2列），类似于干净场景和后向散射。然而，它们的后向散射组件包含物体的细节，而我们的方法正确显示了仅来自介质的信号。同样（第3列），它们的新视角“干净”渲染效果不如我们，尤其是在远距离区域。第4列：在单图像方法中，SeaThru[2]表现最佳（但需要深度图作为输入）。然而，由于深度信息不准确，远距离区域可见颜色伪影。

**场景恢复（图6）。** 在与NeRF方法进行场景分离的比较中，显然我们的分离方法对介质的处理效果最好。NeRF-W[21]的结果中，场景虽然有所分离但模糊，而NeRFReN[14]的结果中，场景的较近部分重建得相对较好，但更远的区域缺失。在这两种方法中，场景的物体在后向散射图像中可见，这显然是错误的，并且减少了物体图像中的相关信号。在我们的结果中，由于适当的建模，后向散射没有被物体污染。与场景恢复方法相比，SeaThru[2]是表现最好的，但仅限于SFM提供深度图的区域。单图像方法的表现不佳，尤其是在中距离及更远的区域。在干净场景的新视角合成中，我们能够预测整个场景的更远部分。  

**消融研究。**  

![table.1](seathru-nerf.table.1.png)

> 表1. 红海数据集上介质中渲染的平均PSNR：我们的方法，3种消融变体（文中描述：I– 1个参数，II– 3个参数，III– 公式13、14）以及其他方法。

(i) **介质参数数量：** 根据第3.2节的讨论，公式(7)的详细版本使用了6个介质系数参数，而简化版本使用1个（表1中标记为I）或3个（表1中标记为II）。表1中对此进行了消融研究，从PSNR来看，使用6个参数的详细模型得分更高。  

![table.1](seathru-nerf.table.1.png)

> 表1. 红海数据集上介质中渲染的平均PSNR：我们的方法，3种消融变体（文中描述：I– 1个参数，II– 3个参数，III– 公式13、14）以及其他方法。

(ii) **渲染方程。** 我们将我们的基本渲染模型（公式13、14，表1中标记为III）与最终模型（公式22，标记为“ours”）进行比较。后者的PSNR较低。

## 6 讨论  

我们为NeRF提供了一个重要的扩展，使其能够渲染在散射介质（如雾、霾和水下）中获取的场景。迄今为止，NeRF提供了一个体渲染框架，但没有考虑介质的性质，导致生成的是二元的“占用”体积。我们的公式允许不透明物体存在于半透明介质中，该介质以波长依赖的方式同时具有散射和吸收特性。我们在具有复杂3D结构的真实世界场景中展示了这一点。我们的场景是前向的，并且包含完全没有物体的区域，而我们的方法能够解释这些区域。在更远的区域，水效应非常强烈，这阻碍了单图像方法和多视图方法中的特征匹配。我们的方法同时结合了所有图像的信息，以整体方式学习场景。这使得场景重建（在介质中和干净的）效果更好，并且能够估计深度和介质参数。  

我们的方法有几个局限性。虽然它基于当前最先进的图像形成模型，但该模型并未考虑多次散射或人工照明。与NeRF常见的情况一样，它需要预先提取的相机姿态，这在能见度差的情况下可能具有挑战性。最后，介质的参数在场景范围在视角之间有足够变化的集合中学习效果更好。该公式的优势在于对介质的建模。因此，在不符合模型假设的场景中（例如，具有显著闪烁的水下场景），它的表现会受到影响。未来，我们计划添加能够解释瞬态效应（如闪烁）的组件，并继续探索更多样化的场景和介质参数的估计。  

**致谢。** 本研究由以色列科学基金会资助（项目号#680/18）、以色列科技部资助（项目号#1001577600和#1001593851）、欧盟地平线2020研究与创新计划（项目号GA 101094924，ANERIS）、Leona M.和Harry B. Helmsley慈善信托基金、Maurice Hatter基金会以及Schmidt海洋技术合作伙伴资助。我们感谢Matan Yuval的大量数据贡献，以及Opher Bar-Nathan和Yuval Goldfracht在实验中的帮助。
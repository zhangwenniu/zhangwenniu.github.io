---
layout: mypost
title: 每日学习笔记
categories: [学习笔记]
---

# 2023年4月4日

## Question 11: prepend是什么意思？

> From GPT.

"prepend" 是一个英语单词，意思是在某个序列或列表的开头添加元素或字符串。在编程语言中，"prepend" 常用于在数组、列表或字符串的开头添加一个元素或字符串。例如，在Python中，可以使用列表的insert()方法或字符串的加号运算符来在开头添加元素或字符串。

> From Olu. 

*prepend* vt. 预先考虑；预谋

## Question 10: Pytorch matmul是什么操作？

> From https://pytorch.org/docs/stable/generated/torch.matmul.html

是矩阵乘法，支持batch矩阵乘法。一维输入时候是向量的点乘操作，逐个元素相乘并得到结果。

## Question 9: Pytorch的hook是什么操作？

> From pytorch中的钩子（Hook）有何作用？ - 知乎用户的回答 - 知乎 https://www.zhihu.com/question/61044004/answer/183682138

用来保存中间变量的梯度。

> From https://www.cnblogs.com/sddai/p/14412250.html

> UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
> warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "

关于文章中提到register_backward_hook只能输出fc2的input和output，目前（2023年4月4日）的版本里，已经有register_full_backward_hook，可以获得完整Module的输入输出。
修改后的代码为：
```python
model = Model()
model.register_full_backward_hook(hook_fn_backward)
```

修改后的输出为：
```
	In the hook_fn_backward
Model(
  (fc1): Linear(in_features=3, out_features=4, bias=True)
  (relu1): ReLU()
  (fc2): Linear(in_features=4, out_features=1, bias=True)
)
grad_output (tensor([[1.]]),)
grad_input (tensor([[22., 26., 30.]]),)
==========Saved inputs and outputs==========
grad output:  (tensor([[1.]]),)
grad input:  (tensor([[22., 26., 30.]]),)
```

# Question 8: weight_norm是什么意思？

> [torch.nn.utils.weight_norm — PyTorch 2.0 documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html?highlight=weight_norm#torch.nn.utils.weight_norm)

> [1602.07868, Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks (arxiv.org)](https://arxiv.org/abs/1602.07868)

weight_norm是arxiv: 1602.007868文章提出的将网络权重再次参数化的方法，分解为方向和幅值（范数），分别是`weight_v`和`weight_g`，`dim`指求取范数、幅值的维度，`dim=k`的维度不会发生变化，`dim=None`会在整体权值矩阵上求取范数。

# Question 15:  Softplus里面的beta是什么意思？

> nn.Modules.activation.py

```python


class Softplus(Module):
    r"""Applies the element-wise function:

    .. math::
        \text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))

    SoftPlus is a smooth approximation to the ReLU function and can be used
    to constrain the output of a machine to always be positive.

    For numerical stability the implementation reverts to the linear function
    when :math:`input \times \beta > threshold`.

    Args:
        beta: the :math:`\beta` value for the Softplus formulation. Default: 1
        threshold: values above this revert to a linear function. Default: 20

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: ../scripts/activation_images/Softplus.png

    Examples::

        >>> m = nn.Softplus()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    __constants__ = ['beta', 'threshold']
    beta: int
    threshold: int

    def __init__(self, beta: int = 1, threshold: int = 20) -> None:
        super(Softplus, self).__init__()
        self.beta = beta
        self.threshold = threshold

    def forward(self, input: Tensor) -> Tensor:
        return F.softplus(input, self.beta, self.threshold)

    def extra_repr(self) -> str:
        return 'beta={}, threshold={}'.format(self.beta, self.threshold)

```


$$
\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))
$$
我还是要说一句，GPT无敌。

> From GPT. 
>
> Beta参数的大小将直接影响Softplus函数的形状。较小的Beta值会产生更平滑的曲线，而较大的Beta值会导致曲线在原点处有更快的增长率。
>
> 具体来说，当Beta=1时，Softplus函数的曲线是一个类似于ReLU函数的“平滑化”版本，随着输入x的增加，输出值逐渐增加并最终趋近于无限大。
>
> 当Beta增加时，Softplus函数的曲线在原点处的增长率会变得更加陡峭，这意味着Softplus函数对小于零的输入值的响应更加敏感。这种形状在某些神经网络中很有用，可以使模型对负输入有更大的响应，从而提高其表现力。
>
> 这将绘制Softplus函数及其导数的图形。可以看出，导数在x<0时是较小的正值，在x>0时则接近于1，这与Softplus函数的形状有关。

```python
# 这段代码由GPT编写。
# 以下是绘制左图和右图，分别展示beta=1、20时的Softplus函数及其导数的代码块：-GPT
import numpy as np
import matplotlib.pyplot as plt

def softplus(x, beta=1):
    return 1/beta * np.log(1 + np.exp(beta*x))

def softplus_derivative(x, beta=1):
    return 1/(1 + np.exp(-beta*x))

x = np.linspace(-10, 10, 100)

# Beta=1
beta_1 = 1
y_1 = softplus(x, beta_1)
y_prime_1 = softplus_derivative(x, beta_1)

plt.subplot(1, 2, 1)
plt.plot(x, y_1, label='Softplus(x)')
plt.plot(x, y_prime_1, label='Softplus\'(x)')
plt.title(f'Softplus Function and Its Derivative (Beta={beta_1})')
plt.xlabel('x')
plt.legend()

# Beta=20
beta_20 = 20
y_20 = softplus(x, beta_20)
y_prime_20 = softplus_derivative(x, beta_20)

plt.subplot(1, 2, 2)
plt.plot(x, y_20, label='Softplus(x)')
plt.plot(x, y_prime_20, label='Softplus\'(x)')
plt.title(f'Softplus Function and Its Derivative (Beta={beta_20})')
plt.xlabel('x')
plt.legend()

plt.show()
# 这个代码块将绘制两个图形，左图中展示Beta=1时的Softplus函数及其导数，右图中展示Beta=20时的Softplus函数及其导数。两个图形将同时出现在同一个窗口中以方便比较。-GPT
```

![image-20230404215843356](F:\Coding\github\zhangwenniu.github.io\_posts\2023-04-04-每日学习笔记.assets\image-20230404215843356.png)

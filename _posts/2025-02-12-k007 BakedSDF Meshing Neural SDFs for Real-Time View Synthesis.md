---
layout: mypost
title: k007, G041 BakedSDF Meshing Neural SDFs for Real-Time View Synthesis
categories: [Neural Radiance Caching]
---

# 链接

- Arxiv: [BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis](https://arxiv.org/abs/2302.14859)

- 发表时间：

[v1] 2023年2月28日，18:58:03 UTC (29,486 KB)

[v2] 2023年5月16日，15:01:42 UTC (32,225 KB)

- 收录于SIGGRAPH 2023

- 网站：[https://bakedsdf.github.io/](https://bakedsdf.github.io/)  

# 文章难点解析

这篇论文《BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis》介绍了一种将神经隐式表面（Neural Signed Distance Fields，简称SDF）转换为高质量三角网格（Mesh），并用于实时视图合成（View Synthesis）的方法。论文的核心目标是将复杂的神经渲染技术与传统的实时图形渲染技术相结合，以实现高效且高质量的3D场景渲染。以下是对论文内容的简化解读：

### **1. 研究背景**
- **视图合成（View Synthesis）**：从一组已知视角的图像中重建3D场景，并从新的视角渲染出逼真的图像。传统方法（如NeRF）虽然能生成高质量的图像，但渲染速度慢，不适合实时应用。
- **实时渲染的挑战**：现有方法（如MobileNeRF）虽然实现了实时渲染，但牺牲了图像质量和几何精度，无法支持高级图形应用（如材质编辑和物理模拟）。

### **2. 研究方法**
论文提出了一种名为 **BakedSDF** 的方法，通过以下三个阶段实现高效的实时视图合成：
1. **优化神经体积-表面表示（Neural Volume-Surface Representation）**：
   - 使用一种混合的神经体积-表面表示（基于VolSDF和mip-NeRF 360），通过神经网络（MLP）学习场景的几何和外观。
   - 场景的几何被表示为一个**零水平集（Zero-Level Set）**，即SDF为零的点集，这些点定义了场景的表面。
   - 通过体积渲染（Volume Rendering）技术优化神经网络，使其能够生成逼真的图像。
2. **生成高质量三角网格（Mesh）**：
   - 从优化后的神经网络中提取三角网格，使用**Marching Cubes算法**。
   - 通过分析训练数据中的光线采样点，去除网格中不合理的表面（如被遮挡的区域）。
   - 对网格进行优化，填补小孔并优化顶点顺序以提高渲染性能。
3. **优化视图依赖的外观模型（View-Dependent Appearance Model）**：
   - 为每个网格顶点分配一个**漫反射颜色（Diffuse Color）**和一组**球高斯（Spherical Gaussians，SGs）**，用于模拟视图依赖的外观（如高光）。
   - 通过最小化像素颜色误差来优化这些参数，使得网格能够高效地渲染出高质量的图像。

### **3. 实验结果**
- **实时渲染性能**：
  - BakedSDF能够在普通硬件上以**超过70帧/秒（FPS）**的速度渲染复杂的3D场景，同时功耗仅为85瓦特，性能显著优于现有的实时渲染方法（如MobileNeRF和Deep Blending）。
  - 在图像质量方面，BakedSDF在PSNR、SSIM和LPIPS等指标上均优于现有的实时方法。
- **网格质量**：
  - 与COLMAP、MobileNeRF和Mip-NeRF 360等方法相比，BakedSDF生成的网格更加平滑、准确，适合用于高级图形应用（如材质编辑和物理模拟）。

### **4. 限制与未来工作**
- **透明物体和复杂几何的表示**：BakedSDF目前无法很好地表示半透明物体（如玻璃、雾）和复杂几何（如密集的树叶、细长结构）。
- **存储需求**：生成的网格文件较大（约430MB/场景），可能不适合存储或流式传输。
- **未来改进方向**：可以通过引入透明度、简化网格和优化存储来进一步提升性能和适用性。

### **5. 总结**
BakedSDF通过将神经隐式表面与传统网格渲染技术相结合，实现了高质量的实时视图合成。这种方法不仅在渲染速度和图像质量上优于现有方法，还生成了高质量的网格，能够支持多种高级图形应用。

# 原文翻译

## 摘要

我们提出了一种用于重建大型无界现实世界场景的高质量网格的方法，这些场景适用于逼真的新视图合成。我们首先优化一种混合神经体积-表面场景表示，该表示旨在具有表现良好的等值面，这些等值面对应于场景中的表面。然后，我们将这种表示烘焙到高质量的三角网格中，并为其配备了一个基于球高斯分布的简单快速的视图依赖外观模型。最后，我们优化这种烘焙后的表示，以最佳地重现捕获的视点，从而生成一个可以利用加速多边形光栅化管线在普通硬件上实现实时视图合成的模型。我们的方法在准确性、速度和功耗方面优于以往的实时渲染场景表示，并且生成的高质量网格能够支持诸如外观编辑和物理模拟等应用。

## 1 引言
当前在新视图合成任务中表现最佳的方法大多基于神经辐射场（NeRF）[Mildenhall 等人 2020]。NeRF通过将场景表示为由多层感知器（MLP）参数化的连续体积函数，能够生成展现出精细几何结构和视图依赖效果的逼真渲染图像。然而，由于NeRF底层的MLP计算成本高昂，且每像素需要查询数百次，因此从NeRF渲染高分辨率图像通常速度较慢。近期的研究通过用离散化的体积表示（如体素网格）替换计算密集型的MLP，提高了NeRF的渲染性能。然而，这些方法需要大量的GPU内存和定制的体积光线行进代码，并且不适合在普通硬件上实现实时渲染，因为现代图形硬件和软件主要面向多边形表面的渲染，而不是体积场。尽管当前类似NeRF的方法能够恢复具有简单几何形状的单个对象的高质量实时可渲染网格 [Boss 等人 2022]，但从现实世界的无界场景捕获（例如Barron 等人 [2022] 的“360度捕获”）中重建详细且表现良好的网格已被证明更具挑战性。最近，MobileNeRF [Chen 等人 2022a] 通过训练一个体积内容仅限于多边形网格表面的NeRF解决了这一问题，然后将该NeRF烘焙到纹理贴图中。尽管这种方法能够产生合理的图像质量，但MobileNeRF将场景几何初始化为一组轴对齐的瓦片，优化后变成一个纹理化的多边形“汤”。这种几何结构不适合纹理编辑、重新照明和物理模拟等常见图形应用。

在本文中，我们展示了如何从类似NeRF的神经体积表示中提取高质量网格。我们的系统，我们称之为BakedSDF，扩展了VolSDF [Yariv 等人 2021] 的混合体积-表面神经表示，以表示无界的现实世界场景。这种表示旨在具有对应于场景中表面的良好行为的零等值面，这使得我们能够使用Marching Cubes算法提取高分辨率的三角网格。我们为这个网格配备了一个基于球高斯分布的快速高效的视图依赖外观模型，并对其进行微调以重现场景的输入图像。我们的系统输出可以在普通设备上以实时帧率进行渲染，我们展示了我们的实时渲染系统在逼真度、速度和功耗方面优于以往的工作。此外，我们还展示了（与类似以往的工作不同）我们模型生成的网格是准确且详细的，这使得标准图形应用（如外观编辑和物理模拟）成为可能。


## 2 相关工作

视图合成，即根据一组捕获的图像渲染场景的新视图的任务，在计算机视觉和图形学领域是一个长期存在的问题。在观测视点采样密集的场景中，可以通过光场渲染来合成新视图——直接在观测光线集合中进行插值 [Gortler等人 1996; Levoy和Hanrahan 1996]。然而，在实际设置中，观测视点的采样较为稀疏，重建场景的三维表示对于渲染令人信服的新视图至关重要。大多数经典的视图合成方法使用三角网格（通常通过多视图立体视觉 [Furukawa和Hernández 2015; Schönberger等人 2016]、泊松曲面重建 [Kazhdan等人 2006; Kazhdan和Hoppe 2013] 和Marching Cubes [Lorensen和Cline 1987] 的流程重建）作为底层三维场景表示，并通过将观测图像重新投影到每个新视点并使用启发式定义 [Buehler等人 2001; Debevec等人 1996; Wood等人 2000] 或学习到的 [Hedman等人 2018; Riegler和Koltun 2020, 2021] 融合权重将它们融合在一起，来渲染新视图。尽管基于网格的表示非常适合使用加速图形管线进行实时渲染，但这些方法生成的网格在细节丰富或材质复杂的区域往往几何形状不准确，从而导致渲染的新视图出现错误。

作为替代方案，基于点的表示 [Kopanas等人 2021; Rückert等人 2022] 更适合建模薄几何形状，但在相机移动时，如果没有可见的裂缝或不稳定的渲染结果，就无法高效地渲染。大多数最近的视图合成方法通过使用几何和外观的体积表示来绕开高质量网格重建的困难，例如体素网格 [Lombardi等人 2019; Penner和Zhang 2017; Szeliski和Golland 1999; Vogiatzis等人 2007] 或多平面图像 [Srinivasan等人 2019; Zhou等人 2018]。这些表示非常适合基于梯度的渲染损失优化，因此可以有效地优化以重建输入图像中看到的详细几何形状。在这些体积方法中最成功的是神经辐射场（NeRF）[Mildenhall等人 2020]，它是许多最先进的视图合成方法的基础（参见Tewari等人 [2022] 的综述）。NeRF将场景表示为一个连续的体积场，该场发射和吸收光线，并使用体积光线追踪来渲染图像。NeRF使用多层感知器（MLP）来参数化从空间坐标到体积密度和发射辐射度的映射，并且必须在沿光线采样的一组坐标上评估该MLP，以得出最终颜色。

后续的研究提出了修改NeRF的场景几何和外观表示以提高质量和可编辑性。Ref-NeRF [Verbin等人 2022] 重新参数化了NeRF的视图依赖外观，以启用外观编辑并改善镜面材质的重建和渲染。其他工作 [Boss等人 2021; Kuang等人 2022; Srinivasan等人 2021; Zhang等人 2021a,b] 尝试将场景的视图依赖外观分解为材质和光照属性。除了修改NeRF的外观表示外，包括UNISURF [Oechsle等人 2021]、VolSDF [Yariv等人 2021] 和NeuS [Wang等人 2021] 在内的论文通过混合体积-表面模型增强了NeRF的全体积几何表示。

NeRF用于表示场景的MLP通常较大且评估成本高昂，这意味着NeRF训练缓慢（每个场景需要数小时或数天）且渲染缓慢（每百万像素需要数秒或数分钟）。最近的方法提出通过用体素网格 [Karnewar等人 2022; Sun等人 2022]、小MLP网格 [Reiser等人 2021]、低秩 [Chen等人 2022b] 或稀疏 [Yu等人 2022] 网格表示，或配备小MLP的多尺度哈希编码 [Müller等人 2022] 替换那个单一的大MLP，以减少计算量为代价增加存储量。尽管这些表示减少了训练和渲染所需的计算量（以增加存储为代价），但可以通过预计算并将训练好的NeRF存储为更高效的表示形式，即“烘焙”，来进一步加速渲染。SNeRG [Hedman等人 2021]、FastNeRF [Garbin等人 2021]、Plenoctrees [Yu等人 2021] 和可扩展的神经室内场景渲染 [Wu等人 2022] 都将训练好的NeRF烘焙到稀疏体积结构中，并使用简化的视图依赖外观模型以避免在每条光线的每个样本上评估MLP。这些方法已经在高端硬件上实现了NeRF的实时渲染，但由于它们使用体积光线行进，因此无法在普通硬件上实现实时性能。

## 3 预备知识

在本节中，我们描述了NeRF [Mildenhall等人 2020] 用于视图合成的神经体积表示，以及mip-NeRF 360 [Barron等人 2022] 为表示无界“360度”场景而引入的改进。NeRF是一种三维场景表示，由一个学习函数组成，该函数将位置 $x$ 和射出光线方向 $d$ 映射到体积密度 $\tau$ 和颜色 $c$。

为了渲染目标相机视图中单个像素的颜色，我们首先计算该像素对应的光线 $r = o + td$，然后在光线上的多个点 $\{t_i\}$ 处评估NeRF。每个点的输出 $\tau_i$ 和 $c_i$ 被组合成一个单一的输出颜色值 $C$：

$$
C = \sum_{i} \exp\left( -\sum_{j<i} \tau_j \delta_j \right) \left(1 - \exp(-\tau_i \delta_i)\right) c_i, \quad \delta_i = t_i - t_{i-1}. \quad (1)
$$

这个 $C$ 的定义是基于数值积分的体积渲染方程的近似 [Max 1995]。NeRF使用一个多层感知器（MLP）来参数化这个学习函数，其权重经过优化以隐式编码场景的几何形状和颜色信息：一组训练输入图像及其相机姿态被转换为一组（光线，颜色）对，然后使用梯度下降优化MLP权重，使得每条光线的渲染结果与其对应的输入颜色相似。形式上，NeRF最小化了真实颜色 $C_{gt}$ 与公式1中产生的颜色 $C$ 之间的损失，该损失在所有训练光线上的平均值为：

$$
L_{\text{data}} = \mathbb{E} \left[ \|C - C_{gt}\|^2 \right]. \quad (2)
$$

如果输入图像提供了足够的场景覆盖（就多视图三维约束而言），这个简单的过程将产生一组准确描述场景三维体积密度和外观的MLP权重。


### Mip-NeRF 360的改进

Mip-NeRF 360 [Barron等人 2022] 扩展了基本的NeRF公式，以重建和渲染现实世界中的“360度”场景，其中相机可以在所有方向上观察无界的场景内容。Mip-NeRF 360引入的两个改进是收缩函数和提议MLP的使用。

### 收缩函数

收缩函数将无界场景点从 $\mathbb{R}^3$ 映射到有界域：

$$
\text{contract}(x) = 
\begin{cases}
x, & \|x\| \leq 1 \\
2 - \frac{1}{\|x\|} \frac{x}{\|x\|}, & \|x\| > 1
\end{cases}. \quad (3)
$$

这种收缩后的坐标非常适合用作MLP输入的位置编码。

### 提议MLP

此外，Mip-NeRF 360表明，具有详细几何形状的大型无界场景需要在原始NeRF框架中使用过大的MLP，并且每条光线需要的样本数量过多，难以实现。因此，Mip-NeRF 360引入了一个提议MLP：一个更小的MLP，它被训练用来限制实际NeRF MLP的密度。这个提议MLP用于分层采样过程，高效地生成一组输入样本，这些样本紧密聚焦于场景中的非空内容，从而为NeRF MLP提供输入。

## 4 方法

我们的方法由三个阶段组成，如图 2 所示。首先，我们使用类似 NeRF 的体积渲染技术优化场景的几何和外观的表面表示。然后，我们将该几何“烘焙”成网格，并证明其足够精确，能够支持逼真的外观编辑和物理模拟。最后，我们训练一个新的外观模型，该模型使用嵌入网格每个顶点的球面高斯函数（Spherical Gaussians, SGs），以取代第一步中计算成本较高的类似 NeRF 的外观模型。通过这种方法生成的 3D 表示可以在普通设备上实时渲染，因为渲染只需对网格进行光栅化并查询少量球面高斯函数。

### 4.1 使用SDF建模密度  

我们的表示结合了mip-NeRF 360在表示无界场景方面的优势以及VolSDF混合体积-表面表示的优良表面属性[Yariv et al. 2021]。VolSDF将场景的体积密度建模为一个由MLP参数化的有符号距离函数(SDF) $f$ 的参数化函数，该函数返回每个点$x \in R^3$到表面的有符号距离$f(x)$。由于我们的重点是重建无界的现实世界场景，我们选择在收缩空间中（方程3）而不是世界空间中参数化$f$。场景的底层表面是$f$的零水平集，即距离表面零距离的点的集合：  

$$
S = \{x : f(x) = 0\} . \quad (4)
$$

遵循VolSDF，我们将体积密度$\tau$定义为：  

$$
\tau(x) = \alpha \Psi_\beta(f(x)) , \quad (5)
$$

其中$\Psi_\beta$是尺度参数为$\beta > 0$的零均值拉普拉斯分布的累积分布函数。注意，当$\beta$接近0时，体积密度接近于在一个对象内部返回$\alpha$而在自由空间中返回0的函数。为了鼓励$f$近似一个有效的有符号距离函数（即对于所有的$x$，$f(x)$返回到$f$的水平集的带符号欧几里得距离），我们惩罚$f$不满足Eikonal方程的偏差[Gropp et al. 2020]：  

$$
L_{SDF} = E_x(\|\nabla f(x)\| - 1)^2 . \quad (6)
$$

请注意，由于$f$是在收缩空间中定义的，因此这个约束也在收缩空间中操作。

最近，Ref-NeRF[Verbin et al. 2022]通过将外观参数化为关于相对于表面法线反射的视角方向的函数来改进视角依赖的外观。我们使用SDF参数化的密度允许轻松采用这种方法，因为SDF具有明确的表面法线：$n(x) = \nabla f(x)/\|\nabla f(x)\|$。因此，在训练模型的这个阶段时，我们采用了Ref-NeRF的外观模型，并通过单独的漫反射和镜面反射组件计算颜色，其中镜面反射组件由关于法线方向反射的视角方向、法线和视角方向之间的点积以及由参数化$f$的MLP输出的256元素瓶颈向量所参数化。我们使用mip-NeRF 360的一个变体作为我们的模型（参见补充材料附录A中的具体训练细节）。类似于VolSDF[Yariv et al. 2021]，我们在方程5中将密度比例因子$\alpha$参数化为$\alpha = \beta^{-1}$。然而，我们发现调度$\beta$而非将其留作可优化参数会导致更稳定的训练。因此，我们根据以下公式对$\beta$进行退火处理：  

$$
\beta_t = \frac{\beta_0}{1 + (\beta_0-\beta_1)t^{0.8}} - \beta_1
$$

其中$t$从0到1变化，$\beta_0 = 0.1$，且对于三个层次采样阶段，$\beta_1$分别为0.015, 0.003, 和0.001。由于用于密度SDF参数化的Eikonal正则化已去除漂浮物并产生良好的法线，我们认为没有必要使用来自Ref-NeRF的方向损失或预测法线，也不需要mip-NeRF 360中的失真损失。


最近，Ref-NeRF[Verbin等人，2022]通过将视角依赖的外观参数化为关于相对于表面法线反射的视角方向的函数来改进它。我们使用的SDF参数化的密度允许轻松采用这种方法，因为SDF具有明确的表面法线：$n(x) = \nabla f(x)/\|\nabla f(x)\|$。因此，在训练模型的这个阶段时，我们采用了Ref-NeRF的外观模型，并通过单独的漫反射和镜面反射组件计算颜色，其中镜面反射组件由关于法线方向反射的视角方向、法线与视角方向之间的点积以及由参数化$f$的MLP输出的256元素瓶颈向量所构成。

我们使用mip-NeRF 360的一个变体作为我们的模型（参见补充材料附录A中的具体训练细节）。类似于VolSDF[Yariv等人，2021]，我们在方程5中将密度比例因子$\alpha$参数化为$\alpha = \beta^{-1}$。然而，我们发现调度$\beta$而非将其留作自由可优化参数会导致更稳定的训练。因此，我们根据以下公式对$\beta$进行退火处理：

$$
\beta_t = \beta_0\left(1 + \frac{\beta_0 - \beta_1}{\beta_1}t^{0.8}\right)^{-1}
$$

其中$t$从0到1变化，$\beta_0 = 0.1$，且对于三个层次采样阶段，$\beta_1$分别为0.015, 0.003, 和0.001。由于用于密度SDF参数化的Eikonal正则化已去除漂浮物并产生良好的法线，我们认为没有必要使用来自Ref-NeRF的方向损失或预测法线，也不需要mip-NeRF 360中的失真损失。


以下是翻译后的中文内容，并按照您的要求使用Markdown格式，数学公式用$（行内）和$$（行间）括起来：

### 4.2 烘焙高分辨率网格

在优化我们的神经体积表示之后，我们通过在一个规则的3D网格上查询恢复的MLP参数化的SDF，然后运行Marching Cubes[Lorensen和Cline 1987]来创建三角形网格。请注意，VolSDF使用一个扩展超出SDF零交叉点（由$\beta$参数化）的密度衰减来建模边界（Note that VolSDF models boundaries using a density fall-off that extends beyond the SDF zero crossing (parameterized by β).）。我们在提取网格时考虑到了这种扩散效应，并选择0.001作为表面交叉的等值面值，否则我们会发现场景几何体略有侵蚀。

当运行Marching Cubes时，MLP参数化的SDF可能在从观察视角遮挡的区域以及提案MLP标记为“自由空间”的区域内包含虚假的表面交叉点。这两种类型区域中的SDF MLP值在训练期间未受到监督，因此我们必须剔除任何会作为虚假内容出现在重建网格上的表面交叉点。为此，我们检查了训练数据中沿光线获取的3D样本。我们计算每个样本的体积渲染权重，即它对训练像素颜色的贡献程度。然后我们将任何具有足够大渲染权重（> 0.005）的样本映射到3D网格中，并将相应的单元格标记为表面提取的候选者。

我们在收缩空间中以均匀间隔的坐标采样我们的SDF网格，在世界空间中这会产生不均匀间隔的非轴对齐坐标。这有一个理想的效果，即对于靠近原点的前景内容创建较小的三角形（在世界空间中），而对于远处的内容则创建较大的三角形。实际上，我们将收缩算子作为一种细节层次策略：因为我们想要渲染的视图接近场景原点，并且因为收缩的形状设计用于撤销透视投影的影响，所有三角形在投影到图像平面时都将具有大约相等的面积。  

提取三角形网格后，我们使用区域增长程序来填充可能存在小孔的区域，这些区域要么是输入视角未能观察到的，要么是在烘焙过程中被提案MLP遗漏的。我们迭代地标记当前网格周围邻域内的体素，并提取这些新激活体素中存在的任何表面交叉点。这有效地解决了SDF MLP中存在表面但由于训练视角覆盖不足或提案MLP中的错误而未被marching cubes提取的情况。然后，我们将网格转换到世界空间，以便它可以由操作于欧几里得空间的传统渲染引擎进行光栅化处理。最后，我们使用顶点顺序优化[Sander等人，2007]对网格进行后处理，通过允许在相邻三角形之间缓存和重用顶点着色器输出，加快现代硬件上的渲染性能。在附录B中，我们详细说明了额外的网格提取步骤，虽然它们并不严格改善重建精度，但能够提供更令人愉悦的交互式观看体验。

以下是翻译后的中文内容，并按照您的要求使用Markdown格式，数学公式用$（行内）和$$（行间）括起来：

### 4.3 建模视角依赖的外观  

上述烘焙过程从基于MLP的场景表示中提取高质量的三角形网格几何。为了建模场景的外观，包括视角依赖效应如镜面反射，我们为每个网格顶点配备了一个漫反射颜色$c_d$和一组球形高斯波瓣。由于远处区域仅能从有限的视角方向观察到，我们不需要在场景的所有地方以相同的精度来建模视角依赖性。在我们的实验中，在中心区域（$\|x\| \leq 1$）使用了三个球形高斯波瓣，在外围使用一个波瓣。图3展示了我们的外观分解。

这种外观表示满足了我们对计算和内存效率的目标，因此可以实时渲染。每个球形高斯波瓣有七个参数：用于波瓣平均值的三维单位向量$\mu$，用于波瓣颜色的三维向量$c$，以及用于波瓣宽度的标量$\lambda$。这些波瓣由视角方向向量$d$参数化，因此与任何给定顶点相交的射线的渲染颜色$C$可以计算为：

$$C = c_d + \sum_{i=1}^{N} c_i \exp(\lambda_i (\mu_i \cdot d - 1)) .\quad (7)$$

为了优化这种表示，我们首先将网格光栅化到所有训练视图中，并存储与每个像素关联的顶点索引和重心坐标。经过这种预处理后，我们可以通过对学习到的每个顶点参数应用重心插值然后运行我们的视角依赖外观模型（模拟片段着色器的操作）轻松地渲染像素。因此，通过最小化每像素颜色损失（如方程2），我们可以优化每个顶点的参数。正如附录B中详述的那样，我们还优化了背景清除颜色，以提供更佳的交互式查看体验。为了避免优化被那些不能很好地被网格几何建模的像素所偏向（例如，软对象边界和半透明对象处的像素），代替VolSDF使用的L2损失，我们在训练期间使用带有超参数$\alpha = 0, c = 1/5$的鲁棒损失$\rho (·, \alpha, c)$，这使得优化对外点更加稳健[Barron 2019]。我们也使用直通估计器[Bengio等人，2013]建模量化，确保优化后的视角依赖外观值能够以8位精度良好表示。

我们发现直接优化这种每个顶点的表示会耗尽GPU内存，从而阻止我们将规模扩大到高分辨率网格。相反，我们优化了基于Instant NGP[Müller等人，2022]的压缩哈希网格表示（参见补充材料中的附录A）。在优化过程中，我们在训练批次内的每个3D顶点位置查询此表示以生成漫反射颜色和球形高斯参数。

优化完成后，我们通过在每个顶点位置查询NGP模型以获取与外观相关的参数，来烘焙出包含在哈希网格中的压缩场景表示。最后，我们使用gLTF格式[ISO/IEC 12113:2022 2022]导出结果网格和每个顶点的外观参数，并使用gzip进行压缩，这是一种网络协议原生支持的格式。



## 5 实验  

我们从输出渲染的准确性以及其速度、能量和内存需求方面评估我们方法的表现。对于准确性，我们测试了模型的两个版本：第4.1节中描述的中间体积渲染结果，我们称之为“离线”模型；以及第4.2和4.3节中描述的烘焙实时模型，我们称之为“实时”模型。作为基线，我们使用了以前为保真度设计的离线模型[Barron等人，2022; Mildenhall等人，2020; Müller等人，2022; Riegler和Koltun，2021; Zhang等人，2020]，以及为性能设计的先前实时方法[Chen等人，2022a; Hedman等人，2018]。此外，我们还将我们方法恢复的网格与通过COLMAP[Schönberger等人，2016]、mip-NeRF 360[Barron等人，2022]和MobileNeRF[Chen等人，2022a]提取的网格进行比较。所有FPS（每秒帧数）测量均针对1920 × 1080分辨率的渲染。



### 5.1 无界场景的实时渲染  

我们在mip-NeRF 360[Barron等人，2022]提供的现实世界场景数据集上评估了我们的方法，该数据集包含了从所有视角捕捉到的复杂室内和室外场景。在表1中，我们展示了我们的离线模型和实时模型版本与基线方法的定量比较结果。虽然我们的离线模型在某些先前的工作面前表现稍逊一筹（鉴于我们的重点在于性能，这是可以预料的），但我们的实时方法在本基准使用的全部三个误差指标上都优于所评估的两个最新的实时状态最优基线。在图4中，我们展示了来自我们模型和这两个最先进的实时基线的渲染效果对比，观察到我们的方法展现出更多的细节和更少的伪影。

在表2中，我们将我们的方法的渲染性能与Instant-NGP（我们评估的最快的“离线”模型）和MobileNeRF（产生最高质量渲染的实时模型，在我们之后）进行了比较。所有方法的性能都在1920 × 1080分辨率下测量。MobileNeRF和我们的方法均在配备Radeon 5500M GPU的16英寸Macbook Pro上进行浏览器内运行，而Instant NGP则在装备有高性能NVIDIA RTX 3090 GPU的工作站上运行。尽管我们的方法相比MobileNeRF（1.27倍）和Instant NGP（4.07倍）需要更多的磁盘存储空间，但我们发现我们的模型显著更高效——我们的模型提供了分别是1.44倍和77倍更高的FPS/Watt指标，同时生成了更高品质的渲染。

相对于MobileNeRF，我们的性能显著提升乍一看可能不寻常，因为我们的方法和MobileNeRF都能产生可快速光栅化的优化网格。这种差异可能是由于MobileNeRF依赖于alpha遮罩（这导致了大量的计算密集型重绘）以及MobileNeRF使用MLP来建模视角依赖的辐射度（其评估所需计算量远超我们的球形高斯方法）。

与Deep Blending[Hedman等人，2018]相比，从表1可以看出我们的方法实现了更高的质量。然而，也值得注意的是，我们的表示方式更为简单：虽然我们的网格可以在浏览器中渲染，但Deep Blending依赖于精心调校的CUDA渲染，并且必须存储场景中所有训练图像的颜色和几何信息。因此，对于室外场景，Deep Blending的总存储成本比相应的我们的网格高出2.66倍（平均为1154.78 MB）。 

### 5.2 网格提取

在图5中，我们展示了我们生成的网格与使用COLMAP [Schönberger等，2016]、MobileNeRF [Chen等，2022a]以及Mip-NeRF 360 [Barron等，2022]的等值面提取结果的定性比较。我们选择与COLMAP进行对比，不仅因为其代表了一种成熟的运动恢复结构（Structure-from-Motion）软件包，还因为COLMAP生成的几何结构被用作Stable View Synthesis和Deep Blending的输入。COLMAP通过对场景进行四面体化，并在其上应用体积分割 [Jancosek和Pajdla，2011；Labatut等，2007] 来获得场景的二值分割，然后将这些区域之间的表面形成三角网格。由于这种二值分割不允许对表面进行任何平均处理，初始重建中的小噪声往往会导致重建网格的噪声，从而呈现出“凹凸不平”的外观。

MobileNeRF将场景表示为一组不相连的三角形集合，因为其唯一目标是视图合成。因此，其优化和剪枝后的“三角形汤”高度嘈杂，无法用于外观编辑等下游任务。正如最近的研究所展示的 [Oechsle等，2021；Wang等，2021；Yariv等，2021]，直接从NeRF预测的密度场中提取等值面有时无法忠实地捕捉场景的几何结构。在图5中，我们使用Mip-NeRF 360展示了这种效果，并提取了其密度场超过50的等值面。请注意，由于花瓶的反射是通过镜像世界几何建模的，桌子的表面不再平整。相比之下，我们的方法生成了一个平滑且高保真的网格，更适合用于外观和照明编辑，如图1所示。

### 5.3 外观模型消融研究

在表3中，我们展示了关于我们的球高斯（Spherical Gaussian，SG）外观模型的消融研究结果。我们发现，将SG的数量减少到2、1和0（即漫反射模型）会导致精度单调下降。然而，当在周围区域使用3个SG时，我们的模型倾向于过拟合训练视图，与我们提出的仅使用单个周围SG的模型相比，质量略有下降。此外，与在所有位置使用3个SG相比，使用单个周围SG可以将顶点的平均大小减少1.52倍（从36字节减少到23.76字节），这显著降低了内存带宽的消耗（渲染的主要性能瓶颈）。

或许令人惊讶的是，用SNeRG [Hedman等，2021]和MobileNeRF [Chen等，2022a]中使用的较小视图依赖的多层感知机（MLP）替换我们的SG外观模型，会显著降低渲染质量，并产生与“1个球高斯”消融实验大致相当的误差指标。鉴于评估一个小MLP（每像素约2070次浮点运算）的成本显著高于评估单个球高斯（每像素21次浮点运算），这一结果尤其令人意外。

此外，我们还对训练外观表示所使用的稳健损失函数进行了消融实验，将其替换为简单的L2损失。结果不出所料，这会提升PSNR（与MSE成反比），但以牺牲其他指标为代价。

### 5.4 局限性

尽管我们的模型在实时渲染无界场景这一既定任务上实现了最先进的速度和精度，但仍存在一些局限性，这些也为未来的改进提供了机会：

我们使用完全不透明的网格表示来建模场景，因此我们的模型可能难以表示半透明内容（如玻璃、雾等）。此外，与基于网格的方法常见的情况一样，我们的模型有时无法准确表示具有小或详细几何结构的区域（如密集的树叶、细薄结构等）。这些问题或许可以通过为网格添加透明度值来解决，但允许连续透明度需要复杂的多边形排序过程，这很难集成到实时光栅化管线中。

我们的技术还存在另一个局限性：我们的模型输出的网格占用大量的磁盘空间（每场景约430兆字节），这在某些应用中可能会给存储或流式传输带来挑战。这一问题可以通过网格简化后进行UV纹理映射来缓解。然而，我们发现现有的简化和纹理映射工具（大多为艺术家制作的3D资产设计）并不适用于我们通过Marching Cubes提取的网格。

## 6 结论

我们提出了一个用于实时渲染大型无界现实世界场景的高质量网格生成系统。我们的技术首先优化了一个混合的神经体素-表面表示，该表示专门用于精确的表面重建。从这种混合表示中，我们提取了一个三角网格，其顶点包含高效的视图依赖外观表示，随后进一步优化该网格表示，以最佳地复现捕获的输入图像。

这使得生成的网格在实时视图合成的速度和精度方面均达到了最先进的水平，并且质量足够高，能够支持下游应用的开展。


## 附录

## A 训练和优化细节

### SDF模型定义与优化

如第4.1节所述，我们使用mip-NeRF 360的一个变体来建模我们的SDF。我们使用与mip-NeRF 360相同的优化设置来训练我们的模型（使用Adam [Kingma和Ba 2015]进行250k次迭代，批量大小为214，学习率从2×10⁻³热启动，然后对数线性插值到2×10⁻⁵，β₁=0.9，β₂=0.999，ε=10⁻⁶），以及类似的MLP架构（一个有4层和256个隐藏单元的提议MLP，以及一个有8层和1024个隐藏单元的NeRF MLP，两者都使用swish/SiLU整流器 [Hendrycks和Gimpel 2016]和8级位置编码）。按照mip-NeRF 360的分层采样过程，我们使用提议MLP评估的64个样本来执行两个重采样阶段，然后使用NeRF MLP的32个样本来执行一个评估阶段。提议MLP通过最小化Lprop + 0.1LSDF来优化，其中Lprop是[Barron等人 2022]中描述的提议损失，旨在限制NeRF MLP密度输出的权重。

### 通过压缩哈希网格优化每个顶点的属性

如第4.3节所述，在优化过程中，我们使用Instant NGP [Müller等人 2022]作为我们顶点属性的底层表示。我们使用以下超参数：L = 18，T = 2²¹，Nmax = 8192。我们从NGP模型中移除了视图方向输入，因为我们在后面的公式7中将其包含进去。我们对哈希网格使用0.1的权重衰减，但不对MLP使用，使用Adam [Kingma和Ba 2015]进行150k次迭代优化，批量大小为2¹⁴，初始学习率为0.001，每50k次迭代降低10倍。


## B 为吸引人的观看体验而进行的调整

在这里，我们详细说明了一些对流程的调整，这些调整并不严格提高重建精度，而是使观看体验更加吸引人。鉴于此，我们发现减轻重建场景内容与背景颜色之间突兀过渡是很重要的。为此，我们在第4.3节中优化的外观参数中也包括了一个全局的背景颜色。也就是说，我们将这个颜色分配给训练数据中没有有效三角形索引的任何像素。为了进一步掩盖几何形状与背景之间的过渡，我们在第4.2节中提取网格之前，用边界几何形状将SDF封闭起来。我们计算了一个凸包，该凸包是32个随机定向平面的交集，其中每个平面的位置都设置为包围99.75%被标记为表面提取候选的体素。然后，我们通过将这个包膨胀一个轻微的范围（×1.025）来使其更加保守。然而，由于提取的网格需要被转换到世界空间进行渲染，我们必须小心避免在光栅化过程中使用无界顶点坐标时可能出现的数值精度问题。我们通过用一个半径为500个世界空间单位的远距离球体来包围场景来解决这个问题。这两个操作很容易通过将每个网格单元的SDF值设置为MLP参数化的SDF和定义的边界几何形状的SDF的逐点最小值来实现。

## C 基线细节

### MobileNeRF查看器配置
请注意，默认情况下，MobileNeRF查看器以降低的分辨率运行，以便在各种设备上实现高帧率。为了进行比较，我们修改它以在不同的分辨率下运行。当我们计算图像质量指标时，我们选择测试集图像的分辨率。此外，当我们测量运行时性能时，我们使用1920×1080的分辨率，这代表了大多数现代显示器的分辨率。

### Instant NGP
表1报告了Instant NGP [Müller等人 2022]方法的质量结果，我们仔细调整它以适用于无界的大场景。我们向Instant NGP的作者寻求帮助以调整他们的方法，并做出了以下更改：
- 使用官方代码发布的big.json配置文件，
- 将批量大小增加4倍至2²⁰，
- 将场景比例从16增加到32。
请注意，这些更改对Instant NGP的渲染时间没有显著影响。默认情况下，Instant NGP查看器配备了动态放大实现，它以较低的分辨率渲染图像，然后应用智能放大。为了公平比较，我们在测量性能时将其关闭，因为这些动态放大器可以应用于任何渲染器。更重要的是，我们希望性能数字与测试集质量指标相对应，而测试集图像中没有任何一张是使用放大计算的。

---
layout: mypost
title: G039-Brief analysis of DeepSeek R1 and its implications for  Generative AI
categories: [DeepSeek]
---

# 链接

- Arxiv: [DeepSeek R1及其对生成式AI影响的简要分析](https://arxiv.org/abs/2502.02523)

- 发布时间：

[v1] 2025年2月4日 17:45:32 UTC (438 KB)

[v2] 2025年2月5日 10:47:30 UTC (438 KB)

[v3] 2025年2月7日 15:02:21 UTC (438 KB)


# 原文翻译

DeepSeek R1及其对生成式AI影响的简要分析

Sarah Mercer*1, Samuel Spillard1, 和 Daniel P. Martin1
1艾伦图灵研究所

## 摘要

2025年1月底，DeepSeek发布了他们的新推理模型（DeepSeek R1）；尽管受到美国GPU出口禁令的影响，该模型以较低的成本开发，却仍能与OpenAI的模型保持竞争力。本报告讨论了该模型及其发布对生成式AI领域更广泛的意义。

我们简要讨论了近几周从中国发布的其他模型及其相似之处；专家混合系统（MoE）、强化学习（RL）的创新使用以及巧妙的工程设计似乎是这些模型能力的关键因素。

这篇思考文章是在紧迫的时间内完成的，提供了该主题的广泛覆盖，可作为那些希望了解模型技术进步及其在生态系统中地位的人的入门材料。文章还确定了几个进一步研究的领域。




## 1 引言

生成式AI相对较短的历史中不断出现模型能力的重大进步。在过去几周内，随着中国公司DeepSeek发布的几篇论文[1]，这种情况再次发生。

2023年12月底，他们发布了DeepSeek-V3[2]，这是OpenAI的GPT4o的直接竞争对手，据报道仅用两个月时间训练完成，成本约为560万美元[3, 4]，相当于其他同类模型成本的1/50[5]。1月20日，他们发布了DeepSeek-R1[6]系列推理模型，包含"众多强大且引人入胜的推理行为"[6]，达到了与OpenAI的o1模型相当的性能水平——而且这些模型对研究人员开放研究[7]。

这种开放性对许多渴望更多了解所使用模型的AI研究人员来说是一个可喜的举措。需要注意的是，这些模型是以"开放权重"的形式发布的，这意味着模型可以被构建和自由使用（在MIT许可证下），但由于没有训练数据，它并不是真正的开源。不过，在相关文档中分享了比往常更多的关于训练过程的细节。





## 2 DeepSeek

在本节中，我们简要概述DeepSeek的最新模型。我们首先讨论DeepSeek V3，这是OpenAI的GPT4o模型的竞争对手，也是DeepSeek R1开发的基础模型。更多详细信息，请参见DeepSeek-V3[2]和DeepSeek-R1[6]的原始论文。

### 2.1 DeepSeek V3 - 基础模型

DeepSeek-V3模型采用了两个主要的效率提升方案：专家混合（MoE）架构和大量的工程效率优化。

MoE架构从高层次来看，本质上是将模型分成一系列专门的小型模型（一个用于数学，一个用于编程等）以减轻训练负担。这种架构在2020年就被用于Google的GShard等机器翻译Transformer中，并在2024年1月被用于Mixtral LLM[8]。DeepSeek也在2024年1月发表了一篇关于他们MoE方法的论文[9]。

2024年期间出现了大量的MoE相关论文，其中几种被下一节中模型使用的MoE技术在2024年底的NeurIPs会议上进行了展示。这表明，至少从架构上来说，DeepSeek V3并不是一个突如其来的突破（事后诸葛亮！）。


### 2.2 DeepSeek R1 - 推理

该项目的目标是使用纯强化学习（RL）来提升推理能力，无需监督数据，专注于自我进化。以他们的V3模型（6710亿参数）为基础，采用可扩展的群组相对策略优化（GRPO）作为RL框架，产生的R1Zero模型在推理和数学方面显示出改进，但也面临可读性差和语言混杂等挑战。

值得注意的是，R1-Zero模型在AIME 2024上的性能从15.6%提升到71.0%，与openAI-o1-0912相当，当DeepSeek团队调整RL（多数投票）后，更是达到了86.7%。

他们继续改进他们的流程，重新引入一些监督微调，最终产生了R1模型，据报道在许多推理和基于数学的评估任务上达到了与OpenAI的o1模型相当的分数。

RL过程鼓励模型生成更多标记（更多"思考时间"）来解决推理任务。随着过程的进行和测试时间计算的增加，反思和探索替代方法等行为自发产生，"顿悟时刻"[6]这个术语被用来描述中间模型学会使用拟人化语气重新思考的时刻。这种自我反思的涌现特性是一个需要进一步研究和评估的关键发现；模型是否通过自我反思"学习"如何更好地回答，就像它在GPT早期"学习"写作散文一样；如果是这样，这些内部"功能"是否能实现更好的泛化？

R1论文的另一个观察是，当他们引入RL提示来鼓励语言一致性时，模型的性能下降，在基准测试性能与可用性和可读性之间进行权衡；最终R1模型在AIME 2024上的性能为79.8%。这引发了一个问题：如果允许模型用任何语言（包括代码）"思考"，而不考虑其思维链（CoT）产物的可读性，然后在向用户呈现输出之前进行翻译，这是否会在不影响可用性的情况下提高性能？相反，能够查看和质询模型的CoT产物，不仅建立了用户信心，还有助于可解释性。

论文还介绍了如何将大型模型的推理模式"蒸馏"到小型模型中（通过监督微调数据集），这些蒸馏版本的表现比对模型进行相同的RL更好。希望这种蒸馏可以继续发展，产生更小但仍然高性能的模型。蒸馏模型相比其原始基准测试的性能有所提高，R1-Distill-Qwen-32B和R1-Distill-Llama-70B在涉及编码和数学推理的任务上超越了OpenAI的o1-mini。同样，未来的研究可以致力于确定这种蒸馏对模型整体态度（价值观和个性）的影响。

### 2.3 复现

1月25日，香港科技大学的研究人员发布了一篇论文[10, 11]，描述了如何仅使用8000个MATH示例，就能在7B模型上实现长思维链（CoT）和自我反思的涌现，并且"在复杂数学推理上取得了令人惊讶的强大结果"。

他们的目标是复现R1-zero模型；他们从Qwen2.5-Math-7B（基础模型）开始，仅使用8000个MATH示例直接进行强化学习（无SFT，无奖励模型）。他们观察到了思维链长度的增加和自我反思的涌现。最终模型在AIME上达到33.3%，在MATH基准测试上达到77.2%（基础模型分别为16.7%和52.4%）；与rStar-MATH[12]相当。他们指出，rStar-MATH使用了50多倍的数据，并需要更复杂的组件。

在采用的方法上有一些显著差异，例如，该项目使用近端策略优化（PPO）而不是GRPO进行强化学习，尽管两者都被认为相对简单，且不需要奖励模型等，但可能更重要的是，他们没有从大型模型开始，而是试图使用较小的7B参数Qwen模型并且没有大规模RL设置来复现这种方法。

HuggingFace正在复现R1[13]，这将完全开源，包括完整的数据和训练流程。他们旨在复现整个流程，包括实现缺失的组件。他们计划通过从DeepSeek-R1中提取高质量推理语料库来复现R1-distil模型，复现用于创建R1-Zero模型的纯强化学习流程，并展示通过多阶段训练（类似于R1的方式）从基础模型过渡到RL调优模型的能力。


## 3 相关值得注意的工作

近几周来自中国的创新不仅限于此。1月22日，字节跳动（截至撰写时为TikTok的母公司）发布了他们的Doubao-1.5-pro模型[14]，其性能超过GPT 4o，成本降低50倍[15]。它同样使用MoE和高度优化的架构，在性能和计算需求之间取得平衡。Doubao是中国最受欢迎的AI聊天机器人之一，拥有6000万活跃用户[16]。该公司专注于构建平衡智能和沟通的AI模型，寻求更具情感意识、更自然的交互。Duobao可能整合了改进的提示优化技术[17]和通过局部敏感哈希实现的通信高效MoE训练[18]。后者旨在解决稀疏门控MoE模型训练中固有的延迟挑战，推理速度提高2.2倍。

1月15日，科大讯飞在完全国产计算平台上训练并推出了自己的深度推理大模型——讯飞星火深度推理X1。它在问题解决过程中展现出类似于"慢思考"的特征，同时以相对较低的计算力达到"行业领先"的结果。它在中文数学能力方面特别强，已经成功应用于教育领域，作为智能教学助手[19]。

1月20日，中国研究公司Moonshot AI发布了Kimi k1.5[20]，在推理任务上报告达到与o1相当的性能（即AIME上77.5%，MATH上96.2%）。该模型也报告在后训练中使用了RL[21]。从技术报道来看，Kimi是多模态的，支持文本/代码和图像。它具有128k的上下文长度，意味着可以通过提示输入整本小说。他们简化的RL框架平衡了探索和利用，并惩罚模型生成过于冗长的响应。他们还通过混合长短思维链模型的权重来鼓励更短/更快的响应[22]。

1月底，Qwen发布了新的模型系列Qwen2.5-VL[23]。这个多模态（视觉和文本）模型相比Qwen2有多项改进，包括更好的文本识别（包括手写、多语言和表格）、改进的物体检测和空间推理、改进的代理功能和更好的视频功能。

2月2日，OpenAI宣布了Deep Research[24]，声称"它在几十分钟内完成了人类需要数小时才能完成的工作"。在DeepSeek模型发布后，有人推测这可能会迫使OpenAI匆忙发布下一个版本以维持市场主导地位。现在判断这是否属实或其对模型产生的影响还为时过早。

## 4 反应和观察

### 4.1 影响和后果

• 这些模型突显了算法效率和资源优化的重要性。DeepSeek表明，无需依赖暴力扩展，也能以显著更少的资源实现高性能。

• OpenAI最近已两次降价，且压力正在增加，要求他们允许用户访问推理标记。
  - 1月29日，OpenAI暗示DeepSeek"可能不当地蒸馏了我们的模型"[25]。截至发稿时，尚未有进一步的分析或确认。
  - 1月31日，OpenAI作为回应部署了他们的o3-mini推理模型[26]。该模型使用审议对齐，在每个推理步骤都会审查一组内部政策，以确保不忽视任何安全规则，但他们也承认推理模型更善于自我越狱[27]。

• 对英伟达产生了影响：构建最先进模型真正需要多少顶级芯片？英伟达股价下跌17%，市值蒸发近6000亿美元[4, 28]。

• 这也表明，美国的CHIPS法案[29]旨在减缓中国在AI竞赛中的步伐，可能无意中促进了创新。

• DeepSeek应用在英国、美国和中国的应用商店排行榜上名列前茅[30]。

### 4.2 AI研究社区对DeepSeek的观察

• 较小的模型可以在本地机器上免费运行，提高隐私性。它们很快就可以通过HuggingFace[31]和Ollama[32]安装。

• 一些研究人员评论说它可能比较脆弱，且难以提示。

• 研究人员声称其推理能力可用于自我越狱[33]，威胁研究人员对其安全护栏的薄弱性表示担忧[34, 35]。

• 对V3论文中描述的成本存在一些质疑，DeepSeek表示训练V3模型的成本约为560万美元。尽管其他人[36]认为提出的数字是合理的。
  - Scale.ai创始人Alexandr Wang表示，他相信DeepSeek拥有50,000个H100 GPU[37]。

• 一些研究人员注意到，类似的方法两年前就在模型上尝试过，但结果远不及现在[38]。假设基础模型的质量是关键因素。

• RLCoT（通过RL学习的思维链）被认为是涌现行为，直到模型规模达到约15亿参数时才会出现。而（简单的）RL算法的选择并不会产生太大差异[39]。

• 用户观察到，思维链内部对话常常充满自我怀疑，表现出很少的自信，但答案却以过度自信的语气给出。这看起来更诚实，因此建立了用户对模型的信任。

• 许多这些系统都在使用生成式AI来帮助创建或整理数据集，以训练更好的推理能力。这种方法是否会遭受与在LLM生成材料上训练LLM相同的退化问题？


### 4.3 政治评论

许多人评论了该模型拒绝回答某些与中共审查相关话题的问题[40]。从国家安全的角度来看，这引发了几个担忧。特别是，当大多数用户从使用美国对齐的LLM转向使用中共对齐的LLM时，风险状况如何变化。尤其是当大量用户使用LLM而不是搜索引擎来获取事实时（参见图1中2025年2月3日生成的响应差异示例）。然而，当模型在本地运行时，审查似乎并不存在。

政治评论家认为，DeepSeek-R1模型的发布特意与特朗普总统就职时间对齐，以削弱人们对美国在AI领域主导地位的认知[40]，或者可能是为了削弱星门计划（Stargate Project）的影响[41]。当然，这也可能是为了赶在（中国）新年前发布。

美国[42]和澳大利亚[43]政府对员工使用DeepSeek表示担忧，美国海军以"安全和道德"为由禁止使用该应用[44]。同时，该应用也在意大利全国范围内被禁止，等待隐私监管机构Garante对应用处理个人数据的调查[45]。加上最近的数据泄露事件[46]允许研究人员访问超过100万条明文聊天历史，这描绘出快节奏AI环境中数据处理实践令人担忧的图景。

一位"白宫AI和加密货币沙皇"表示"有实质性证据表明DeepSeek在这里所做的是从OpenAI的模型中提取知识"[42]。有趣的是，看看OpenAI如何缓解师生威胁，以及他们如何在不影响可用性的情况下实现这一点。此外，如果OpenAI选择采取更严格的使用政策，看看其影响也会很有趣；这可能会迫使更多人转向开源的非西方替代方案。或者，这可能导致前沿模型领域的分裂，形成针对目标受众定制的封闭花园、孤立模型。事实上，我们已经看到了这方面的证据，比如OpenEuroLLM项目[47]。

注：原文提到的"图1"（Fig. 1）在给出的文本中并未包含，因此无法提供具体的图片内容翻译。

## 5 讨论

我们认为这一波推理模型发布潮，伴随着更低的训练和推理成本，是中国对数据（和计算）规模限制的技术回应。这些模型展示了KISS方法和巧妙工程的创新组合，建立在开源文献的基础上，许多技术可以追溯到最近的论文。尽管如此，文档中令人遗憾地缺少了用于训练的数据细节。

通过推理来改进数学和编程的重点可能是为了支持未来的代理方法（2025年被吹捧为代理之年）。但应该注意的是，这些评估在自动化程度上处于较容易的一端；正确的数学答案是确定的，带有单元测试的编程任务也可以轻松自动化，因此更适合RL类型的方法。

然而，如果我们考虑简单的RL允许模型通过相对较小的数据集（如8000个MATH）进行"技能提升"，那么还可以在小型模型上开发/赋予哪些其他技能？这种技术是否只对通过/失败数据集有效？或者在提升模型的故事写作创造力时是否能获得类似的回报？

对于技术使用和真实训练成本的不确定性的回应：我们显然难以提供准确可靠的结论。这确实提出了一个有趣的研究问题：从已发布的模型中可以获得哪些关于开发流程的见解？同样，能否从训练中使用的数据集获得任何见解？

对小型模型的影响是双重的：
1. 已证实可以将信息从大型模型蒸馏到小型模型的能力 - 为后训练提供了捷径
2. 使用简单强化学习的方法可以以较低的计算成本带来显著（尽管狭窄）的性能改进

这两种方法都可能改变D&NS组合的风险阈值，包括（但不限于）：恶意网络、错误/虚假信息（包括深度伪造生成）等，因为它们可能为小型、非集中化模型提供更好的推理能力基础。

虽然这些模型并没有"修复"与LLM相关的问题，如幻觉[5]，但DeepSeek的开放权重发布，加上媒体的关注，提出了这些模型是否"足够好"的问题；考虑到较小的、蒸馏的模型是免费提供的，它们是否足够好以获得广泛采用（企业、研究人员和爱好者）？有些人已经在树莓派上安装了Qwen的蒸馏版本（虽然只能达到每秒1.2个标记）。更便宜的API费率已经促使开发人员编写自己的VSCode插件，使用DeepSeek模型而不是GitHub的copilot。一些人假设这种草根采用是通向AGI的关键非技术步骤（普及性而不是能力）。如果是这样，就需要评估模型中代表的文化对社会的影响（如果有的话）；西方与东方价值观。

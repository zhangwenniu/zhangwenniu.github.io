---
layout: post
title: Unisurf, Unifying Surface Reconstruction via Implicit Surface Learning
categories: [sdf]
---

# 链接

- [Arxiv Link](https://arxiv.org/abs/2104.10078)

- [GitHub Link](https://github.com/autonomousvision/unisurf)

- [ICCV 2021 oral](https://openaccess.thecvf.com/content/ICCV2021/html/Oechsle_UNISURF_Unifying_Neural_Implicit_Surfaces_and_Radiance_Fields_for_Multi-View_ICCV_2021_paper.html)


# 论文重点和难点

好的！这篇论文的重点和难点主要集中在如何将**神经隐式表面（Implicit Surfaces）** 和**神经辐射场（Neural Radiance Fields，简称NeRF）** 结合起来，用于从多视角图像中重建高质量的3D场景。以下是对重点和难点的详细解读：



### **1. 论文的核心目标和背景**


#### **重点**

- **目标**：提出一种新的方法（UNISURF），能够在没有像素级物体掩码（masks）的情况下，从多视角图像中重建出高质量的3D表面。

- **背景**：

  - 神经隐式表面（如IDR）需要精确的像素级掩码作为监督，限制了其在复杂场景中的应用。

  - 神经辐射场（如NeRF）虽然能够生成高质量的视图合成，但其重建的3D表面不够准确，且无法直接优化表面。

  - **关键创新**：将隐式表面和辐射场统一到一个框架中，结合两者的优点。



### **2. 核心方法：UNISURF**

#### **重点**

- **统一表面和体积渲染**：

  - 作者提出了一种新的公式，将隐式表面模型和体积辐射场结合起来。这种统一的公式允许同时进行表面渲染（Surface Rendering）和体积渲染（Volume Rendering）。

  - 表面渲染直接在表面上计算颜色，适合精确的几何重建。

  - 体积渲染通过沿射线采样体积密度来计算颜色，适合处理不确定性和模糊性。

  - **关键公式**：

    - 表面渲染：  

      $$
      \hat{C}_s(r) = c_\theta(x_s, n_s, h_s, d)
      $$

      其中，$x_s$是通过根查找找到的表面点，$n_s$是表面法线，$h_s$是几何特征。

    - 体积渲染：  

      $$
      \hat{C}_v(r) = \sum_{i=1}^{N} o_\theta(x_i) \prod_{j<i} (1 - o_\theta(x_j)) c_\theta(x_i, n_i, h_i, d)
      $$

      其中，$o_\theta(x)$是连续的占用场，表示空间中某点是否被占据。


- **优化策略**：

  - **难点**：表面渲染需要精确的表面，但在优化初期表面是模糊的。因此，作者设计了一个动态采样策略：

    - 在优化初期，使用体积渲染覆盖整个空间，逐步解决模糊性和不确定性。

    - 随着优化的进行，逐渐将采样点集中在估计的表面附近，以提高精度。

  - **采样区间衰减**：  

    $$
    \Delta_k = \max(\Delta_{\text{max}} \exp(-k \beta), \Delta_{\text{min}})
    $$

    其中，$k$是迭代次数，$\beta$是超参数。


- **损失函数**：

  - 使用重建损失（$\ell_1$）和表面正则化（$\ell_2$）来优化模型。

  - **表面正则化**：鼓励表面法线在局部区域内保持一致，避免表面过于复杂或不连续。



### **3. 实验结果**

#### **重点**

- **数据集**：在DTU MVS、BlendedMVS和SceneNet数据集上进行了实验。

- **对比方法**：

  - **COLMAP**：经典的多视角立体重建方法。

  - **NeRF**：神经辐射场方法。

  - **IDR**：最先进的隐式表面重建方法，需要像素级掩码。

- **结果**：

  - 在DTU数据集上，UNISURF在没有掩码的情况下，与需要掩码的IDR方法表现相当。

  - 在BlendedMVS和SceneNet数据集上，UNISURF能够重建出复杂的多物体场景，而其他方法（如NeRF）则会出现表面噪声或细节丢失。

  - **难点**：对于复杂的背景和纹理较少的区域，UNISURF仍然能够重建出合理的几何结构，而其他方法可能会失败。



### **4. 难点和挑战**

- **难点1：表面和体积渲染的结合**

  - 表面渲染和体积渲染的结合需要精心设计。作者通过动态采样策略和统一的公式，解决了在优化初期表面模糊的问题。

- **难点2：无掩码重建**

  - 传统的隐式表面方法依赖于像素级掩码来约束优化过程。UNISURF通过体积渲染逐步解决模糊性，最终实现无掩码的精确重建。

- **难点3：优化过程的稳定性**

  - 在优化过程中，如何平衡表面细节和全局结构是一个挑战。作者通过表面正则化和采样区间衰减策略，确保了优化的稳定性和精度。

- **难点4：复杂场景的处理**

  - 对于包含多个物体和复杂背景的场景，如何避免背景干扰并重建出目标物体是一个挑战。UNISURF通过学习背景模型，解决了这一问题。



### **5. 总结**

- **贡献**：UNISURF提出了一种新的框架，将隐式表面和辐射场统一起来，能够在没有掩码的情况下重建高质量的3D表面。

- **优势**：

  - 不需要像素级掩码，适用于更广泛的场景。

  - 结合了表面渲染和体积渲染的优点，重建精度更高。

  - 动态采样策略和表面正则化确保了优化的稳定性和细节表现。

- **局限性**：

  - 仅适用于非透明物体。

  - 对于过曝或纹理较少的区域，重建精度可能下降。

# 论文详细讲解

### **1. 研究背景与动机**

#### **1.1 3D重建的挑战**

从多视角图像中重建3D场景是计算机视觉中的一个经典问题。传统方法（如经典的多视角立体视觉，MVS）依赖于特征匹配和深度图融合，但这些方法通常需要复杂的管道，并且在处理复杂场景时存在局限性。近年来，**神经隐式表示**（如隐式表面和神经辐射场）成为3D重建和视图合成的新范式。

#### **1.2 神经隐式表面与神经辐射场**

- **隐式表面方法**（如IDR、DVR）：通过神经网络表示3D几何，将表面定义为某个隐式函数的等值面。这些方法能够重建出高质量的几何细节，但需要**像素级物体掩码**作为监督，限制了其在复杂场景中的应用。

- **神经辐射场（NeRF）**：将场景表示为体积密度和颜色场，通过体积渲染合成新视图。虽然NeRF在视图合成中表现出色，但其重建的表面通常不够准确，且无法直接优化表面。

#### **1.3 本文的动机**

本文的核心动机是**将隐式表面和神经辐射场统一到一个框架中**，从而结合两者的优点：

- 利用隐式表面的精确几何表示能力；

- 利用神经辐射场的体积渲染能力，解决不确定性和模糊性；

- **无需像素级掩码**，扩展到更复杂的场景。



### **2. 方法：UNISURF**

#### **2.1 统一表面和体积渲染**

本文提出了一种新的框架——**UNISURF**，它将隐式表面和神经辐射场统一起来。核心思想是：

- 将隐式表面表示为一个**连续的占用场**（occupancy field），而不是直接预测体积密度。

- 通过占用场，既可以进行表面渲染（直接在表面上采样），也可以进行体积渲染（沿射线采样体积密度）。

**关键公式**：

1. **表面渲染**：  

   $$
   \hat{C}_s(r) = c_\theta(x_s, n_s, h_s, d)
   $$

   其中，$x_s$是通过根查找找到的表面点，$n_s$是表面法线，$h_s$是几何特征，$d$是视图方向。

2. **体积渲染**：  

   $$
   \hat{C}_v(r) = \sum_{i=1}^{N} o_\theta(x_i) \prod_{j<i} (1 - o_\theta(x_j)) c_\theta(x_i, n_i, h_i, d)
   $$

   其中，$o_\theta(x)$是占用场，表示空间中某点是否被占据。

#### **2.2 优化策略**

在优化过程中，作者设计了一个动态采样策略：

- **初期**：使用体积渲染覆盖整个空间，解决模糊性和不确定性。

- **后期**：逐渐将采样点集中在估计的表面附近，提高精度。

**采样区间衰减**：  

$$
\Delta_k = \max(\Delta_{\text{max}} \exp(-k \beta), \Delta_{\text{min}})
$$

其中，$k$是迭代次数，$\beta$是超参数。这种策略使得模型在训练初期能够快速捕捉粗略的几何结构，而在后期能够精细调整表面。

#### **2.3 损失函数**

- **重建损失**：使用$\ell_1$损失，衡量渲染图像与真实图像之间的差异。

- **表面正则化**：使用$\ell_2$损失，鼓励表面法线在局部区域内保持一致，避免表面过于复杂或不连续。

**公式**：

$$
L = L_{\text{rec}} + \lambda L_{\text{reg}}
$$

$$
L_{\text{rec}} = \sum_{r \in R} \|\hat{C}_v(r) - C(r)\|_1
$$

$$
L_{\text{reg}} = \sum_{x_s \in S} \|n(x_s) - n(x_s + \epsilon)\|_2
$$


#### **2.4 实现细节**

- **网络架构**：占用场使用8层MLP，激活函数为Softplus；颜色场使用4层ReLU MLP。

- **优化**：使用Adam优化器，学习率0.0001，训练450k次迭代。

- **推理**：支持体积渲染和表面渲染两种方式，表面渲染速度更快。




### **3. 实验**

#### **3.1 数据集**

- **DTU MVS数据集**：包含49到64张图像，分辨率1200×1600，提供相机参数和真实3D形状。

- **BlendedMVS数据集**：大规模多视角图像数据集，包含复杂场景。

- **SceneNet数据集**：室内场景的合成数据集，用于测试复杂场景的重建能力。

#### **3.2 对比方法**

- **COLMAP**：经典的MVS方法，使用Poisson Surface Reconstruction重建网格。

- **NeRF**：神经辐射场方法，通过体积密度提取几何。

- **IDR**：最先进的隐式表面方法，需要像素级掩码。


#### **3.3 结果**

- **DTU数据集**：

  - UNISURF在没有掩码的情况下，与需要掩码的IDR方法表现相当。

  - 在Chamfer距离上，UNISURF的平均值为1.02，接近IDR的0.90。

  - COLMAP虽然在某些指标上表现更好，但生成的网格不完整。

  - NeRF重建的表面存在噪声和细节丢失。


- **BlendedMVS和SceneNet数据集**：

  - UNISURF能够重建出复杂的多物体场景，而NeRF的表面噪声较多。

  - COLMAP在室内场景中表现较好，但在复杂背景或均匀颜色区域会出现问题。

#### **3.4 消融研究**

- **渲染方式对比**：

  - **表面渲染（SR）**：无法收敛。

  - **均匀体积渲染（Uniform VR）**：过于平滑，丢失细节。

  - **层次体积渲染（HVR）**：结果不够精确。

  - **UNISURF**：结合了表面和体积渲染的优点，表现最佳。

- **表面正则化的影响**：

  - 不使用表面正则化时，模型在纹理较少的区域（如桌面）会出现不平滑的表面。



### **4. 结论**

#### **4.1 贡献总结**

UNISURF提出了一种新的框架，将隐式表面和神经辐射场统一起来，能够在没有掩码的情况下重建高质量的3D表面。该方法在多个数据集上表现出色，证明了其在复杂场景中的适用性。

#### **4.2 局限性**

- **非透明物体**：UNISURF仅适用于非透明物体。

- **过曝或纹理较少的区域**：这些区域的重建精度可能下降。

- **稀疏可见区域**：对于在图像中很少被看到的区域，重建精度较低。


#### **4.3 未来工作**

- **引入先验知识**：为解决稀疏可见区域和纹理较少区域的不确定性，可以引入先验知识。

- **学习概率模型**：开发能够捕捉物体规律和不确定性的概率神经表面模型，以提高重建精度。



### **5. 总结**

这篇论文的核心在于提出了一个创新的框架，将隐式表面和神经辐射场结合起来，解决了现有方法的局限性。通过动态采样策略和统一的渲染公式，UNISURF能够在没有掩码的情况下重建高质量的3D表面，并在多个数据集上验证了其有效性。


# 论文方法部分详解

### **4. 方法**

#### **4.1 统一表面和体积渲染**

**核心思想**：  

UNISURF的核心是将隐式曲面模型和神经辐射场统一起来，以便同时利用表面渲染和体积渲染的优点。具体来说，作者通过一个**连续的占用场（occupancy field）** 来表示场景，而不是直接预测体积密度（如NeRF）或依赖于逐像素掩码（如IDR）。

**占用场的定义**：  

占用场是一个连续函数，将三维空间中的每个点映射到一个概率值，表示该点是否被物体占据：

$$
o_\theta(x): \mathbb{R}^3 \to [0, 1]
$$

其中，$x \in \mathbb{R}^3$是三维空间中的一个点，$\theta$是模型参数。曲面被定义为占用概率为0.5的等值面：

$$
S = \{x_s \mid o_\theta(x_s) = 0.5\}
$$

**渲染公式**：  

UNISURF支持两种渲染方式：

1. **体积渲染**：通过沿射线采样体积密度来计算颜色。  
   公式如下：

   $$
   \hat{C}_v(r) = \sum_{i=1}^{N} o_\theta(x_i) \prod_{j<i} (1 - o_\theta(x_j)) c_\theta(x_i, n_i, h_i, d)
   $$

   其中，$x_i$是沿射线$r$的采样点，$o_\theta(x_i)$是占用概率，$c_\theta(x_i, n_i, h_i, d)$是颜色场，$n_i$和$h_i$分别是点$x_i$处的法线和几何特征。

2. **表面渲染**：直接在隐式曲面上计算颜色。  
   公式如下：

   $$
   \hat{C}_s(r) = c_\theta(x_s, n_s, h_s, d)
   $$

   其中，$x_s$是通过根查找（root-finding）找到的曲面上的点，$n_s$和$h_s$分别是$x_s$处的法线和几何特征。

**统一的关键**：  

通过占用场，UNISURF可以在优化的早期阶段使用体积渲染来捕捉粗略的几何结构，而在优化的后期阶段，逐渐聚焦到曲面上，使用表面渲染来提高精度。这种动态切换是通过一个**采样区间衰减策略**实现的。



#### **4.2 损失函数**

UNISURF的优化目标是最小化以下正则化的损失函数：

$$
L = L_{\text{rec}} + \lambda L_{\text{reg}}
$$

其中：
1. **重建损失（Reconstruction Loss）**：  

   使用$\ell_1$范数计算渲染图像与真实图像之间的差异：

   $$
   L_{\text{rec}} = \sum_{r \in R} \|\hat{C}_v(r) - C(r)\|_1
   $$

   其中，$R$是小批量中的所有射线集合，$C(r)$是真实颜色，$\hat{C}_v(r)$是通过体积渲染得到的颜色。

2. **表面正则化（Surface Regularization）**：  

   使用$\ell_2$范数鼓励曲面上的法线在局部区域内保持一致，避免表面过于复杂或不连续：

   $$
   L_{\text{reg}} = \sum_{x_s \in S} \|n(x_s) - n(x_s + \epsilon)\|_2
   $$

   其中，$n(x_s)$是$x_s$处的法线，$\epsilon$是一个小的随机扰动。

**法线计算**：  

法线可以通过占用场的梯度计算得到：

$$
n(x_s) = \frac{\nabla_{x_s} o_\theta(x_s)}{\|\nabla_{x_s} o_\theta(x_s)\|_2}
$$

这种正则化项对于纹理较少的区域特别有用，因为它引入了对平滑表面的归纳偏差。



#### **4.3 优化策略**

**动态采样策略**：  

在优化过程中，UNISURF使用一个动态采样策略来平衡体积渲染和表面渲染的优点。具体来说，作者设计了一个**采样区间衰减策略**，在优化的早期阶段使用较大的采样区间来捕捉粗略的几何结构，而在优化的后期阶段逐渐缩小采样区间，聚焦到曲面上。

**采样区间衰减公式**：  

采样区间$\Delta$在每次迭代中按照以下公式衰减：

$$
\Delta_k = \max(\Delta_{\text{max}} \exp(-k \beta), \Delta_{\text{min}})
$$

其中：

- $k$是迭代次数；

- $\beta$是衰减率；

- $\Delta_{\text{max}}$是初始采样区间；

- $\Delta_{\text{min}}$是最终采样区间。

**采样过程**：  

假设$x_s = o + t_s d$是曲面上的点，采样点$x_i = o + t_i d$通过以下方式生成：

$$
t_i \sim U\left(t_s + \frac{2i - 2}{N - 1} \Delta, t_s + \frac{2i}{N - 1} \Delta\right)
$$

**优化过程**：  

在优化的早期阶段，采样点覆盖整个优化体积，利用体积渲染引导重建过程。在优化的后期阶段，采样点集中在估计的曲面附近，利用表面渲染提高精度。这种策略结合了体积渲染的鲁棒性和表面渲染的精确性。



#### **4.4 实现细节**

**网络架构**：  

- **占用场网络**：使用一个8层的MLP，激活函数为Softplus，隐藏层维度为256。网络初始化为一个球体，以便在优化初期提供一个合理的初始形状。

- **颜色场网络**：使用一个4层的ReLU MLP，输入包括三维位置$x$、视图方向$d$、法线$n$和几何特征$h$。

**编码方式**：  

- 使用傅里叶特征（Fourier Features）对三维位置$x$和视图方向$d$进行编码，分别使用6层和4层编码。

**优化过程**：  

- 使用Adam优化器，学习率为0.0001。

- 每次迭代随机采样一个视图，并从该视图中随机采样$M=1024$个像素/射线。

- 使用256个均匀采样点进行根查找，通过8步割线法（Secant Method）找到曲面上的点。

- 在区间内采样$N=64$个点，在自由空间采样32个点。

- 采样区间衰减参数：$\beta = 1.5 \times 10^{-5}$，$\Delta_{\text{min}} = 0.05$，$\Delta_{\text{max}} = 1.0$。

- 总共训练450k次迭代，分别在200k和400k次迭代后进行学习率衰减。

**推理过程**：  

- 支持两种渲染方式：体积渲染和表面渲染。表面渲染速度更快。

- 使用多分辨率等值面提取（MISE）算法提取网格，初始分辨率为$64^3$，通过3次上采样生成最终网格。



### **总结**

UNISURF通过将隐式曲面和神经辐射场统一起来，结合了体积渲染和表面渲染的优点。通过动态采样策略和表面正则化，UNISURF能够在没有输入掩码的情况下重建出高质量的三维几何结构。这种统一框架不仅提高了重建精度，还扩展了神经隐式表示在复杂场景中的应用范围。


## 关键技术解释

### 割线法

**割线法（Secant Method）** 是一种求解非线性方程的数值方法，它通过迭代的方式逼近方程的根。割线法是牛顿法的一种变体，它不需要计算导数，而是使用函数值来估计导数。

#### 割线法的基本思想

割线法的基本思想是使用两个初始点$x_0$和$x_1$，通过这两点的函数值$f(x_0)$和$f(x_1)$来估计函数的导数。然后，使用这个估计的导数来更新解的估计值。

#### 割线法的迭代公式

割线法的迭代公式如下：

$$
x_{n+1} = x_n - f(x_n) \frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}
$$

其中，$x_n$和$x_{n-1}$是当前和前一次迭代的解的估计值，$f(x_n)$和$f(x_{n-1})$是对应的函数值。

#### 割线法的步骤

1. 选择两个初始点$x_0$和$x_1$，使得$f(x_0)$和$f(x_1)$异号，即$f(x_0) \cdot f(x_1) < 0$。
2. 计算$x_2$使用迭代公式。
3. 检查$x_2$是否满足收敛条件，即$|f(x_2)| < \epsilon$，其中$\epsilon$是给定的容忍度。
4. 如果满足收敛条件，则停止迭代，返回$x_2$作为方程的根。
5. 如果不满足收敛条件，则将$x_1$和$x_2$作为新的初始点，重复步骤2-4。

#### 割线法的优缺点

**优点**：
- 割线法不需要计算导数，因此对于那些导数难以计算的函数，割线法是一个很好的选择。
- 割线法的收敛速度通常比二分法快。

**缺点**：
- 割线法的收敛速度比牛顿法慢。
- 割线法对于某些函数可能不收敛。

#### 割线法的应用

割线法在工程和科学计算中被广泛应用于求解非线性方程。例如，在优化问题中，割线法可以用来求解梯度为零的点，即函数的极值点。

#### 割线法的示例

假设我们要求解方程$f(x) = x^2 - 2 = 0$。我们可以选择$x_0 = 1$和$x_1 = 2$作为初始点。然后，使用割线法的迭代公式进行迭代：

1. 计算$x_2$：
   $$
   x_2 = x_1 - f(x_1) \frac{x_1 - x_0}{f(x_1) - f(x_0)} = 2 - (2^2 - 2) \frac{2 - 1}{(2^2 - 2) - (1^2 - 2)} = 2 - 2 \cdot \frac{1}{3} = \frac{4}{3}
   $$

2. 检查$x_2$是否满足收敛条件：
   $$
   |f(x_2)| = \left|\left(\frac{4}{3}\right)^2 - 2\right| = \left|\frac{16}{9} - 2\right| = \left|\frac{16}{9} - \frac{18}{9}\right| = \left|-\frac{2}{9}\right| = \frac{2}{9}
   $$

   如果$\frac{2}{9} < \epsilon$，则停止迭代，返回$x_2 = \frac{4}{3}$作为方程的根。否则，继续迭代。

通过多次迭代，割线法可以逼近方程的根$\sqrt{2}$。

### 损失函数部分代码

```python
# Coding From: https://github.com/autonomousvision/unisurf/blob/224efc73d50acf422edacaebcaf59a8b8ba6d1ff/model/extracting.py#L234

    def refine_mesh(self, mesh, occ_hat, c=None):
        ''' Refines the predicted mesh.

        Args:   
            mesh (trimesh object): predicted mesh
            occ_hat (tensor): predicted occupancy grid
            c (tensor): latent conditioned code c
        '''

        self.model.eval()

        # Some shorthands
        n_x, n_y, n_z = occ_hat.shape
        assert(n_x == n_y == n_z)
        # threshold = np.log(self.threshold) - np.log(1. - self.threshold)
        threshold = self.threshold

        # Vertex parameter
        v0 = torch.FloatTensor(mesh.vertices).to(self.device)
        v = torch.nn.Parameter(v0.clone())

        # Faces of mesh
        faces = torch.LongTensor(mesh.faces)

        # detach c; otherwise graph needs to be retained
        # caused by new Pytorch version?
        # c = c.detach()

        # Start optimization
        optimizer = optim.RMSprop([v], lr=1e-5)

        # Dataset
        ds_faces = TensorDataset(faces)
        dataloader = DataLoader(ds_faces, batch_size=self.refine_max_faces,
                                shuffle=True)

        # We updated the refinement algorithm to subsample faces; this is
        # usefull when using a high extraction resolution / when working on
        # small GPUs
        it_r = 0
        while it_r < self.refinement_step:
            for f_it in dataloader:
                f_it = f_it[0].to(self.device)
                optimizer.zero_grad()

                # Loss
                face_vertex = v[f_it]
                eps = np.random.dirichlet((0.5, 0.5, 0.5), size=f_it.shape[0])
                eps = torch.FloatTensor(eps).to(self.device)
                face_point = (face_vertex * eps[:, :, None]).sum(dim=1)

                face_v1 = face_vertex[:, 1, :] - face_vertex[:, 0, :]
                face_v2 = face_vertex[:, 2, :] - face_vertex[:, 1, :]
                face_normal = torch.cross(face_v1, face_v2)
                face_normal = face_normal / \
                    (face_normal.norm(dim=1, keepdim=True) + 1e-10)

                face_value = torch.cat([
                    # torch.sigmoid(self.model.decode(p_split, c).logits)
                    self.model.decoder(p_split, None, only_occupancy=True).squeeze(-1)
                    for p_split in torch.split(
                        face_point.unsqueeze(0), 20000, dim=1)], dim=1)

                normal_target = -autograd.grad(
                    [face_value.sum()], [face_point], create_graph=True)[0]

                normal_target = \
                    normal_target / \
                    (normal_target.norm(dim=1, keepdim=True) + 1e-10)
                loss_target = (face_value - threshold).pow(2).mean()
                loss_normal = \
                    (face_normal - normal_target).pow(2).sum(dim=1).mean()

                loss = loss_target + 0.01 * loss_normal

                # Update
                loss.backward()
                optimizer.step()

                # Update it_r
                it_r += 1

                if it_r >= self.refinement_step:
                    break

        mesh.vertices = v.data.cpu().numpy()
        return mesh

```

这段代码实现了一个网格优化过程，用于细化预测的三维网格。它与论文中提到的损失函数（特别是表面正则化部分）密切相关。接下来，我将结合代码和论文内容进行详细讲解。

#### **1. 代码功能概述**

这段代码实现了一个网格优化过程，用于细化预测的三维网格。它通过优化网格顶点的位置，使得网格更接近目标几何形状。优化过程中使用了两个主要的损失项：

1. **目标损失（Target Loss）**：确保网格顶点的预测占用值接近阈值（即接近表面）。
2. **法线损失（Normal Loss）**：确保网格的法线与目标法线一致，这与论文中的表面正则化项相似。

#### **2. 代码与论文的对应关系**

##### **2.1 损失函数**

论文中提到的损失函数为：
$$
L = L_{\text{rec}} + \lambda L_{\text{reg}}
$$
其中：
- $L_{\text{rec}}$ 是重建损失，确保预测的颜色或占用值与目标一致。
- $L_{\text{reg}}$ 是正则化项，确保表面法线的平滑性。

在代码中，这两个损失项分别对应为：
- **目标损失（`loss_target`）**：计算预测占用值与阈值的差异。
- **法线损失（`loss_normal`）**：计算网格法线与目标法线的差异。

##### **2.2 目标损失（Target Loss）**

论文中的重建损失为：
$$
L_{\text{rec}} = \sum_{r \in R} \|\hat{C}_v(r) - C(r)\|_1
$$
在代码中，目标损失计算为：
```python
loss_target = (face_value - threshold).pow(2).mean()
```
这里的`face_value`是通过模型预测的占用值，`threshold`是表面的阈值。损失函数使用了平方差（L2损失），而不是论文中的L1损失。这是实现上的一个变体，但目的相同：确保预测值接近阈值。

##### **2.3 法线损失（Normal Loss）**

论文中的表面正则化项为：
$$
L_{\text{reg}} = \sum_{x_s \in S} \|n(x_s) - n(x_s + \epsilon)\|_2
$$
在代码中，法线损失计算为：
```python
loss_normal = (face_normal - normal_target).pow(2).sum(dim=1).mean()
```
这里的`face_normal`是网格的法线，`normal_target`是通过预测值的梯度计算得到的目标法线。这个损失项确保网格的法线与目标法线一致，从而实现表面的平滑性。

##### **2.4 法线计算**

论文中提到，法线可以通过隐式函数的梯度计算得到：
$$
n(x_s) = \frac{\nabla_{x_s} o_\theta(x_s)}{\|\nabla_{x_s} o_\theta(x_s)\|_2}
$$
在代码中，目标法线`normal_target`通过自动微分计算得到：
```python
normal_target = -autograd.grad([face_value.sum()], [face_point], create_graph=True)[0]
```
这里使用了PyTorch的`autograd`功能来计算梯度，这与论文中的隐式微分一致。

#### **3. 代码实现细节**

##### **3.1 网格顶点参数化**

代码将网格顶点参数化为一个可优化的参数`v`：
```python
v0 = torch.FloatTensor(mesh.vertices).to(self.device)
v = torch.nn.Parameter(v0.clone())
```
这允许我们通过梯度下降优化网格顶点的位置。

##### **3.2 随机采样点**

为了计算法线损失，代码在每个三角形面上随机采样点：
```python
eps = np.random.dirichlet((0.5, 0.5, 0.5), size=f_it.shape[0])
face_point = (face_vertex * eps[:, :, None]).sum(dim=1)
```
这与论文中提到的在邻域内采样点的思想一致。

##### **3.3 损失函数组合**

最终的损失函数是目标损失和法线损失的加权和：
```python
loss = loss_target + 0.01 * loss_normal
```
这里的权重`0.01`是超参数，用于平衡两个损失项的贡献。

##### **3.4 优化过程**

代码使用`RMSprop`优化器对网格顶点进行优化：
```python
optimizer = optim.RMSprop([v], lr=1e-5)
```
优化过程中，代码通过梯度下降逐步调整顶点位置，以最小化损失函数。

#### **4. 总结**

这段代码实现了一个网格优化过程，通过目标损失和法线损失来细化预测的三维网格。它与论文中提到的损失函数紧密相关，特别是表面正则化部分。代码通过优化网格顶点的位置，使得网格更接近目标几何形状，同时保持表面的平滑性。




### 狄利克雷分布

**狄利克雷分布（Dirichlet Distribution）**是一种定义在单纯形（simplex）上的连续概率分布，常用于表示多类别分布的概率分布。它是Beta分布的多维推广，广泛应用于贝叶斯统计、机器学习和概率模型中。

#### **1. 狄利克雷分布的基本概念**

##### **1.1 单纯形（Simplex）**

单纯形是一个多维空间中的几何对象，表示所有分量非负且和为1的点的集合。例如：
- 在二维空间中，单纯形是一个三角形。
- 在三维空间中，单纯形是一个四面体。
- 在更高维度中，单纯形是一个超多面体。

数学上，一个$K$维单纯形可以表示为：
$$
\Delta^{K-1} = \left\{ (x_1, x_2, \dots, x_K) \mid x_i \geq 0, \sum_{i=1}^K x_i = 1 \right\}
$$

##### **1.2 狄利克雷分布的定义**

狄利克雷分布是一个定义在单纯形上的概率分布，其概率密度函数（PDF）为：
$$
f(\mathbf{x} \mid \boldsymbol{\alpha}) = \frac{1}{B(\boldsymbol{\alpha})} \prod_{i=1}^K x_i^{\alpha_i - 1}
$$
其中：
- $\mathbf{x} = (x_1, x_2, \dots, x_K)$ 是单纯形上的一个点。
- $\boldsymbol{\alpha} = (\alpha_1, \alpha_2, \dots, \alpha_K)$ 是狄利克雷分布的参数，且 $\alpha_i > 0$。
- $B(\boldsymbol{\alpha})$ 是归一化常数，称为**多变量Beta函数**，定义为：
  $$
  B(\boldsymbol{\alpha}) = \frac{\prod_{i=1}^K \Gamma(\alpha_i)}{\Gamma\left(\sum_{i=1}^K \alpha_i\right)}
  $$
  其中，$\Gamma(\cdot)$ 是Gamma函数。

#### **2. 狄利克雷分布的性质**

##### **2.1 参数的意义**

狄利克雷分布的参数$\boldsymbol{\alpha}$决定了分布的形状：
- 如果所有$\alpha_i$都大于1，分布会集中在单纯形的内部。
- 如果所有$\alpha_i$都小于1，分布会集中在单纯形的边界上。
- 如果所有$\alpha_i$都等于1，分布是均匀的。

##### **2.2 期望和方差**

狄利克雷分布的期望和方差分别为：
- **期望**：
  $$
  \mathbb{E}[x_i] = \frac{\alpha_i}{\sum_{j=1}^K \alpha_j}
  $$
- **方差**：
  $$
  \text{Var}(x_i) = \frac{\alpha_i (\sum_{j=1}^K \alpha_j - \alpha_i)}{\left(\sum_{j=1}^K \alpha_j\right)^2 \left(\sum_{j=1}^K \alpha_j + 1\right)}
  $$

#### **3. 狄利克雷分布的应用**

##### **3.1 贝叶斯统计**

狄利克雷分布常用于贝叶斯统计中的先验分布，特别是当后验分布是多项分布时。例如，在文本分类中，狄利克雷分布可以作为文档中单词分布的先验。

##### **3.2 机器学习**

在机器学习中，狄利克雷分布用于表示多类别分布的概率分布。例如，在深度学习中，狄利克雷分布可以用于生成模型中的隐变量分布。

##### **3.3 三维重建**

在三维重建中，狄利克雷分布可以用于在三角形面上随机采样点。例如，在代码中：
```python
eps = np.random.dirichlet((0.5, 0.5, 0.5), size=f_it.shape[0])
face_point = (face_vertex * eps[:, :, None]).sum(dim=1)
```
这里，`np.random.dirichlet((0.5, 0.5, 0.5), size=f_it.shape[0])`生成了一个在三角形面上的随机点，这些点用于计算法线损失。

#### **4. 示例**

假设我们有一个三角形面，其顶点为$(0, 0)$、$(1, 0)$和$(0, 1)$。我们希望在这个三角形面上随机采样一个点。可以使用狄利克雷分布：
```python
import numpy as np

# 生成狄利克雷分布的随机样本
alpha = (0.5, 0.5, 0.5)
sample = np.random.dirichlet(alpha)

# 三角形的顶点
vertices = np.array([[0, 0], [1, 0], [0, 1]])

# 计算采样点
point = np.dot(sample, vertices)
print(point)
```
输出可能是：
```
[0.2, 0.3]
```
这个点位于三角形面上，且满足单纯形的条件：所有分量非负且和为1。

#### **5. 总结**

狄利克雷分布是一种定义在单纯形上的概率分布，广泛应用于贝叶斯统计和机器学习中。它通过参数$\boldsymbol{\alpha}$控制分布的形状，可以用于表示多类别分布的概率分布。在三维重建中，狄利克雷分布可以用于在三角形面上随机采样点，从而计算法线损失。



# 原文翻译

**UNISURF：统一神经隐式曲面和辐射场用于多视图重建**  
Michael Oechsle<sup>1,2,3</sup>，Songyou Peng<sup>1,4</sup>，Andreas Geiger<sup>1,2</sup>  
<sup>1</sup>马克斯·普朗克智能系统研究所，图宾根  
<sup>2</sup>图宾根大学  
<sup>3</sup>ETAS GmbH，斯图加特  
<sup>4</sup>苏黎世联邦理工学院  
{firstname.lastname}@tue.mpg.de  

**摘要**  
神经隐式三维表示已成为从多视图图像重建曲面和合成新视图的强大范式。不幸的是，现有方法（如DVR或IDR）需要精确的逐像素物体掩码作为监督。与此同时，神经辐射场彻底改变了新视图合成，但NeRF估计的体积密度无法实现精确的表面重建。我们的关键见解是，隐式曲面模型和辐射场可以以统一的方式表述，从而使用同一模型实现表面和体积渲染。这种统一视角使我们能够开发出更高效的采样过程，并能够在没有输入掩码的情况下重建精确的表面。我们在DTU、BlendedMVS和一个合成室内数据集上进行了比较。实验结果表明，我们的方法在重建质量上优于NeRF，并且在不需要掩码的情况下与IDR表现相当。

## **1. 引言**  

从一组图像中捕捉三维场景的几何和外观是计算机视觉中的一个基石问题。为了实现这一目标，基于坐标的神经模型在过去几年中已成为三维几何和外观重建的强大工具。许多最近的方法采用由神经网络参数化的连续隐式函数作为几何[3, 8, 12, 31, 32, 37, 41, 43, 47, 57]或外观[34, 38, 39, 40, 47, 52, 61]的三维表示。这些神经三维表示在多视图图像的几何重建和新视图合成方面表现出色。除了三维表示的选择（例如，占用场、无符号或有符号距离场）之外，神经隐式多视图重建的一个关键要素是渲染技术。一些工作将隐式曲面表示为等值面，从而从表面上渲染外观[38, 52, 61]，而另一些工作则通过沿视线采样来积分密度[22, 34, 49]。

在现有工作中，表面渲染技术在三维重建方面表现出色[38, 61]。然而，它们需要逐像素的物体掩码作为输入，并且需要适当的网络初始化，因为表面渲染技术仅在曲面与射线相交的局部位置提供梯度信息。直观来说，相对于局部梯度的优化可以看作是对初始神经曲面（通常初始化为球体）的迭代变形过程。额外的约束（如掩码监督）对于收敛到有效的曲面是必要的，具体见图2的说明。由于依赖于掩码，表面渲染方法仅限于物体级别的重建，并且无法扩展到更大的场景。

相反，像NeRF[34]这样的体积渲染方法在新视图合成方面表现出色，包括对于更大场景的合成。然而，从底层体积密度中提取的曲面通常不光滑，并且由于辐射场表示的灵活性，在存在歧义时无法充分约束三维几何，从而导致伪影，具体见图3。

**贡献**：在本文中，我们提出了**UNISURF（UNIfied Neural Implicit SUrface and Radiance Fields）**，这是一个隐式曲面和辐射场的统一框架，目标是从一组RGB图像中重建实体（即非透明）物体。我们的框架结合了表面渲染和体积渲染的优点，能够在没有掩码的情况下从多视图图像中重建精确的几何结构。通过恢复隐式曲面，我们能够在优化过程中逐渐缩小体积渲染的采样区域。在优化的早期阶段，使用较大的采样区域可以捕捉粗略的几何结构并解决歧义。在后期阶段，我们将采样点靠近曲面，从而提高重建精度。我们证明了我们的方法能够在没有掩码监督的情况下在DTU MVS数据集[1]上捕捉精确的几何结构，并取得了与使用强掩码监督的最先进的隐式神经重建方法（如IDR[61]）相当的结果。此外，我们还在BlendedMVS数据集[60]的场景以及SceneNet[29]的合成室内场景上展示了我们的方法。代码可在以下链接获取：[https://github.com/autonomousvision/unisurf](https://github.com/autonomousvision/unisurf)。

## **2. 相关工作**  

在本节中，我们首先讨论多视图图像三维重建领域的相关工作。接下来，我们概述神经隐式表示以及可微渲染方面的最新进展。

**多视图图像的三维重建**：从多张图像中重建三维几何是一个长期存在的计算机视觉问题[14]。在深度学习时代之前，经典的多视图立体（MVS）方法[2, 4, 5, 7, 20, 48, 50, 51]主要集中在跨视图匹配特征[4, 48]或用体素网格表示形状[2, 5, 7, 20, 27, 42, 50, 54, 55]。前者方法通常需要复杂的流程，包括额外的步骤，如融合深度信息[9, 30]和网格化[18, 19]，而后者由于立方级的内存需求，分辨率受限。相比之下，用于三维重建的神经隐式表示不会受到离散化伪影的影响，因为它们通过神经网络的连续输出表示曲面。最近的基于学习的MVS方法试图替换经典MVS流程的某些部分。例如，一些工作学习匹配二维特征[15, 21, 26, 56, 62]、融合深度图[11, 46]或从多视图图像中推断深度图[16, 58, 59]。与这些基于学习的MVS方法不同，我们的方法仅需要在优化过程中进行弱二维监督。此外，我们的方法能够产生高质量的三维几何，并合成逼真且一致的新视图。

**神经隐式表示**：近年来，神经隐式函数已成为表示三维几何[3, 8, 12, 31, 32, 37, 41, 43, 47, 57]和外观[22, 24, 34, 38, 39, 40, 47, 49, 52]的有效方式，因为它们能够连续地表示三维内容，且无需离散化，同时具有较小的内存占用。这些方法中的大多数需要三维监督。然而，最近的一些工作[23, 34, 38, 52, 61]展示了如何通过可微渲染直接从图像进行训练。我们将这些方法分为两类：表面渲染和体积渲染。表面渲染方法（如DVR[38]和IDR[61]）直接在物体表面上确定辐射，并使用隐式梯度提供可微渲染公式，从而可以从多视图图像中优化神经隐式曲面。在视图方向上进行条件约束使得IDR能够捕捉到高水平的细节，即使在非朗伯表面的情况下也是如此。然而，DVR和IDR都需要所有视图的逐像素物体掩码作为输入。相比之下，我们的方法无需掩码即可实现类似的重建。NeRF[34]及其后续工作[6, 28, 35, 36, 44, 45, 49, 53, 63]通过沿射线学习辐射场的alpha合成来使用体积渲染。这种方法在新视图合成方面表现出色，并且不需要掩码监督。然而，从体积密度中恢复的三维几何远非令人满意，具体见图3。一些后续工作（如Neural Body[44]、D-NeRF[45]和NeRD[6]）使用NeRF的体积密度提取网格，但没有一个考虑直接优化曲面。与这些工作不同，我们的目标是捕捉精确的几何结构，并提出一种在极限情况下接近表面渲染的体积渲染公式。


## **3. 背景**  

从多视图图像中学习神经隐式三维表示的两个主要要素是三维表示以及将三维表示与二维观测联系起来的渲染技术。本节提供了我们在本文中统一的隐式曲面和体积辐射表示的相关背景，用于表示实体（非透明）物体和场景。

**隐式曲面模型**：占用网络[31, 38]将曲面表示为二值占用分类器的决策边界，该分类器由神经网络参数化：

$$
o_\theta(x): \mathbb{R}^3 \to [0, 1] \quad (1)
$$

其中，$x \in \mathbb{R}^3$是一个三维点，$\theta$是模型参数。曲面被定义为所有三维点的集合，其中占用概率为0.5：$S = \{x_s \mid o_\theta(x_s) = 0.5\}$。为了为曲面上的每个三维点$x_s$分配颜色，可以联合学习颜色场$c_\theta(x_s)$。特定像素/射线$r$的颜色预测如下：

$$
\hat{C}(r) = c_\theta(x_s) \quad (2)
$$

其中，$x_s$是通过沿射线$r$进行根查找得到的（具体细节见[38]）。占用场$o_\theta$和颜色场$c_\theta$的参数$\theta$通过优化重建损失并使用梯度下降来确定，具体见[24, 38, 61]。尽管表面渲染可以精确估计几何和外观，但现有方法严重依赖于物体掩码的监督，因为表面渲染方法只能推理与曲面相交的射线。

**体积辐射模型**：与隐式曲面模型不同，NeRF[34]将场景表示为彩色体积密度，并通过沿射线的alpha混合积分辐射[25, 34]。具体来说，NeRF使用神经网络将三维位置$x \in \mathbb{R}^3$和视图方向$d \in \mathbb{R}^3$映射到体积密度$\sigma_\theta(x) \in \mathbb{R}^+$和颜色值$c_\theta(x, d) \in \mathbb{R}^3$。在视图方向$d$上进行条件约束可以模拟视图依赖效应（如镜面反射）[34, 40]，并且在朗伯假设不成立时提高重建质量[61]。假设相机中心位置为$o$，给定射线$r = o + td$上的$N$个样本$\{x_i\}$，NeRF使用数值积分近似像素/射线$r$的颜色：

$$
\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(-\sigma_\theta(x_i) \delta_i)) c_\theta(x_i, d) \quad (3)
$$

其中，$T_i$是沿射线的累积透射率：

$$
T_i = \exp\left(-\sum_{j<i} \sigma_\theta(x_j) \delta_j\right) \quad (4)
$$

$\delta_i = |x_{i+1} - x_i|$是相邻样本之间的距离。由于公式（3）是可微的，因此可以通过优化重建损失来估计密度场$\sigma_\theta$和颜色场$c_\theta$的参数$\theta$（具体见[34]）。尽管NeRF不需要物体掩码进行训练，但从体积密度中提取场景几何需要仔细调整密度阈值，并且由于密度场中的歧义，会导致伪影（见图3）。

## **4. 方法**  
我们现在描述我们的主要贡献。与NeRF不同，NeRF也适用于非实体场景（例如雾、烟），我们将重点放在可以用三维曲面和视图依赖的表面颜色表示的实体物体上。我们的方法利用了体积辐射表示的强大能力来学习粗略的场景结构，而无需掩码监督，同时表面渲染作为归纳偏差，通过一组精确的三维曲面来表示物体，从而实现精确的重建。

### **4.1 统一表面和体积渲染**  
我们首先指出，公式（3）可以重写为：

$$
\hat{C}(r) = \sum_{i=1}^{N} \alpha_i(x_i) \prod_{j<i} (1 - \alpha_j(x_j)) c(x_i, d) \quad (5)
$$

其中，alpha值$\alpha_i(x) = 1 - \exp(-\sigma(x) \delta_i)$。假设实体物体，$\alpha$成为一个离散的占用指示变量$o \in \{0, 1\}$，分别在自由空间取值为0，在占据空间取值为1：

$$
\hat{C}(r) = \sum_{i=1}^{N} o(x_i) \prod_{j<i} (1 - o(x_j)) c(x_i, d) \quad (6)
$$

我们认识到这个表达式是实体物体的图像形成模型[55]，其中项$o(x_i) \prod_{j<i} (1 - o(x_j))$在射线$r$上的第一个占据样本$x_i$处评估为1，而在所有其他样本处评估为0。$\prod_{j<i} (1 - o(x_j))$是一个可见性指标，如果在样本$x_i$之前没有占据样本$x_j$（$j < i$），则为1。因此，$\hat{C}(r)$取射线$r$上的第一个占据样本的颜色$c(x_i, d)$。

为了统一隐式曲面和体积辐射模型，我们直接通过连续的占用场$o_\theta$（公式1）来参数化$o$，而不是预测体积密度$\sigma$。按照[61]的做法，我们将颜色场$c_\theta$在表面法线$n$和几何网络的特征向量$h$上进行条件约束，这在[61]中被证明对于隐式曲面的情况非常有效。重要的是，我们统一的公式允许同时进行体积渲染和表面渲染：

$$
\hat{C}_v(r) = \sum_{i=1}^{N} o_\theta(x_i) \prod_{j<i} (1 - o_\theta(x_j)) c_\theta(x_i, n_i, h_i, d) \quad (7)
$$

$$
\hat{C}_s(r) = c_\theta(x_s, n_s, h_s, d) \quad (8)
$$

其中，$x_s$是通过沿射线$r$进行根查找得到的，$n_s$和$h_s$分别是$x_s$处的法线和几何特征。需要注意的是，$x_s$依赖于占用场$o_\theta$，但为了清晰起见，这里省略了这种依赖性。更多细节请参考补充材料。

这种统一公式的优势在于，它允许直接在表面上渲染，也可以在整个体积中渲染，从而在优化过程中逐步消除歧义。正如我们的实验所证明的，结合两者对于在没有掩码监督的情况下获得精确重建至关重要。能够通过根查找快速恢复表面$S$，使得体积渲染更加高效，逐步聚焦并优化物体表面，我们将在第4.3节中描述这一点。此外，表面渲染能够更快地合成新视图，如图5所示。

### **4.2 损失函数**  
我们优化以下正则化的损失函数：

$$
L = L_{\text{rec}} + \lambda L_{\text{reg}} \quad (9)
$$

其中，$L_{\text{rec}}$是$\ell_1$重建损失，$L_{\text{reg}}$是$\ell_2$表面正则化，它鼓励曲面上的点$x_s$与其邻域内采样点的法线相似：

$$
L_{\text{rec}} = \sum_{r \in R} \|\hat{C}_v(r) - C(r)\|_1 \quad (10)
$$

$$
L_{\text{reg}} = \sum_{x_s \in S} \|n(x_s) - n(x_s + \epsilon)\|_2 \quad (11)
$$

其中，$R$表示小批量中的所有像素/射线集合，$S$是对应的曲面点集合，$C(r)$是像素/射线$r$的观测颜色，$\epsilon$是一个小的随机均匀三维扰动。$x_s$处的法线由下式给出：

$$
n(x_s) = \frac{\nabla_{x_s} o_\theta(x_s)}{\|\nabla_{x_s} o_\theta(x_s)\|_2} \quad (12)
$$

这可以通过双重反向传播计算得到[38]。

### **4.3 优化**  

隐式曲面模型的关键假设是，只有射线与曲面的第一个交点区域对渲染方程有贡献。然而，在优化的早期阶段，这一假设并不成立，因为曲面尚未定义清晰。因此，现有方法[38, 61]需要强掩码监督。相反，在优化的后期阶段，关于近似曲面的知识对于在评估体积渲染方程（公式7）时绘制信息丰富的样本是有价值的。因此，我们采用一个单调递减的采样间隔来绘制体积渲染期间的样本，如图4所示。换句话说，在优化的早期阶段，样本$\{x_i\}$覆盖整个优化体积，有效地利用体积渲染引导重建过程。在优化的后期阶段，样本$\{x_i\}$被绘制在估计的曲面附近。由于曲面可以通过占用场$o_\theta$直接通过根查找得到[38]，这消除了现有方法（如NeRF）中需要的分层两阶段采样。我们的实验表明，这种程序对于估计精确几何结构特别有效，同时允许在优化的早期阶段解决歧义。更正式地说，假设$x_s = o + t_s d$，我们通过在以$t_s$为中心的区间$[t_s - \Delta, t_s + \Delta]$内使用分层采样来获得样本$x_i = o + t_i d$：

$$
t_i \sim U\left(t_s + \frac{2i - 2}{N - 1} \Delta, t_s + \frac{2i}{N - 1} \Delta\right) \quad (13)
$$

在训练过程中，我们从一个较大的采样区间$\Delta_{\text{max}}$开始，并逐渐减小$\Delta$，以便更精确地采样和优化曲面，具体衰减公式如下：

$$
\Delta_k = \max(\Delta_{\text{max}} \exp(-k \beta), \Delta_{\text{min}}) \quad (14)
$$

其中，$k$表示迭代次数，$\beta$是超参数。实际上，可以证明，当$\Delta \to 0$且$N \to \infty$时，体积渲染（公式7）确实接近表面渲染（公式8）：$\hat{C}_v(r) \to \hat{C}_s(r)$。补充材料中提供了这一极限的正式证明。正如我们的实验所证明的，这种衰减策略对于捕捉精细几何结构至关重要，因为它结合了优化初期对大且不确定的体积的体积渲染，以及优化后期对表面的表面渲染。为了减少自由空间伪影，我们将这些样本与在相机和曲面之间随机采样的点结合起来。对于没有曲面交点的射线，我们在整个射线上使用分层采样。

### **4.4 实现细节**  

**架构**：与Yariv等人[61]类似，我们使用一个8层的MLP（多层感知机）来表示占用场$o_\theta$，其中激活函数为Softplus，隐藏层维度为256。我们将网络初始化为一个球体[13]。相比之下，辐射场$c_\theta$则使用一个4层的ReLU MLP进行参数化。我们使用傅里叶特征[34]对三维位置$x$和视图方向$d$进行编码，编码层级为$k$。通过实验，我们发现对于三维位置$x$，$k=6$效果最佳；对于视图方向$d$，$k=4$效果最佳。  

**优化**：在所有实验中，我们针对单个场景的多视图图像拟合模型。在优化模型参数时，我们首先随机采样一个视图，然后根据相机的内参和外参从该视图中随机采样$M$个像素/射线$R$。接下来，我们渲染所有射线以计算公式（9）中的损失函数。对于根查找，我们使用256个均匀采样点，并应用8步割线法[31]。在我们的渲染过程中，我们在区间内使用$N=64$个查询点，并在相机与区间下界之间的自由空间中使用32个点。区间的衰减参数为$\beta = 1.5 \times 10^{-5}$，$\Delta_{\text{min}} = 0.05$，$\Delta_{\text{max}} = 1.0$。我们使用Adam优化器，学习率为0.0001，并在每次迭代中优化$M=1024$个像素，分别在200k次和400k次迭代后进行两次衰减。总共，我们训练模型450k次迭代。  

**推理**：我们的方法支持推断三维形状以及合成新视图图像。对于图像合成，我们可以以两种不同的方式渲染我们的表示：体积渲染或表面渲染。如图5所示，这两种渲染方法都能得到类似的结果。然而，我们观察到表面渲染的速度比体积渲染更快。为了提取网格，我们应用了多分辨率等值面提取（MISE）算法[31]。我们使用$64^3$作为初始分辨率，并在3个步骤中进行上采样，而不进行基于梯度的细化。

## **5. 实验评估**  
我们在多视图三维重建方面进行了实验，以评估我们的方法。首先，我们在广泛使用的DTU MVS数据集[17]上对我们的方法与现有方法（IDR[61]、NeRF[34]、COLMAP[48]）进行了定性和定量比较。其次，我们展示了BlendedMVS数据集[60]的样本以及SceneNet数据集[29]的合成场景渲染的定性比较。最后，我们通过消融研究分析了我们的渲染过程和损失函数。在补充材料中，我们提供了LLFF数据集[33]的结果。

### **5.1 基线**
为了验证我们方法的有效性，我们将我们的方法与以下三种基线进行了比较。

**COLMAP [48]**：我们将COLMAP[48]视为经典的MVS基线，因为它在多视图重建方面表现出色，并且在相关工作中被广泛使用[38, 61]。我们使用筛选泊松曲面重建（sPSR）[19]从COLMAP的输出中重建网格。按照[38, 61]的做法，我们展示了对于最佳修剪参数7以及能够生成封闭网格（修剪参数0）的结果。

**NeRF [34]**：尽管NeRF的目标是新视图合成，但其体积密度可以提取几何结构。为了从NeRF中提取网格，我们定义了一个密度阈值50。我们在补充材料中验证了这一选择。

**IDR [61]**：IDR是神经隐式曲面的最先进的多视图重建方法。IDR能够以令人印象深刻的细节重建曲面，并且能够处理镜面反射表面，但需要输入掩码。我们没有将DVR[38]纳入比较，因为IDR的视图依赖建模已被证明优于DVR（见[61]）。


### **5.2 数据集**

**DTU MVS数据集 [17]**：该数据集包含49到64张分辨率为1200×1600的图像，以及所有视图的相机内参和外参。该数据集包含不同形状和外观的物体。非朗伯反射效应使得一些物体特别具有挑战性。对于每个扫描，提供了真实三维形状以及官方评估程序。与之前的工作[38, 61]类似，我们使用评估脚本的“Surface”方法，并在使用相应掩码清理的网格上评估所有方法。官方评估程序计算预测形状和数据集中提供的真实形状之间的采样点的Chamfer距离。对于评估IDR[61]，我们使用了IDR作者提供的所有图像的逐像素掩码。

**BlendedMVS数据集 [60]**：该数据集是一个大规模数据集，包含多视图图像以及相应的相机外参和内参。我们使用BlendedMVS低分辨率集的样本，图像分辨率为768×576。这些样本包含24到64张不同视角的未掩码图像。我们将场景定义为物体位于中心，最近的相机靠近单位球体。

**SceneNet数据集 [29]**：为了测试我们的模型在复杂室内场景上的表现，我们从[29]中选取了两个场景进行评估。使用BlenderProc[10]渲染包含多个物体的部分场景的图像。第一个场景是一个包含床、灯和床头柜的卧室场景；另一个场景是一个包含沙发、窗帘和圆桌的客厅场景。我们分别使用了83张和40张图像。

### **5.3 DTU数据集上的比较**

在表1中，我们在DTU MVS数据集上对我们的方法与基线进行了定量比较。虽然使用修剪参数$\zeta = 7$的COLMAP基线在Chamfer距离上表现最佳，但它生成的网格不完整，存在非封闭的表面。我们的方法在没有强掩码监督的情况下，与最先进的神经隐式模型IDR表现相当。NeRF和COLMAP（$\zeta = 0$）也不使用输入掩码，但在Chamfer距离上的表现较差。

在图6中，我们展示了我们的方法和基线的定性结果。虽然COLMAP能够提供详细的重建，但由于修剪，它会导致不完整的几何结构。对于NeRF，重建的表面存在孔洞和噪声伪影。相比之下，我们的方法和IDR（使用输入掩码）能够产生具有高质量细节的精确表面。我们特别指出，我们的模型能够准确捕捉场景的整体空间布局，同时也能捕捉到几何细节，例如头骨的牙齿和其他表面细节。

### **5.4 BlendedMVS和SceneNet数据集上的比较**

为了展示我们模型在更多样化场景中的能力，我们使用了BlendedMVS数据集的样本以及SceneNet中的室内场景。由于这些场景没有提供物体掩码，因此我们仅将COLMAP和NeRF作为基线方法进行比较。尽管我们尝试运行IDR，但这些场景未能收敛，导致输出退化。对于包含复杂背景的场景，我们使用一个背景模型来学习捕获我们感兴趣区域之外的外观信息，更多细节请参考补充材料。

我们的定性结果（见图8）表明，与现有的隐式曲面模型不同，我们的方法能够为包含多个物体和背景的复杂场景重建出合理的几何结构。虽然COLMAP在室内场景中表现出色，但在均匀颜色区域（例如第二行中的圆桌）会出现伪影。对于BlendedMVS实验，NeRF能够推理出整体空间结构，但其表面细节不够准确，且包含显著的噪声，与UNISURF相比差距较大。更多结果可以在补充材料中找到。

### **5.5 消融研究**

我们最终探讨了渲染设计选择的影响，并展示了损失函数的消融研究。

**渲染过程**：通过比较图7中的不同变体，我们论证了我们对渲染过程的选择。首先，我们考虑了一个仅在优化过程中使用表面渲染的基线（SR）。我们对表面颜色使用$\ell_1$重建损失，并通过隐式微分进行反向传播[38]。第二个基线使用均匀体积渲染，共采样96个查询点（均匀VR）。第三个基线使用NeRF中的层次体积采样（HVR）[34]，共采样64+64个点来查询占用场。虽然表面渲染无法收敛，但均匀VR和HVR会导致形状过于平滑且膨胀，丢失细节。这表明，我们提出的统一模型相比于基线方法能够实现更精确的重建。

**损失函数**：在图7中，我们还展示了我们表面正则化项（公式9）的消融研究。如果没有这个正则化项，表面在平坦和模糊区域（例如桌面）会变得不够平滑。这种正则化项对于那些观测频率较低的区域特别有用，因为它引入了对平滑表面的归纳偏差。


## **6. 讨论与结论**

本工作提出了UNISURF，这是一个用于从多视图图像中捕捉高质量隐式曲面几何的隐式曲面和辐射场的统一表述，无需输入掩码。我们相信，神经隐式曲面和先进的可微渲染技术将在未来的三维重建方法中发挥关键作用。我们的统一表述为在比以往更一般的场景中优化隐式曲面提供了一种可能。

**局限性**：按照设计，我们的模型仅限于表示实体、非透明的表面。过曝和纹理较少的区域也是导致不准确性和表面不平滑的限制因素。此外，在图像中很少被看到的区域，重建精度较低。更多细节请参考补充材料。在未来的工作中，为了从很少被看到和纹理较少的区域中解决歧义，需要引入先验知识以进行重建。虽然我们在优化过程中引入了显式的平滑先验，但学习一个能够捕捉物体规律和不确定性的概率神经表面模型将有助于解决歧义，从而实现更准确的重建。

**致谢**：本研究得到了NVIDIA研究基金、ERC启动基金LEGO-3D（850533）以及DFG EXC编号2064/1——项目编号390727645的支持。Songyou Peng得到了马克斯·普朗克ETH学习系统中心的支持。
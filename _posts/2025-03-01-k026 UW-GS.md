---
layout: mypost
title: k026 UW-GS, Distractor-Aware 3D Gaussian Splatting for Enhanced Underwater Scene Reconstruction
categories: [3DGS, 水下]
---

如果有研究水下场景重建的研究者，可以通过邮箱zhangwenniu@163.com联系我，我目前创建了一个水下场景重建的交流群，欢迎感兴趣的研究者加入。

# 论文链接

- [论文链接](https://arxiv.org/abs/2410.01517)

- [Github链接](https://github.com/WangHaoran16/UW-GS)


# 重点难点介绍

## 论文重点内容

### 1. **研究目标**

本文提出了一种针对水下场景的新型3D高斯点绘制方法（UW-GS），旨在解决传统3D高斯点绘制（3DGS）在水下环境中表现不佳的问题。水下场景的特点包括：

- **光吸收和散射**：导致颜色失真、对比度下降。

- **动态物体干扰**：如游动的鱼和漂浮物，会引入重建伪影。

- **远距离物体模糊**：由于光衰减，远距离区域的细节难以重建。

### 2. **核心贡献**

本文的主要贡献包括：

1. **新型颜色表现模型**  

   通过多层感知机（MLP）估计水下介质参数（如衰减因子、背景光等），并根据这些参数调整高斯点的颜色，从而更好地模拟水下场景的颜色变化。这一模型能够有效解决因光衰减和散射导致的颜色失真问题。

2. **基于物理的密度控制策略**  

   针对远距离物体因光衰减导致的高斯点稀疏问题，本文提出了一种新的密度控制策略。通过补偿光衰减对梯度计算的影响，使得高斯点能够在远距离区域更有效地分裂和克隆，从而提高重建质量，减少模糊伪影。

3. **二值运动掩码（BMM）**  

   为处理动态物体的干扰，本文引入了二值运动掩码。在训练过程中，通过检测和排除动态物体的影响，显著提高了重建结果的鲁棒性，避免了动态物体引入的伪影。

4. **新的损失函数**  

   结合散射介质特性、伪深度图和像素级二值运动掩码，设计了一种新的损失函数，用于优化模型参数，进一步提高重建精度。

5. **新的水下数据集（S-UW）**
  
   为了验证方法的有效性，作者收集了一个新的水下数据集，包含动态物体掩码，为后续研究提供了宝贵的资源。

### 3. **实验验证**

- **数据集**：在SeaThru-NeRF、IW和S-UW三个数据集上验证了UW-GS的有效性。

- **性能指标**：使用PSNR、SSIM和LPIPS三个指标评估重建质量。

- **结果**：UW-GS在多个场景中均优于现有的3DGS和SeaThru-NeRF方法，平均PSNR提升高达1.26dB，证明了其在水下场景重建中的优越性。



## 论文难点内容

### 1. **颜色表现模型的复杂性**

- **难点**：如何准确估计水下介质参数（如衰减因子、背景光等），并将其与高斯点的颜色表现相结合。

- **解决方案**：通过多层感知机（MLP）估计这些参数，并利用仿射变换调整高斯点的颜色。这一过程需要精确的物理模型支持，同时需要大量训练数据以确保模型的泛化能力。

### 2. **基于物理的密度控制策略**

- **难点**：在水下环境中，光的衰减会导致高斯点的梯度信息丢失，使得远距离区域的高斯点难以分裂和克隆，从而导致重建结果模糊。

- **解决方案**：通过补偿光衰减对梯度计算的影响，调整高斯点的分裂和克隆策略。这一策略需要精确估计光衰减因子，并在训练过程中动态调整密度控制策略。

### 3. **动态物体的处理**

- **难点**：水下场景中的动态物体（如游动的鱼和漂浮物）会引入重建伪影，如何在训练过程中排除这些动态物体的影响是一个挑战。

- **解决方案**：引入二值运动掩码（BMM），通过检测动态物体并将其从训练数据中排除，从而提高重建结果的鲁棒性。这一方法需要精确的动态物体检测算法，并在训练过程中动态调整掩码。

### 4. **损失函数的设计**

- **难点**：如何设计一个能够同时考虑散射介质特性、伪深度图和动态物体影响的损失函数，以优化模型参数并提高重建精度。

- **解决方案**：结合散射介质特性、伪深度图和像素级二值运动掩码，设计了一个综合损失函数。这一损失函数需要在训练过程中动态调整权重，以平衡不同因素的影响。

### 5. **浅水场景的挑战**

- **难点**：在浅水场景中，水面光线的干扰较大，导致光衰减和散射的影响更加复杂，模型的性能提升不如深水场景明显。

- **解决方案**：尽管UW-GS在浅水场景中表现良好，但作者指出，浅水场景的复杂性仍然是一个需要进一步研究的方向。未来的工作可能需要引入更多的物理模型和数据增强方法，以进一步提高模型在浅水场景中的性能。



## 总结

本文通过提出一种新型的3D高斯点绘制方法（UW-GS），解决了传统方法在水下场景中的不足。通过引入新的颜色表现模型、基于物理的密度控制策略和二值运动掩码，显著提高了水下场景重建的质量。尽管在浅水场景中仍存在挑战，但UW-GS为水下场景重建领域提供了新的思路和方法。

# 详细介绍

UW-GS: Distractor-Aware 3D Gaussian Splatting for Enhanced Underwater Scene Reconstruction

### **1. 论文信息**
- **标题**: UW-GS: Distractor-Aware 3D Gaussian Splatting for Enhanced Underwater Scene Reconstruction  
- **作者**: Haoran Wang, Nantheera Anantrasirichai, Fan Zhang, David Bull  
- **单位**: School of Computer Science, University of Bristol, Bristol, UK  
- **联系方式**: {yp22378,n.anantrasirichai,fan.zhang,dave.bull}@bristol.ac.uk  
- **代码地址**: [https://github.com/WangHaoran16/UW-GS](https://github.com/WangHaoran16/UW-GS)  
- **论文版本**: arXiv:2410.01517v2 cs.CV 23 Dec 2024

### **2. 研究背景**

水下场景重建在海洋考古、地质勘探、海洋科学等领域具有重要应用价值。然而，水下环境复杂，光的传播受到吸收和散射的影响，导致图像对比度下降、颜色失真。此外，水下场景中还存在动态物体（如游动的鱼和漂浮物），这些物体会给3D重建带来额外的挑战。传统的3D高斯点绘制（3DGS）方法在水下场景中表现不佳，主要原因是：

1. 3DGS假设场景处于透明介质中，无法有效处理水下的光衰减和散射。

2. 动态物体的存在会导致重建结果中出现伪影。

3. 远距离物体的颜色信息因光衰减而变得稀疏，导致重建结果模糊。

### **3. 研究动机**

为了解决上述问题，本文提出了一种针对水下场景的新型3D高斯点绘制方法（UW-GS），通过改进颜色表现模型、密度控制策略和动态内容处理，显著提升了水下场景重建的质量。

### **4. 方法介绍**

#### **4.1 问题定义**

3D高斯点绘制（3DGS）是一种基于点云的3D场景表示方法，通过一组高斯点（Gaussians）来表示场景。每个高斯点由中心位置（µ）、协方差矩阵（Σ）、透明度（α）和视图依赖的颜色（c）组成。在渲染时，这些高斯点被投影到2D空间，通过α混合生成最终图像颜色。

在水下环境中，由于光的吸收和散射，传统的3DGS方法无法有效表示场景。本文通过引入新的颜色表现模型、基于物理的密度控制策略和二值运动掩码（BMM），解决了这些问题。

#### **4.2 新的颜色表现模型**

传统的3DGS方法使用球谐函数（Spherical Harmonics, SH）来表示视图依赖的颜色，但这种方法假设场景处于透明介质中，无法处理水下的光衰减和散射。为了解决这一问题，本文提出了一个新的颜色表现模型，通过多层感知机（MLP）估计水介质参数，并根据这些参数调整高斯点的颜色。

具体来说，MLP以高斯点的深度（z）和视图方向（θ）为输入，估计以下参数：

- **衰减因子（TD）**：表示直接光的衰减。

- **背景光因子（TB）**：表示散射光的影响。

- **介质系数（βd, βb）**：用于调节衰减和散射的影响。

- **背景光颜色（b）**：表示无限远处的背景光颜色。

通过这些参数，可以计算出修正后的高斯点颜色（cm）：

$$ cm = TD \cdot c + (1 - TB) \cdot b $$

这种颜色表现模型能够根据距离调整物体的颜色，从而更好地模拟水下场景的视觉效果。

#### **4.3 基于物理的密度控制策略**

在传统的3DGS方法中，高斯点的分布是通过2D位置梯度动态调整的。然而，在水下环境中，由于光的衰减，远距离区域的高斯点颜色梯度会变得非常小，导致这些区域的高斯点无法有效分裂和克隆，从而出现模糊伪影。

为了解决这一问题，本文提出了一种基于物理的密度控制策略。具体来说，通过补偿衰减因子（TD）对梯度计算的影响，使得高斯点能够在远距离区域更有效地分裂和克隆。此外，本文还引入了像素覆盖数量的考虑，避免仅依赖视图数量来计算平均2D位置梯度，从而进一步提高了远距离区域的重建质量。

#### **4.4 二值运动掩码（BMM）**

水下场景中动态物体的存在会引入重建伪影，影响重建结果的质量。为了解决这一问题，本文引入了二值运动掩码（BMM），通过检测和排除动态物体的影响，提高重建结果的鲁棒性。

BMM的计算过程如下：

1. **残差计算**：计算每个像素的残差（ϵ），即渲染图像与真实图像之间的差异。

2. **阈值判断**：根据残差的分布，设置阈值（Tϵ），将残差小于阈值的像素视为静态物体（inliers），其余像素视为动态物体（outliers）。

3. **空间平滑**：通过3×3扩散核（B）对掩码进行空间平滑处理，避免高频噪声的影响。

4. **局部分类**：根据每个8×8块的16×16邻域的平均值，进一步分类像素，确保掩码的准确性。

最终，BMM通过结合上述三种方法，生成一个鲁棒的掩码，用于排除动态物体的影响。

#### **4.5 损失函数**

为了优化模型参数，本文设计了一个综合损失函数，包括以下三个部分：

1. **重建损失（LRec）**：结合L1损失和结构相似性（D-SSIM）损失，用于优化重建图像的质量。

2. **深度监督损失（Lda）**：通过伪深度图（由DepthAnything生成）监督高斯点的深度信息，减少浮动物体（floaters）的影响。

3. **灰度世界假设损失（Lg）**：基于灰度世界假设，约束场景中物体的颜色分布，确保模型能够合理估计介质参数。

综合损失函数定义为：
$$ L = LRec + Lda + Lg $$

### **5. 实验验证**

#### **5.1 数据集**

本文在以下三个数据集上验证了UW-GS的有效性：

1. **SeaThru-NeRF**：包含四个不同海域的图像序列，经过白平衡增强处理。

2. **IW数据集**：包含中等深度水下场景的图像，提供动态物体掩码。

3. **S-UW数据集**：作者新收集的浅水水下视频数据集，包含四个不同场景，提供动态物体掩码。

#### **5.2 实现细节**

- **框架**：基于PyTorch实现。

- **初始化**：使用COLMAP初始化点云和相机位置。

- **训练**：在单个RTX 3090 GPU上进行训练，共15,000次迭代。前1,000次迭代仅优化深度损失，后续迭代同时优化高斯点属性和介质MLP。

#### **5.3 基线方法**

本文将UW-GS与以下基线方法进行了比较：

1. **Instant-NGP**：一种快速的神经图形原语方法。

2. **SeaThru-NeRF**：一种针对散射介质的神经辐射场方法。

3. **3DGS**：传统的3D高斯点绘制方法。

#### **5.4 评估指标**

使用以下三个指标评估重建质量：

1. **PSNR（峰值信噪比）**：越高越好。

2. **SSIM（结构相似性）**：越高越好。

3. **LPIPS（感知相似性）**：越低越好。

#### **5.5 实验结果**

- **SeaThru-NeRF数据集**：UW-GS在PSNR、SSIM和LPIPS指标上均优于基线方法，平均PSNR提升2.09dB和2.70dB（相比3DGS和SeaThru-NeRF）。

- **S-UW数据集**：UW-GS在多个场景中表现最佳，平均PSNR提升0.43dB和2.96dB（相比3DGS和SeaThru-NeRF）。

此外，UW-GS在处理动态物体方面表现出色，能够有效排除动态物体的影响，减少伪影。

### **6. 局限性**

尽管UW-GS在水下场景重建中取得了显著效果，但在浅水场景中，由于水面光线干扰较大，其性能提升不如深水场景明显。此外，BMM可能会不完美地分类动态物体，导致静态区域的重建质量略有下降。

### **7. 结论**

本文提出了一种新型的3D高斯点绘制方法（UW-GS），通过改进颜色表现模型、密度控制策略和动态内容处理，显著提升了水下场景重建的质量。实验结果表明，UW-GS在多个水下数据集上均优于现有的方法，为水下环境探索和研究提供了新的技术手段。

### **8. 未来工作**

未来的研究方向可能包括：

1. 进一步优化浅水场景的重建性能。

2. 探索更多的物理模型和数据增强方法，以提高模型的泛化能力。

3. 将UW-GS应用于更广泛的水下应用场景，如实时水下导航和动态场景重建。



# 方法部分详解

## **3. 方法（Method）**

### **3.1 问题定义（Problem Formulation）**

UW-GS 是基于 3D 高斯点绘制（3DGS）的一种改进方法，旨在解决水下场景重建中的挑战。3DGS 是一种点云表示方法，通过一组高斯点（Gaussians）来表示 3D 场景。每个高斯点由以下参数定义：

- **中心位置** $ \mu $  

- **3D 协方差矩阵** $ \Sigma $  

- **透明度（Opacity）** $ \alpha $  

- **视图依赖的颜色（View-dependent color）** $ c $，通常用球谐函数（Spherical Harmonics, SH）表示。

在渲染时，这些高斯点被投影到 2D 空间，通过 α 混合生成最终图像颜色：

$$ C = \sum_{i=1}^{N} \alpha_i c_i \prod_{j=1}^{i-1} (1 - \alpha_j) $$

其中 $ N $ 是高斯点的数量。

然而，传统的 3DGS 方法在水下环境中表现不佳，主要原因是：

1. 水下光的吸收和散射会导致颜色失真和对比度下降。

2. 动态物体（如游动的鱼）会引入重建伪影。

3. 远距离物体的颜色信息因光衰减而变得稀疏，导致重建结果模糊。

为了解决这些问题，UW-GS 引入了以下创新模块：

### **3.2 新的颜色表现模型（Color Appearance Model）**

传统的 3DGS 方法使用球谐函数（SH）来表示高斯点的颜色，这种方法假设场景处于透明介质中，无法处理水下的光衰减和散射。UW-GS 提出了一种新的颜色表现模型，通过多层感知机（MLP）估计水介质参数，并根据这些参数调整高斯点的颜色。

具体来说，MLP 以高斯点的深度（$ z $）和视图方向（$ \theta $）为输入，估计以下参数：

- **衰减因子（TD）**：表示直接光的衰减。

- **背景光因子（TB）**：表示散射光的影响。

- **介质系数（βd, βb）**：用于调节衰减和散射的影响。

- **背景光颜色（b）**：表示无限远处的背景光颜色。

通过这些参数，可以计算出修正后的高斯点颜色（$ c_m $）：

$$ c_m = TD \cdot c + (1 - TB) \cdot b $$

这种颜色表现模型能够根据距离调整物体的颜色，从而更好地模拟水下场景的视觉效果。例如，随着物体距离相机的增加，其颜色会逐渐接近背景光颜色，同时对比度会降低。

### **3.3 基于物理的密度控制策略（Physics-based Density Control）**

在传统的 3DGS 方法中，高斯点的分布是通过 2D 位置梯度动态调整的。然而，在水下环境中，由于光的衰减，远距离区域的高斯点颜色梯度会变得非常小，导致这些区域的高斯点无法有效分裂和克隆，从而出现模糊伪影。

为了解决这一问题，UW-GS 提出了一种基于物理的密度控制策略。具体来说，通过补偿衰减因子（TD）对梯度计算的影响，使得高斯点能够在远距离区域更有效地分裂和克隆。

具体实现如下：

1. **梯度补偿**：在计算 2D 位置梯度时，引入衰减因子的逆（$ \frac{1}{TD} $），以补偿光衰减对梯度的影响：

   $$ \delta L_{\text{Rec}} = \frac{\delta L_{\text{Rec}}}{TD} $$

   这样可以确保远距离区域的高斯点能够获得足够的梯度信息，从而进行有效的分裂和克隆。

2. **像素覆盖数量的考虑**：为了避免仅依赖视图数量来计算平均 2D 位置梯度，UW-GS 引入了像素覆盖数量的考虑。通过计算高斯点在 2D 空间中覆盖的像素数量，而不是单纯依赖视图数量，从而进一步提高了远距离区域的重建质量。

3. **局部调整**：为了处理靠近相机的浮动物体（floaters），UW-GS 引入了一个与深度相关的缩放因子（$ S_z $），用于局部调整 2D 位置梯度：

   $$ \delta L_{\text{Rec}} = S_z \cdot \delta L_{\text{Rec}} $$

   这样可以有效抑制靠近相机的浮动物体，而不影响其他区域的重建质量。

通过这些改进，UW-GS 能够在远距离区域生成更密集的高斯点，从而减少模糊伪影，提高重建质量。

### **3.4 二值运动掩码（Binary Motion Mask, BMM）**

水下场景中动态物体的存在会引入重建伪影，影响重建结果的质量。为了解决这一问题，UW-GS 引入了二值运动掩码（BMM），通过检测和排除动态物体的影响，提高重建结果的鲁棒性。

BMM 的计算过程如下：

1. **残差计算**：计算每个像素的残差（$ \epsilon $），即渲染图像与真实图像之间的差异：

   $$ \epsilon = \| \hat{I} - I \|_2 $$

   其中 $ \hat{I} $ 是渲染图像，$ I $ 是真实图像。

2. **阈值判断**：根据残差的分布，设置阈值（$ T_\epsilon $），将残差小于阈值的像素视为静态物体（inliers），其余像素视为动态物体（outliers）。

3. **空间平滑**：通过 3×3 扩散核（$ B $）对掩码进行空间平滑处理，避免高频噪声的影响：

   $$ \omega_2 = \omega_1 * B \geq T_* $$

   其中 $ \omega_1 $ 是初始掩码，$ T_* $ 是平滑后的阈值。

4. **局部分类**：根据每个 8×8 块的 16×16 邻域的平均值，进一步分类像素，确保掩码的准确性：

   $$ \omega_3(R_{8 \times 8}) = \omega_2(R_{16 \times 16}) \geq T_R $$

   其中 $ T_R $ 是局部分类的阈值。

最终，BMM 通过结合上述三种方法，生成一个鲁棒的掩码，用于排除动态物体的影响。在训练过程中，BMM 被应用于重建损失函数中，从而提高重建结果的鲁棒性。

### **3.5 损失函数（Loss Function）**

为了优化模型参数，UW-GS 设计了一个综合损失函数，包括以下三个部分：

1. **重建损失（Reconstruction Loss, $ L_{\text{Rec}} $）**  

   结合 L1 损失和结构相似性（D-SSIM）损失，用于优化重建图像的质量：

   $$ L_{\text{Rec}} = \lambda \cdot \omega \cdot L_1 + (1 - \lambda) \cdot \omega \cdot L_{\text{D-SSIM}} $$

   其中 $ \lambda $ 是损失权重，$ \omega $ 是二值运动掩码。

2. **深度监督损失（Depth-supervised Loss, $ L_{\text{da}} $）**  

   通过伪深度图（由 DepthAnything 生成）监督高斯点的深度信息，减少浮动物体的影响：

   $$ L_{\text{da}} = \lambda_d L_d + \lambda_{\text{ca}} L_{\text{ca}} $$

   其中：

   - $ L_d $ 是深度监督的 L1 损失：

     $$ L_d = \| \hat{D} - D \| $$

   - $ L_{\text{ca}} $ 是通道深度对齐损失，基于水下图像形成模型，约束深度信息：

     $$ L_{\text{ca}} = \sum_{x \in \{r,g,b\}} \sum_{y \in \{d,b\}} \| \hat{z}_{y,x} - z \| $$

     其中 $ \hat{z}_{y,x} = -\log(T_{y,x}) / \beta_y $。

3. **灰度世界假设损失（Gray-world Assumption Loss, $ L_g $）**  
   基于灰度世界假设，约束场景中物体的颜色分布，确保模型能够合理估计介质参数：

   $$ L_g = \sum_{x \in \{r,g,b\}} \| \mu(J_x) - 0.5 \|_2^2 $$

   其中 $ \mu(J_x) $ 是通道 $ x $ 的平均值。

综合损失函数定义为：

$$ L = L_{\text{Rec}} + L_{\text{da}} + L_g $$

通过结合上述损失函数，UW-GS 能够在训练过程中同时优化重建质量、深度信息和颜色分布，从而提高水下场景重建的精度和鲁棒性。



## **方法总结**

UW-GS 通过以下创新点显著提升了水下场景重建的质量：

1. **新型颜色表现模型**：通过 MLP 估计水介质参数，并根据这些参数调整高斯点的颜色，从而更好地模拟水下场景的颜色变化。

2. **基于物理的密度控制策略**：通过补偿光衰减对梯度计算的影响，使得高斯点能够在远距离区域更有效地分裂和克隆，减少模糊伪影。

3. **二值运动掩码（BMM）**：通过检测和排除动态物体的影响，提高重建结果的鲁棒性。

4. **综合损失函数**：结合重建损失、深度监督损失和灰度世界假设损失，优化模型参数，提高重建精度。

这些创新模块共同作用，使得 UW-GS 在多个水下数据集上表现出色，显著优于现有的方法。

# 原文翻译

## **UW-GS: 针对水下场景重建的干扰感知型3D高斯点绘制方法**

**作者**: Haoran Wang, Nantheera Anantrasirichai, Fan Zhang, David Bull  

**单位**: 布里斯托大学计算机科学学院，布里斯托，英国  

**邮箱**: {yp22378,n.anantrasirichai,fan.zhang,dave.bull}@bristol.ac.uk  

**摘要**  

3D 高斯点绘制（3DGS）能够实现实时高质量的 3D 场景渲染。然而，3DGS 假设场景处于清晰介质环境中，在水下场景中（光吸收和散射普遍存在且存在动态物体）难以生成令人满意的表示。为了克服这些问题，我们提出了一种新型的基于高斯点绘制的方法——UW-GS，专门用于水下应用。该方法引入了一种颜色表现模型，用于模拟随距离变化的颜色变化；采用了一种新的基于物理的密度控制策略，以增强远距离物体的清晰度；并使用二值运动掩码来处理动态内容。通过针对散射介质优化设计的损失函数，并结合伪深度图的支持，UW-GS 在性能上优于现有方法，PSNR增益高达1.26dB。为了充分验证模型的有效性，我们还开发了一个新的水下数据集 S-UW，包含动态物体掩码。UW-GS 的代码和 S-UW 数据集可在 [https://github.com/WangHaoran16/UW-GS](https://github.com/WangHaoran16/UW-GS) 获取。


## **1. 引言**

近年来，海洋探索的重要性日益增加，这主要得益于其在水下考古、地质学和其他海洋科学中的应用。然而，水下活动常常受到现有技术的限制、潜水专家的稀缺以及高昂的运营成本的制约。因此，3D场景重建变得至关重要，因为它允许在岸上捕获、可视化和分析水下数据，从而无需持续的水下物理存在即可进行更广泛和详细的研究。最新的3D场景表示方法通常采用基于神经辐射场（NeRF）[32]和3D高斯点绘制（3DGS）[22]的技术来建模野外捕获的静态视频内容。NeRF模型隐式地将场景表示为由深度神经网络参数化的辐射场，该网络预测体积密度和颜色。尽管NeRF能够提供有效的视图合成，但其训练需要大量的计算时间。此外，隐式的神经辐射场也倾向于产生次优的重建结果[14]。相比之下，3DGS为训练和渲染提供了一种更快的替代方案[6]，它使用一组空间中的点（称为“高斯点”）来表示场景，并且通常能够提供更好的质量。尽管NeRF和3DGS发展迅速，但大多数方法都是为清晰介质设计的，只有少数研究尝试在诸如雾和水下等弥散环境中重建3D场景[34]。水下场景尤其面临由水深、海面波浪和水清澈度等因素影响的高度变化的光照条件。在这些环境中，视觉内容常常由于光的吸收和后向散射而经历显著的衰减和退化，导致3D重建方法（尤其是3DGS）在颜色和密度估计中出现不准确，如图1上排所示。此外，水下场景通常包含移动元素，如鱼和漂浮物，增加了复杂性。这些元素通常被称为3D建模中的“干扰物”，因为它们并非始终存在，并且会在某些视图中产生伪影[35]，如图1下排所示。为了处理这些问题，模型必须准确区分静态和动态成分，而在流体介质中，这一任务变得更加困难。遗憾的是，现有的方法[26, 39]并未解决这一问题。

为了应对上述问题，我们提出了一种新的基于高斯点绘制（GS）的方法——UW-GS，专门用于水下场景。与传统3DGS方法不同，后者由于使用球谐函数而难以处理光散射，我们的方法包含一个新颖的颜色表现模型，将介质效应整合到高斯属性中，并采用仿射变换来解释不同距离处的外观变化。我们还引入了一种基于物理的密度控制策略，通过确保足够的高斯表示来增强远距离物体的清晰度。为了管理干扰物，我们为GS开发了一种鲁棒性估计算法，在渲染过程中使用二值运动掩码以最小化动态内容对训练的影响。我们提出的UW-GS使用一种新的基于散射介质特性的损失函数进行优化，以减少模糊效应并实现更准确的介质效应估计。我们还将DepthAnything[47]生成的伪深度图整合到我们的方法中，以增强其鲁棒性。鉴于水下数据集的稀缺性，我们收集了一个新的数据集，包含四个浅水区域的广阔场景，与现有数据集相比，每个场景都呈现出独特的挑战。我们还在该数据库中提供了动态物体掩码图，以促进进一步的研究和评估。总之，我们的主要贡献如下：

1. 我们首次提出了一个具有干扰物感知能力的水下场景表示的高斯点绘制解决方案，即UW-GS。
2. UW-GS采用了新颖的颜色表现模型，通过转换高斯颜色来解决由于光衰减和散射导致的低对比度和颜色失真问题。
3. 我们提出了一个新的基于物理的密度控制策略，旨在防止在高吸收和散射介质中出现稀疏的高斯点。这种方法显著增强了远距离区域细节的清晰度。
4. 我们提出了一个新的损失函数，将散射介质特性与伪深度图和像素级二值运动掩码相结合，分别解决水下场景中的估计深度模糊和移动干扰物问题。
5. 我们引入了第一个浅水水下数据集，其中包含相关的干扰物标记。该数据集呈现出独特的挑战，包括光线闪烁和水流，增加了3D重建的复杂性。


## **2. 相关工作**

### **水下成像**

水下环境常常面临诸如色偏、图像畸变和对比度损失等问题，这主要是由于水的物理特性及其与光的相互作用[16]。鉴于雾和水下图像处理之间的相似性，研究者们采用了诸如暗通道先验[17]、灰度世界假设[11]和直方图均衡化[15]等技术。此外，还有多项研究对水下光行为进行了建模，包括水下光传播特性[9]、辐射传输理论[10]以及光反射模型[9]。目前，被广泛使用的是改进型水下图像形成模型（UIFM）[1]，该模型指出直接光和后向散射光的衰减与散射因子是不同的，因此需要独立建模。许多研究[20, 21, 50]利用这一模型取得了显著的成果。然而，仅依赖单图像处理在类似场景中可能会产生不一致的结果，限制了这些方法的可靠性和有效性，尤其是在用于三维建模时。

### **神经辐射场（NeRFs）**

近年来，神经辐射场（NeRFs）[32]在三维重建领域取得了重大进展。NeRF利用辐射场——一种沿射线对每个采样点计算密度和颜色的体积表示方法[8]。在此基础上，后续的NeRF技术通过多种方法改进了合成效果，包括采用锥形追踪代替传统的射线追踪[3, 4]、超分辨率技术[18, 28]、基于网格的方法[5, 38, 41]以及去模糊技术[42]。此外，还开发了使用更少参数的更紧凑模型[45]，并引入了更快的渲染技术[29]。NeRF还被扩展到动态场景[30]和少样本渲染场景[37]。然而，这些方法在散射介质中通常表现不佳。为解决这一问题，研究者开发了scatterNeRF[34]，该方法能够有效地在雾天场景中区分物体与介质。SeaThru-NeRF[26]利用改进型水下图像形成模型[1]，通过新颖的架构确定水介质系数。此外，其他研究[39]致力于准确表示水下场景，引入动态渲染和色调映射以处理变化的光照条件。尽管取得了这些进展，但基于NeRF的方法仍然面临着训练和渲染速度缓慢的挑战，尤其是在大型场景中。

### **三维高斯点绘制（3DGS）**

三维高斯点绘制（3DGS）[22]是一种显式表示方法，它利用一组可训练的三维高斯点。由于其训练速度快、能够高质量地实现实时渲染，3DGS已成为计算机图形学中一项非常受欢迎的技术。我们的方法也基于3DGS开发，充分利用了这些优势。为了优化这一方法以应对复杂数据，研究者们提出了许多改进措施，包括去模糊技术[7, 24]、抗锯齿技术[46, 49]、频率正则化[51]以及改进的密度控制策略[52]。此外，3DGS还被扩展到四维渲染[43]，以适应动态场景的渲染。尽管取得了这些进展，但3DGS的固有特性仍然限制了其在复杂水下环境中的表现[27]。


## **3. 方法**

### **3.1 问题定义**

三维高斯点绘制（3DGS）是一种基于点的方法，使用离散的高斯点云来表示三维场景。在渲染特定视图时，这些三维高斯点会被投影到二维空间。每个高斯点的模型如下：

$$
G(x) = \exp\left(-\frac{1}{2}(x - \mu)^T \Sigma^{-1}(x - \mu)\right), \quad (1)
$$

其中，$G$ 是以中心位置 $\mu$ 和三维协方差矩阵 $\Sigma$ 为参数的高斯函数。每个高斯点还具有透明度 $\alpha$ 和视图依赖的颜色 $c$，颜色通过球谐函数（SH）表示。最终图像颜色通过 $\alpha$ 混合方法生成，并按照公式（2）进行光栅化：

$$
C = \sum_{i=1}^{N} \alpha_i c_i \prod_{j=1}^{i-1} (1 - \alpha_j), \quad (2)
$$

其中，$N$ 表示高斯点的数量。

在训练过程中，二维位置梯度（反向传播中的中间结果）被用作指示“颜色欠表达”问题的依据[23]。基于其平均值，通过分裂和克隆，从COLMAP[36]获得的稀疏点云会自适应增长。此外，3DGS通过定期修剪来控制高斯点的数量，以防止过度密集。然而，公式（2）对于表示水下场景是不足够的[39]。与透明的空气介质不同，水下环境中的吸收和散射效应会引入额外的衰减和后向散射，从而影响场景的表示。根据文献[1]的建议，水下图像强度可以建模为：

$$
I = J \cdot T_D + B_\infty \cdot (1 - T_B), \quad (3)
$$

其中，$I$ 表示相机捕获的颜色，$J$ 是场景中物体的真实颜色（无介质效应），$B_\infty$ 表示无限远处的背景光颜色。$T_D$ 和 $T_B$ 分别反映了介质（水）对直接来自物体的光和被水后向散射的光的影响：

$$
T_D = \exp(-\beta_d \cdot z), \quad T_B = \exp(-\beta_b \cdot z), \quad (4)
$$

其中，$\beta_d$ 和 $\beta_b$ 是两个 RGB 通道的介质系数，$z$ 是距离。结合公式（3），我们可以推断，随着物体远离相机，其颜色（直接信号）会逐渐减弱，而背景光（后向散射信号）会逐渐增强。假设背景光是理想的均匀分布，那么在水下场景中远离相机的区域，颜色过渡是平滑的，这会导致颜色失真和对比度降低。如 Mini-splatting[12]所述，这一问题会削弱原始3DGS中自适应密度控制的有效性，这将在第3.4节中讨论。因此，只有有限数量的稀疏高斯点负责表示这些区域，这通常会导致明显的模糊伪影。这种问题在水下场景中由于光吸收而普遍存在。因此，我们提出了一个新的颜色表现模型和一个基于物理的密度控制模块，用于UW-GS。此外，移动物体（如鱼和漂浮颗粒）对水下三维重建提出了挑战。它们随时间变化的位置会导致静态物体的外观出现错误，并在新视图的渲染中引入漂浮物。最近的研究[31, 44, 48]利用时间信息实现了四维渲染；然而，这些方法通常对具有可预测运动轨迹和小位移的前景运动进行建模。在水下环境中，移动物体通常表现出快速移动和显著的位置变化，使得动态渲染方法不太适用。受文献[35, 53]的启发，我们提出了一个像素级掩码，称为二值运动掩码（BMM），用于检测异常物体并将其从场景中移除。

### **3.2 UW-GS概述**

我们的UW-GS方法的示意图如图2所示，其中我们的颜色表现模型被集成到针对水下环境修改的3DGS中。颜色表现模型使用编码深度和视图方向作为输入，通过多层感知器（MLP）估计水介质的参数，并使用仿射变换相应地调整高斯点的颜色。经过颜色调整的三维高斯点将被发送进行二维投影，然后在光栅化模块中生成像素颜色，以输出最终的水下图像。3DGS包括一个基于物理的密度控制模块，该模块使用MLP估计水介质，并在训练期间使用二值运动掩码（BMM），以分别减轻水介质的负面影响和处理移动物体。


## **3. 方法**

### **3.3 颜色表现模型**

原始的3DGS[22]中的高斯颜色表现是通过球谐函数（SH）得出的，这假设场景介质是透明的，难以模拟水中的变化衰减和后向散射效应。为了解决这个问题，我们提出了一种新的颜色表现方法。图2的左侧面板展示了我们方法的工作流程。类似于[25]，我们使用一个额外的多层感知机（MLP）$f$，以位置编码的深度和视图方向作为输入来估计介质属性：

$$
(T_D^i, T_B^i, \beta_d^i, \beta_b^i, b^i) = f(\gamma(z_i), \gamma(\theta_i)), \quad i \in [1, N]. \quad (5)
$$

这里，$z_i$ 和 $\theta_i$ 分别指高斯点的深度和视图方向。我们通过测量连接高斯点中心和相机的线的长度和法向量来表示这些参数。通过在主干之后使用五个头层，我们可以估计衰减和后向散射因子 $T_D^i$ 和 $T_B^i$，以及每个高斯点的介质系数：$\beta_d^i$，$\beta_b^i$ 和 $b^i$。这些系数根据物理模型调节前两个因子。使用获得的参数，通过应用仿射变换计算修正后的颜色 $c_m$：

$$
c_m^i = T_D^i \cdot c_i + (1 - T_B^i) \cdot b^i. \quad (6)
$$

这个模型使我们能够调整物体外观，以模拟水下场景中随距离变化的可见度。然后，将修改后的高斯颜色发送到3DGS模块。

### **3.4 基于物理的密度控制**

在标准的3DGS框架中，高斯点云是自适应密集的，以获得更好的表示能力。3DGS使用二维位置梯度 $\frac{\partial L_{\text{Rec}}}{\partial \text{mean}_{2D}}$，这是从像素颜色梯度 $\frac{\partial L_{\text{Rec}}}{\partial \text{color}}$ 派生出来的。如果这个梯度超过阈值 $\tau$，高斯点将被分裂或克隆。然而，在水下场景中，公式（3）表明 $\frac{\partial L_{\text{Rec}}}{\partial \text{color}}$ 是从 $T_D \cdot \frac{\partial L_{\text{Rec}}}{\partial \text{color}_{\text{object}}}$ 计算的，因此每个高斯颜色梯度都有一个由于光吸收效应而产生的衰减因子 $T_D$。如图3左侧块所示，颜色梯度的衰减将导致二维位置梯度计算的偏差；我们将这种效应表示为函数 $\alpha_{\text{atten}}(z)$。类似于 $T_D$，这个函数与深度 $z$ 反相关，这意味着在水下场景中，二维位置梯度会随着距离的增加而减小。因此，高斯点在远处区域几乎无法分裂或克隆，导致密集失败。这个问题会导致之前提到的模糊伪影，因为稀疏的高斯分布如图3右侧块所示。为了减轻衰减因子对梯度计算的影响，我们引入了一种称为基于物理的密度控制的精细密集策略。对于二维位置梯度计算，这种方法采用从新颜色表现模型估计的衰减因子的逆 $\frac{1}{T_D}$ 来处理水下颜色梯度模型 $T_D \cdot \frac{\partial L_{\text{Rec}}}{\partial \text{color}_{\text{object}}}$。因此，我们的 $\frac{\partial L_{\text{Rec}}}{\partial \text{color}} = \frac{\partial L_{\text{Rec}}}{\partial \text{color}_{\text{object}}}$。

这种调整有效地补偿了二维位置梯度计算的偏差，使3DGS能够生成更密集的高斯点云，具有更好的表示精度。此外，通过调整二维位置梯度，这种方法优化了物体颜色信息显著衰减的区域，从而提高了3DGS在颜色平滑过渡区域的性能。为了进一步解决大尺寸高斯投影引起的模糊伪影，我们遵循[52]的方法，考虑二维空间中覆盖的像素数量，而不是仅依赖视图数量来计算平均二维位置梯度进行密度控制。这使得以前由稀疏高斯表示的区域在原始3DGS中能够更有效地增长。结果是，大规模的点云表现出更好的表示能力，从而减少了模糊和针状伪影。此外，为了解决靠近相机的浮动物体问题，这是3DGS的常见问题，受[52]的启发，我们局部减少了与深度 $z$ 相关的缩放因子 $S_z$ 的二维位置梯度。因此，我们的最终像素颜色梯度为 $\frac{\partial L_{\text{Rec}}}{\partial \text{color}} = S_z \frac{\partial L_{\text{Rec}}}{\partial \text{color}_{\text{object}}}$。结合我们的基于物理的密度控制，该方法全局缩放梯度，有助于抑制靠近相机的浮动物体，而不影响我们在其他区域的增强效果。

## **3.5 二值运动掩码（Binary Motion Mask）**

尽管采用改进的颜色表现模型能够提升水下场景的建模效果，但处理数据集中的动态内容仍然具有挑战性。一些研究（例如[2, 19]）通过引入额外的时间维度来建模移动物体。然而，水下动态内容（如游动的鱼和闪烁的物体）通常具有随机的轨迹和高速运动。在这种情况下，使用变形场网络的动态场景渲染算法既不高效也不有效。受RobustNeRF[35]的启发，我们在重建损失函数中引入了一个二值运动掩码（BMM）$\omega$，以消除干扰物，公式如下：

$$L_{\text{Rec}} = \lambda \cdot \omega \cdot L_1 + (1 - \lambda) \cdot \omega \cdot L_{\text{D-SSIM}}, \quad (7)$$

其中，$\lambda$ 是损失权重。正如[35]中所指出的，修剪估计器在检测集合中一部分异常值方面非常有效。在我们的实现中，$\omega^t$ 在第 $t$ 次迭代时按如下方式计算：

$$\omega_1^t = \epsilon^t \leq T_\epsilon, \quad (8)$$

其中，$\epsilon^t$ 是残差，计算公式为 $\epsilon^t = \| \hat{I}^t - I \|_2$，而 $\hat{I}^t$ 是在第 $t$ 次迭代时的渲染图像。在公式（8）中，我们假设残差低于阈值 $T_\epsilon$（通过排序上一次迭代的残差来设置）的属于内点，其余的则被认为是异常值。然而，仅依赖这个掩码还不足以有效区分干扰物，这可能会在我们的渲染中引入失真。因此，我们应用了一个 $3 \times 3$ 的扩散核 $B$ 来维持空间平滑性，如公式（9）所示。此外，为了避免高频内容被当作异常值处理，我们将每个 $8 \times 8$ 的块 $R_{8 \times 8}$ 按照其 $16 \times 16$ 邻域 $R_{16 \times 16}$ 的平均值进行分类。第三个掩码的描述如下：

$$\omega_2^t = \omega_1^t * B \geq T_*, \quad (9)$$

$$\omega_3^t(R_{8 \times 8}) = \omega_2^t(R_{16 \times 16}) \geq T_R, \quad (10)$$

其中，$T_*$ 和 $T_R$ 是阈值，可以像 $T_\epsilon$ 一样手动调整。如果一个像素通过上述三个加权公式中的任何一个被分类为内点，那么它最终将被视为内点，以防止我们的估计过于激进。最终的 BMM 在时间 $t$ 时从以下公式获得：

$$\omega^t = \omega_1^t \cup \omega_2^t \cup \omega_3^t. \quad (11)$$

## **3.6 损失函数（Loss Function）**

用于优化的最终损失函数 $L$ 包括重建损失 $L_{\text{Rec}}$（见第3.5节）、深度监督损失 $L_{\text{da}}$ 和灰度世界假设损失 $L_g$，定义如下：

$$L = L_{\text{Rec}} + L_{\text{da}} + L_g. \quad (12)$$

**深度监督损失 $L_{\text{da}}$**：深度信息是水下场景建模的关键。在3DGS中，我们可以像渲染图像一样渲染深度图：

$$\hat{D} = \sum_{i \in N} \alpha_i z_i \prod_{j=1}^{i-1} (1 - \alpha_j). \quad (13)$$

然而，不合理定位的高斯点集（如浮动物体）会严重影响我们对水下介质系数的估计，因为它们会导致错误的深度测量。为了增强高斯点中心与相机中心之间深度测量的可靠性，并减少浮动物体问题，我们实现了深度监督损失，记作 $L_{\text{da}}$：

$$L_{\text{da}} = \lambda_d L_d + \lambda_{\text{ca}} L_{\text{ca}},$$

$$L_d = \| \hat{D} - D \|, \quad L_{\text{ca}} = \sum_{x \in \{r,g,b\}} \sum_{y \in \{d,b\}} \| \hat{z}_{y,x} - z \|, \quad (14)$$

其中，$\lambda_d$ 和 $\lambda_{\text{ca}}$ 是超参数，$L_d$ 是深度监督的 L1 损失。我们使用由 DepthAnything[47] 预测的合成真实深度图 $D$，这是一个能够适应任何环境的单目深度估计模型。请注意，$D$ 和 $\hat{D}$ 都是相对对齐而不是绝对对齐的。受[40]的启发，我们还包含了名为通道深度对齐损失的 $L_{\text{ca}}$。该损失基于水下图像形成模型（公式3），通过另一种方式计算每个高斯点的近似深度：

$$\hat{z}_{xy} = -\frac{\log(T_{yx})}{\beta_y}. \quad z \text{ 是高斯点的测量深度。}$$

$L_{\text{ca}}$ 对 $T_D$ 和 $T_B$ 施加限制，它们与深度密切相关。在 $L_{\text{da}}$ 中，我们在像素和高斯点级别校正深度信息。

**灰度世界假设损失 $L_g$**：基于图像恢复先验知识[13]的灰度世界假设损失 $L_g$ 也被纳入：

$$L_g = \sum_{x \in (r,g,b)} \| \mu(J_x) - 0.5 \|_2^2. \quad (15)$$

基于灰度世界假设，$L_g$ 旨在约束场景中具有相似 RGB 通道平均值的物体颜色，以确保 $\beta_d$、$\beta_b$ 和 $b$ 能够被适当地预测。

## **4. 实验配置**

### **数据集**

实验中使用了三个数据集进行性能评估：

1. **SeaThru-NeRF[26]**：包含在不同海域拍摄的四个图像序列，每个序列都经过白平衡增强预处理以降低噪声。

2. **野外数据集（IW）[39]**：在中等深度水下拍摄，提供动态物体掩码。该数据集用于评估二值运动掩码（BMM）的性能。

3. **我们的数据集（S-UW）**：包含四个不同场景的高清水下视频，每个场景包含24张图像。这些场景的示例如图4所示。与现有数据集不同，S-UW是在浅水区域拍摄的，引入了额外的挑战，例如光线闪烁。对于所有视频，我们每八帧选择一帧用于测试。

### **实现细节**

我们的方法使用Pytorch实现。我们利用COLMAP[36]初始化点云并估计序列图像的相机位置。我们修改了原始3DGS中使用的diff-gaussian-rasterization，以渲染深度图，并在反向过程中增加了用于深度损失的额外梯度计算分支。对于介质MLP，我们使用了两个具有64个特征的线性层，并为每个输出增加了额外的头层。对于$\beta_D$和$\beta_B$，我们采用了Softplus激活函数，而对于$T_D$、$T_B$和$b$，我们使用了Sigmoid激活函数。

在训练过程中，我们在单个RTX3090 GPU上进行了15,000次迭代。前1,000次迭代作为预热阶段，在此阶段中，我们仅使用深度损失$L_d$和深度监督损失$L_{\text{da}}$来调整初始化的点云并预训练MLP。随后，我们使用与原始3DGS相同的优化设置来优化高斯属性，并应用独立的Adam优化器来训练我们的介质MLP。对于包含动态物体的场景，我们应用了二值运动掩码（BMM）。

### **基线方法与评估指标**

我们测试了我们的方法，并与三种最先进的方法进行了比较：Instant-NGP[33]、SeaThru-NeRF[26]和原始3DGS[22]。我们使用了它们的官方实现，但在相同的序列上使用相同的训练集分割策略进行训练。我们在静态区域计算了全局PSNR、SSIM和LPIPS值。此外，在动态场景中，我们使用数据集提供的运动掩码排除移动物体后，也使用这三种指标进行评估。

## **5. 结果与讨论**

**定量结果**  

表1展示了SeaThru-NeRF和S-UW数据集上每个基准的渲染质量评估结果。对于SeaThru-NeRF数据集，我们的方法表现最佳，与3DGS和SeaThru-NeRF相比，平均PSNR分别提高了2.09dB和2.70dB，尽管在日本红海场景中SSIM值排名第二。对于S-UW数据集，尽管UW-GS在泻湖场景的PSNR和海床场景的SSIM中排名第二，但总体表现仍然最佳，平均PSNR超出3DGS和SeaThru-NeRF分别为0.43dB和2.96dB。与3DGS相比的有限提升可以归因于水面不稳定的光照。

**定性结果**  

图5展示了基线方法与UW-GS之间的视觉比较。第一行显示了SeaThru-NeRF数据集的结果，第二行显示了S-UW数据集的结果。总体而言，我们的方法通过更好地重建远距离区域并减少伪影，提高了新视图合成结果的质量。我们的方法在各种场景中都表现出有效性和鲁棒性。此外，我们的方法还展示了从散射介质中区分物体的能力。基于我们模型估计的水介质，我们尝试“排干”水以呈现更清晰的恢复结果。图6表明，我们的方法在水下场景增强方面具有潜力。

**消融研究**  

我们通过一系列改进架构来隔离我们的贡献：（V1）仅使用球谐函数来表示视图依赖的颜色（注意MLP也将协助密度控制），（V2）在没有基于物理的密度控制的情况下训练UW-GS，以及（V3）移除深度监督损失。表2验证了我们的架构能够实现整体最佳性能，而其他方法则出现了性能下降。我们还分析了BMM中使用的三个掩码对移动物体移除的贡献。我们从IW数据集[39]中选择了两个具有挑战性的场景，这些场景包含运动掩码，有助于通过排除动态内容来评估重建质量。表3表明，我们的方法表现最佳，证明了结合三个掩码的BMM能够在训练期间最好地保留静态内容。

**局限性**  

由于水面光线的干扰不可忽视，我们的方法在浅水场景中的改进并不明显。主观上，我们的方法在没有任何额外的预处理或后处理的情况下，从场景渲染中消除了动态干扰物。更多的主观结果包含在补充材料中。然而，由于BMM可能无法完美分类，静态区域的重建质量在一定程度上有所下降，这在表4中IW数据集的客观结果中有所体现。

## **6. 结论**

本文提出了一种基于3DGS的水下应用新方法。我们引入了一种新的颜色表现模型，以更好地表示距离依赖的衰减和后向散射，这显著提高了重建结果，特别是在受水效应强烈影响的远区域。此外，我们提出了BMM，通过在训练期间排除干扰物，有效消除了因干扰物而产生的阴影状伪影。我们还引入了一个具有挑战性的水下数据集，包含4个不同场景。我们的方法在三个数据集的多个场景中进行了测试，超越了现有的静态水下场景重建基线，并展示了处理动态场景的能力，突显了其在水下环境探索和研究中的潜力。

## **致谢**

本研究得到了英国研究与创新（UKRI）MyWorld地方优势计划（SIPF00006/1）和工程与自然科学研究委员会（EPSRC）早期职业研究国际合作基金（EP/Y002490/1）的支持。

---
layout: mypost
title: 019 NeuMesh, Learning Disentangled Neural Mesh-based Implicit Field for Geometry and Texture Editing
categories: [论文阅读, Mesh重建, NeRF]
---

- [标题](#标题)
  - [链接](#链接)
  - [作者](#作者)
- [要点](#要点)
  - [目的](#目的)
  - [思想](#思想)
  - [方法](#方法)
- [想法](#想法)
  - [优点](#优点)
  - [缺点](#缺点)
- [后续要读的文章](#后续要读的文章)

# 标题

NeuMesh: Learning Disentangled Neural Mesh-based Implicit Field for Geometry and Texture Editing

2022, ECCV, 0 citations.

## 链接

论文链接：

- [Paper Page Link](https://arxiv.org/abs/2207.11911); [PDF Link](https://arxiv.org/pdf/2207.11911.pdf)
- [Project Link](https://zju3dv.github.io/neumesh/)

代码实现：

- [NeuMesh GitHub](https://github.com/zju3dv/neumesh)。

相关知识链接：

- 论文的一个贡献点是同时考虑了空间中的Mesh网格和纹理信息，并将几何结构和纹理表示解耦合，信息对应到空间中各个顶点上中。论文中提到UVMap，相关参考资料：[UVMap-Blog	](https://blog.csdn.net/cgsmalcloud/article/details/114542644)
- 关于SSIM、PSNR、LPIPS的相关度量指标解读：[SSIM/PSNR/LPIPS](https://blog.csdn.net/weixin_43135178/article/details/127664187)
  - ![image-20221109162017289](C:\Coding\github\zhangwenniu.github.io\_posts\2022-10-19-019 NeuMesh.assets\image-20221109162017289.png)

- BRDF是论文附录Discussion里面谈到的，关于物体模型的地方。参见[BRDF等辐射率的解读](https://www.cnblogs.com/herenzhiming/articles/5789043.html)


##  作者

> Bangbang Yang1∗, Chong Bao1∗, Junyi Zeng1, Hujun Bao1, Yinda Zhang2†, Zhaopeng Cui1†, and Guofeng Zhang1† 
>
> 
>
> 1 State Key Lab of CAD&CG, Zhejiang University 
>
> 2 Google

浙江大学计算机辅助设计和计算机图形学国家重点实验室的几个人。这篇文章的突出风格是，为了解决纹理编辑的目标任务，不断深入处理，引入多篇文章中的解决思路和处理办法，最终实现三维模型的隐式场纹理编辑。

计算机视觉希望能够得到一个通用的解决方法，计算机图形学可能更倾向于将一个问题做的比较好。

这篇文章算是神经隐式场表面重建工作的下游任务，并不致力于解决更好的表面重建效果。而是在默认表面已经有好的重建效果之后，使用特定的模型来解决实际的应用目标任务。


# 要点

## 目的

输入是某个物体来自多个视角的不同照片。

输出是编辑后的物体，这种编辑包括：对某些区域进行纹理交换；对某些区域进行纹理填充；在某张图片上作图，反映到整体三维模型中。

整体的想法是，能够尽可能的与目前的图形学编辑的流程、三维建模作图的流程对应起来。比方说作图，在某张图片上面作图成功，最好也能反应到其他的图像上面；如果想要编辑三维模型的纹理结构，最好是在三维模型的表面能够直接的编辑，不要翻折到UV纹理空间上面再编辑，因为UV纹理空间不容易展开，对人类的思维也是一种挑战。



## 思想

这篇文章，重要的一点在于，他通过实验、各种技术，证明了，这种纹理的解耦合是可以实现的。NeRF学习到的纹理编码是可以解构、并且对应的地方进行解码，变换到对应的区域的。这是没问题的。

这是一个挺优秀的实践。


## 方法


# 想法

## 优点



## 缺点



# 后续要读的文章

- [    ]  ARAP. As rigid as possible. 引用这篇文章的时候，是在说明移动某些顶点，以进行网格编辑。需要交互式的移动某些顶点，使用" out-of-box mesh deforming methods"。out of box是论文中一直没有明确说明的问题。
  - As-rigid-as-possible Surface Modeling. In: Symposium on Geometry Processing.
  - 47. Sorkine, O., Alexa, M.: As-rigid-as-possible Surface Modeling. In: Symposium on Geometry Processing. vol. 4, pp. 109–116 (2007) 7, 9, 22
- [    ] SGPN. 基于点的实例分割的文章。文章中用这篇工作的方法，选择两个苹果对应的三维模型的区域，进行纹理交换。2018年的文章。这里主要是进行标注。
  - SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation. 
  - 58. Wang, W., Yu, R., Huang, Q., Neumann, U.: SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2569–2578 (2018) 9, 24
- [    ] 迭代交互式的点配准工作，用于将两个mesh区域进行配准。
  - Open3D: A Modern Library for 3D Data Processing.
  - 73. Zhou, Q.Y., Park, J., Koltun, V.: Open3D: A Modern Library for 3D Data Processing. arXiv preprint arXiv:1801.09847 (2018) 9
- [    ] 对源区域和目标区域进行三维非刚性配准。1991年的工作，看起来还是有很多人在用。神奇。marching cubes也是一种方法。强啊。
  - 53. Umeyama, S.: Least-Squares Estimation of Transformation Parameters Between Two Point Patterns. IEEE Transactions on Pattern Analysis & Machine Intelligence 13(04), 376–380 (1991) 9, 22
- [    ] NeuTex。可以纹理编辑的隐式场渲染方法：NeuTex。算是一个SOTA方法。如果不做纹理编辑的话，可以不看这篇文章的。
  - NeuTex: Neural Texture Mapping for Volumetric Neural Rendering.
  - 59. Xiang, F., Xu, Z., Hasan, M., Hold-Geoffroy, Y., Sunkavalli, K., Su, H.: NeuTex: Neural Texture Mapping for Volumetric Neural Rendering. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 71197128 (2021) 4, 8, 9, 10, 11, 13
- [    ] 多视角神经表面重建任务，试图解耦合几何与外观的任务。2020年的NIPS，看来这不是空穴来风，大家都在做这件事情了。其实也是一件挺自然的想法。
  - Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance. 
  - 66. Yariv, L., Kasten, Y., Moran, D., Galun, M., Atzmon, M., Ronen, B., Lipman, Y.: Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance. Advances in Neural Information Processing Systems 33, 2492–2502 (2020) 4, 6, 8, 9, 10, 21

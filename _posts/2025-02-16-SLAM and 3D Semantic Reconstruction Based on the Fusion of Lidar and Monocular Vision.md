---
layout: mypost
title: SLAM and 3D Semantic Reconstruction Based on the Fusion of Lidar and Monocular Vision
categories: [SLAM]
---

# 链接

- [论文链接](https://www.mdpi.com/1424-8220/23/3/1502)


一投就中，几乎不退稿的综合3区sci

期刊名称：
《 SENSORS 》

影响因子:3.9

国人占比:31.2%(第一)

审稿周期:约1个月

录用率:80%

期刊详情：

MDPI出品的又一超级水刊，中科院3区月刊，年发文量10000篇左右，录用率高达80%，审稿还出奇的快，官网显示平均审稿周期21天，不到1个月，虽然20年有被关“小黑屋”，但是最新数据是非预警状态，依然是广大老铁的毕业评职不二之选！

# 论文难点解析


### **论文主题**
论文提出了一种新的方法，结合激光雷达（Lidar）和单目相机（只有一个摄像头的视觉系统），用于实现无人车辆的定位、地图构建和三维语义重建。简单来说，就是让无人车辆通过这两种传感器更好地“看懂”周围环境，并生成带有语义信息（比如道路、车辆、行人等）的三维地图。


### **研究背景**
- **SLAM（同步定位与建图）**：无人车辆需要在未知环境中定位自己，并同时构建地图。SLAM技术通常使用摄像头、激光雷达、惯性测量单元（IMU）或GPS等传感器来实现。
- **单目视觉SLAM**：使用摄像头获取丰富的视觉信息，但在光照不足的场景下定位精度较差。
- **激光雷达SLAM**：激光雷达提供准确的深度信息，但无法感知颜色和纹理，且在低纹理环境中表现不佳。
- **多传感器融合**：结合激光雷达和摄像头的优势，可以弥补单一传感器的不足，提高定位精度和地图构建质量。


### **研究方法**
论文提出了一种基于激光雷达和单目视觉融合的SLAM和三维语义重建方法，主要步骤如下：

1. **语义分割**：使用BiSeNetV2模型对单目图像进行语义分割，将图像中的物体（如道路、车辆、行人等）分类并标记。
2. **数据融合**：将激光雷达的三维点云投影到语义分割后的二维图像上，生成带有语义信息的稀疏深度图。
3. **深度插值**：通过深度插值算法对稀疏深度图进行上采样，使其与图像特征点匹配，生成密集的语义深度图。
4. **定位与优化**：使用ORB-SLAM2算法结合语义深度图进行实时定位，并通过捆绑调整（Bundle Adjustment）优化定位精度。
5. **三维语义重建**：利用定位信息和语义深度图，逐步构建三维语义点云地图。通过统计滤波和体素滤波去除噪声和冗余点，最终生成完整的三维语义地图。


### **实验与结果**
- **数据集**：使用KITTI视觉里程计数据集和CityScapes数据集进行实验。
- **语义分割效果**：BiSeNetV2模型在CityScapes数据集上训练后，对KITTI数据集中的物体进行了有效的语义分割，平均精度（AP）为21.8。
- **定位精度**：与ORB-SLAM2和DynaSLAM相比，论文提出的方法在KITTI数据集上的定位误差减少了约87%。与DEMO和DVL-SLAM相比，定位精度也有显著提升。
- **三维重建质量**：通过与Google Earth测量的真实尺寸对比，论文方法重建的物体宽度误差小于0.5%。与DynSLAM相比，重建质量更高，且存储需求减少了76%。


### **结论**
论文提出的方法通过融合激光雷达和单目视觉的优势，实现了高精度的定位和高质量的三维语义重建。这种方法在无人车辆的实际应用中具有重要的工程价值，未来还可以进一步结合更多传感器（如IMU）来进一步提升性能。


# 原文翻译

## 4. 实验与分析  

本文的实验平台由CPU Intel Xeon E3-1230、GPU Nvidia GTX1080Ti和16GB内存组成，操作系统为Ubuntu 16.04。我们使用公开数据集KITTI视觉里程计[9]和CityScapes[29]进行实验。

### 4.1 语义分割  

CityScapes数据集使用车载摄像头记录城市环境数据，并提供了详细的语义标注。因此，我们使用CityScapes数据集来训练Mask R-CNN模型。标注文件以JSON格式保存。CityScapes数据集包含5000张街景图像，其中2975张用于训练，500张用于验证，1525张用于测试。图像分辨率为1024×2048。  

KITTI视觉里程计数据集是在德国西南部的卡尔斯鲁厄市收集的，包含11个序列（编号为00到10），并提供了真实轨迹[9]。其中，序列01为高速公路场景，其余序列为城市道路或乡村道路场景。RGB图像序列以PNG格式存储，分辨率为1241×376；激光雷达扫描数据以BIN格式存储，大小约为1900 KB。  

我们训练并分割了18类常见于道路场景的物体，使用目标检测评估标准包括平均精度均值（mean AP）、AP50和AP75[30]。训练后的模型被应用于KITTI视觉里程计数据集，以不同颜色表示场景中的物体，分割结果如图3所示。训练后，AP50值达到39.4，AP75值达到29.8，平均精度均值（mean AP）为21.8。BiSeNetV2在CityScapes数据集上训练的18类物体的识别精度如表1所示。从表中可以看出，面积较大的物体识别效果更好。

### 4.2 数据融合与深度插值

图4展示了原始RGB图像与密集深度图的对比。我们将稀疏的激光雷达扫描投影到语义图像上，然后对融合后的数据进行深度插值。在深度图中，远处的黑色区域是激光雷达扫描范围之外的区域，距离车辆位置约120米。由于过远的物体和天空无法将发射的点云反射回激光雷达扫描仪，同时受到安装在KITTI数据采集车辆上的Velodyne64激光雷达垂直视场角（26.8°）的限制，深度图顶部1241×150区域的信息丢失。然而，由于我们的方法专注于交通场景，缺失区域中的建筑屋顶和天空部分并不需要。

我们提出的透视投影和上采样方法将点云坐标转换为笛卡尔坐标（而非极坐标），保留了二维图像的外观信息。基于透视投影和深度插值，我们不仅实现了点云与语义图像的融合，还提高了投影点的密度。这种融合方法使得视觉里程计能够以较低的复杂度直接匹配单目图像和激光雷达点云以实现定位。


### 4.3 基于激光雷达和单目视觉融合的定位精度

本文提出的算法使用RGB序列和通过深度插值算法上采样后的密集深度图来定位无人车辆。我们在KITTI视觉里程计数据集上，使用ATE（绝对轨迹误差）[31]和位移误差[32]来评估定位精度。ATE是估计轨迹与真实轨迹之间的直接差异，能够非常直观地反映算法的精度和轨迹的全局一致性。需要注意的是，估计的姿态和真实轨迹通常不在同一坐标系中，因此我们使用公开的评估工具EVO来对齐两条轨迹并计算误差。对于所有从$i = 1$到$i = m$的姿态，我们在李代数格式下计算ATE的均方根误差（RMSE），公式如下：

$$
\text{ATE} = \sqrt{\frac{1}{m} \sum_{i=1}^{m} \text{trans} \left( \mathbf{GT}_i^{-1} \mathbf{T}_i \right)^2}
\tag{18}
$$

其中，$\mathbf{GT}_i$和$\mathbf{T}_i$分别表示真实轨迹和估计轨迹。  
KITTI官方提出的位移误差标准是100米、200米……800米多类等长轨迹的位移误差的算术平均值，并以百分比形式计算。轨迹误差值越小，定位精度越高。位移误差的计算公式如下：

$$
E_{\text{trans}}(F) = \frac{1}{|F|} \sum_{(i,j) \in F} \left\| \left( \mathbf{GT}_j \ominus \mathbf{GT}_i \right) \ominus \left( \mathbf{T}_j \ominus \mathbf{T}_i \right) \right\|^2
\tag{19}
$$

其中，$F$是记录无人车辆运动的图像序列，$\mathbf{GT}$和$\mathbf{T}$分别是无人车辆姿态的真实值和估计值，$\{\mathbf{GT}, \mathbf{T}\} \in \text{SE}(3)$。$\mathbf{T}_j \ominus \mathbf{T}_i$是从子序列的第$i$帧到第$j$帧的姿态变换。

图5展示了KITTI视觉里程计数据集中00、01、05和07序列的估计轨迹与真实轨迹的对比。在图（a）和（b）中，虚线表示无人车辆的真实轨迹，实线表示估计轨迹，ATE值对应右侧的颜色谱。每个序列的轨迹基本与真实轨迹一致，这表明本文提出的算法能够获得精度较高的定位结果。在图（c）和（d）中，我们将本文方法的估计轨迹与ORB-SLAM2进行了比较，结果表明本文方法的实验结果漂移更小。

表2对比了本文方法与单目ORB-SLAM2[6]和DynaSLAM[7]在KITTI视觉里程计数据集上的轨迹误差。在00和02~10序列中，本文方法的平均ATE为1.39米，与ORB-SLAM2和DynaSLAM相比，误差分别减少了87.51%和87.21%。由于序列01记录的是空旷的高速公路场景，缺乏可提取的近距离特征点，难以满足文献[5]中提出的特征选择标准，因此单目ORB-SLAM2和DynaSLAM算法无法工作。而本文提出的算法利用了准确的深度信息，而不是通过三角测量获得深度值，因此能够有效定位。

我们将本文方法与其他多传感器融合SLAM方法进行了比较。表3对比了本文方法与DEMO[18]和DVL-SLAM[19]在KITTI视觉里程计数据集上的平均位移误差。在11个序列中，本文方法在7个序列中优于DEMO，在6个序列中优于DVL-SLAM。由于KITTI使用的Velodyne64激光雷达的探测范围为120米，在序列01中无法提供更远距离的深度信息，导致一些特征点被丢弃，从而产生较大的定位误差。在缺少深度信息的区域，DEMO通过三角测量估计特征点的深度，保留了比本文方法更多的特征点；DVL-SLAM基于直接法，不受近距离特征点过少的影响。因此，它们在序列01中具有更高的定位精度。

### 4.4 三维重建

由于KITTI视觉里程计数据集未提供三维场景中物体的真实尺寸，本节中我们使用MeshLab[33]测量三维重建序列05中物体的长度，并通过Google Earth[34]测量真实长度。我们通过计算两者的相对误差来进行定量评估。随后，我们对序列06进行三维重建，并与DynSLAM[22]在相同序列中的重建质量进行定性对比。为了便于比较，我们在定量和定性评估中展示的三维重建均保留了场景的原始颜色。在本节末尾，我们将展示序列00的三维语义重建结果，其中不同颜色表示点云中物体的类别。

#### 4.4.1 重建的定量评估

序列01的经纬度约为北纬9°03′06.6″，东经8°23′51.4″[9]。在图7中，（a）是使用MeshLab测量三维重建长度的示意图，测得房屋长度为27.74米；（b）是使用Google Earth测量房屋真实长度的示意图，测得长度为27.71米。以MeshLab的测量结果为观测值，Google Earth的测量结果为真实值，重建误差约为0.09%。

图8展示了我们重建的道路宽度与Google Earth的对比。（a）是通过MeshLab测量的重建道路宽度，宽度为4.21米；（b）是通过Google Earth手动测量的实际道路宽度，测得宽度为4.20米。重建道路宽度的相对误差约为0.24%。

---

**图7：重建宽度与真实建筑宽度的对比**  
(a) 使用MeshLab测量的建筑长度为27.74米。  
(b) 使用Google Earth测量的真实建筑长度为27.71米。

---

**图8：重建宽度与真实道路宽度的对比**  
(a) 使用MeshLab测量的重建道路宽度为4.21米。  
(b) 使用Google Earth测量的真实道路宽度为4.20米。


### 4.4.2 重建质量的定性评估

图9展示了我们的方法与DynSLAM在序列06中相同场景的重建质量对比。我们重建的点云总数为4.8×10⁹，而DynSLAM生成的点云总数为20×10⁹。由于激光雷达扫描角度较小，我们无法进行全视场的三维重建，并且我们对冗余和噪声进行了过滤，因此点云的总量相对较小，所需存储空间比DynSLAM低约76%。

图10展示了我们的方法与DynSLAM算法对同一墙面重建质量的对比。（a）是序列06中的第10帧图像。以（a）中右侧的白墙为参考，对比（b）中DynSLAM的重建结果和（c）中我们方法的重建结果。从对比中可以看出，DynSLAM重建的墙面严重失真，而我们的方法正确地恢复了墙面的几何结构。

序列00记录了一个564米×496米的城市场景。图11展示了序列00的重建结果，其中车辆、草坪、树木、道路等物体以不同颜色表示。语义重建保留了场景的几何结构，无人车辆可以利用重建地图中的语义信息完成避障和导航任务。

Daniel等人[35]将相机的语义图像与激光雷达点云融合，专注于非铺装道路场景。相比之下，我们的方法适用于城市驾驶场景。与我们的方法类似，David等人[36]也将激光雷达点云投影到语义分割的二维图像上，并将点云与语义标签关联，以维护和自动标记大规模三维点云地图。然而，为了推断稀疏激光雷达点的底层几何特征，他们需要在实验区域内行驶一次，提取预先构建的密集地图中的小密集区域，然后进行语义重建。我们的方法通过深度插值解决了稀疏性问题。在无人车辆首次行驶时，可以同时进行定位和语义重建，无需提前构建地图。通过融合激光雷达和单目相机，我们的方法能够在场景的单次通过中自动生成带有道路特征的世界表示，无需人工标记。

---

**图9：我们的方法与DynSLAM在序列06中的重建质量对比**  
(a) DynSLAM的重建结果；  
(b) 我们方法的重建结果。

---

**图10：相同墙面的重建质量对比**  
(a) 序列06的第10帧图像；  
(b) DynSLAM的重建结果；  
(c) 我们方法的重建结果。

---

**图11：序列00的语义重建结果**  
(a) 序列00的完整语义重建结果；  
(b) 局部细节的放大图。

---

## 5. 结论

在本文中，我们提出了一种基于激光雷达（Lidar）和单目视觉融合的SLAM（同步定位与建图）及三维语义重建方法。我们设计了一种深度插值算法，并利用语义分割网络BiSeNetV2实现了语义图像与激光雷达扫描的融合。激光雷达提供的精确深度信息被用于优化基于特征点的视觉SLAM的定位精度。此外，我们增加了一个密集建图线程，结合姿态和语义信息，在定位无人车辆的同时，同步重建室外场景的三维语义地图。通过对KITTI视觉里程计数据集的实验验证，我们可以得出以下结论：

1. **基于投影和插值方法**，我们实现了稀疏激光雷达点云的上采样，并将其与高分辨率二维图像融合。语义分割图像用于提供语义信息。实验结果表明，融合后的数据具有高分辨率和高可见性，可用于后续算法的实现和实验验证。

2. **与基于视觉的SLAM方法和激光雷达视觉融合SLAM方法相比**，我们在KITTI视觉里程计数据集上进行了实验。结果表明，在所有11个序列中，我们的方法的定位误差显著低于ORB-SLAM2和DynaSLAM。与DEMO和DVL-SLAM（基于激光雷达视觉融合）相比，我们的方法的定位精度也得到了提升。

3. **在三维重建方面**，我们重建的物体宽度与Google Earth测量的数据相比，误差小于0.5%。与DynSLAM在室外环境的三维重建相比，我们的重建质量更高，且存储空间需求减少了76%。生成的地图表示可以随着时间持续更新，同时，生成的语义重建具有人类可理解的标签，能够更好地支持未来自动驾驶技术的实际应用。

**未来工作**：我们将考虑融合更多传感器以提高定位精度和重建质量，例如通过结合相机和惯性测量单元（IMU）构建视觉惯性里程计（VIO）系统，使算法在缺乏可提取特征点的场景中表现更好。
